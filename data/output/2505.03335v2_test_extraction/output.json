{
  "document": {
    "id": "2505_6bed9150",
    "pages": [
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "May 9, 2025",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Andrew Zhao 1\n, Yiran Wu3\n, Yang Yue 1\n, Tong Wu2\n, Quentin Xu1\n, Yang Yue 1\n, Matthieu Lin1\n,\nShenzhi Wang 1\n, Qingyun Wu3\n, Zilong Zheng 2, and Gao Huang 1,",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1 Tsinghua University 2 Beijing Institute for General Artificial Intelligence 3 Pennsylvania State University",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "zqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pageheader",
            "text": "arXiv:2505.03335v2 [cs.LG] 7 May 2025",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning\ncapabilities of large language models by learning directly from outcome-based rewards. Recent RLVR\nworks that operate under the zero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training. The scarcity of high-\nquality, human-produced examples raises concerns about the long-term scalability of relying on human\nsupervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited\nlearning potential for a superintelligent system. To address these concerns, we propose a new RLVR\nparadigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own\nlearning progress and improves reasoning by solving them, without relying on any external data. Under\nthis paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training\ncurriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks\nand verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded\nlearning. Despite being trained entirely without external data, AZR achieves overall SOTA performance\non coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be\neffectively applied across different model scales and is compatible with various model classes.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "Code Project Page Logs Models\nFigure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "Code Project Page Logs Models",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "footnote",
            "text": "Corresponding author(s)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "figuregroup",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nSupervised Learning Reinforcement Learning with Verifiable Rewards Absolute Zero (Ours)\nLess Human Supervision",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "Supervised Learning Reinforcement Learning with Verifiable Rewards Absolute Zero (Ours)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Less Human Supervision",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 2. Absolute Zero Paradigm. Supervised learning relies on human-curated reasoning traces for behavior cloning. Reinforcement\nlearning from verified rewards, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a\nrespective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm, Absolute\nZero, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks\noptimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that\nprovides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "1. Introduction",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Large language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement\nLearning with Verifiable Rewards (RLVR) (Lambert et al., 2024). Unlike methods that explicitly imitate intermediate reasoning steps,\nRLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets (DeepSeek-AI et al., 2025;\nTeam et al., 2025; Jaech et al., 2024; OpenAI, 2025b;a). A particularly compelling variant is the \"zero\" RLVR paradigm (DeepSeek-AI\net al., 2025), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies\nRLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of\nreasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability (Villalobos et al., 2024). As reasoning\nmodels continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable (Yue\net al., 2025). A similar scalability bottleneck has already been identified in the domain of LLM pretraining (Sutskever et al., 2024).\nFurthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed\ntasks risks imposing constraints on their capacity for autonomous learning and growth (Hughes et al., 2024). This underscores the need\nfor a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which\nAI systems may surpass human intelligence.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "To this end, we propose \"Absolute Zero\", a new paradigm for reasoning models in which the model simultaneously learns to define tasks\nthat maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In\ncontrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to\nhacking (Silver et al., 2017; Chen et al., 2025; 2024), the Absolute Zero paradigm is designed to operate in open-ended settings while\nremaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how\nhumans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models (Hughes\net al., 2024). Similar to AlphaZero (Silver et al., 2017), which improves through self-play, our proposed paradigm requires no human\nsupervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward\nenabling large language models to autonomously achieve superhuman reasoning capabilities.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Building on this new reasoning paradigm, we introduce the Absolute Zero Reasoner (AZR), which proposes and solves coding tasks. We\ncast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable\nfeedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a\nprogram, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We\ntrain the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of\nthe proposed approach.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Despite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks\nin math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned\nwith domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically\ntrained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "compared to models trained in the \"zero\" setting using in-domain data. These surprising results highlight that general reasoning skills\ncan emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a\nfirst pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting\nfindings summarized below:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "\u2022 Code priors amplify reasoning. The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b.\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\nmay potentially amplify overall reasoning improvements after AZR training.\n\u2022 Cross domain transfer is more pronounced for AZR. After RLVR, expert code models raise math accuracy by only 0.65 points on\naverage, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\u2022 Bigger bases yield bigger gains. Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\u2022 Comments as intermediate plans emerge naturally. When solving code induction tasks, AZR often interleaves step-by-step plans\nas comments and code (Appendix C.3), resembling the ReAct prompting framework (Yao et al., 2023). Similar behavior has been\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B) (Ren et al., 2025). We therefore believe that allowing\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\u2022 Cognitive Behaviors and Token length depends on reasoning mode. Distinct cognitive behaviors\u2014such as step-by-step reasoning,\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\u2022 Safety alarms ringing. We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the\n\"uh-oh moment\", example shown in Figure 32, highlighting the need for future work on safety-aware training (Zhang et al., 2025a).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 Code priors amplify reasoning. The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b.\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\nmay potentially amplify overall reasoning improvements after AZR training.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 Cross domain transfer is more pronounced for AZR. After RLVR, expert code models raise math accuracy by only 0.65 points on\naverage, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 Bigger bases yield bigger gains. Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 Comments as intermediate plans emerge naturally. When solving code induction tasks, AZR often interleaves step-by-step plans\nas comments and code (Appendix C.3), resembling the ReAct prompting framework (Yao et al., 2023). Similar behavior has been\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B) (Ren et al., 2025). We therefore believe that allowing\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 Cognitive Behaviors and Token length depends on reasoning mode. Distinct cognitive behaviors\u2014such as step-by-step reasoning,\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 Safety alarms ringing. We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the\n\"uh-oh moment\", example shown in Figure 32, highlighting the need for future work on safety-aware training (Zhang et al., 2025a).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "2. The Absolute Zero Paradigm",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "2.1. Preliminaries",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Supervised Fine-Tuning (SFT). SFT requires the datasets of task-rationale-answer demonstrations D = {(x, c\u22c6\n, y\u22c6\n)}, where\nx is the query, c\n\u22c6\nis the gold chain-of-thought (CoT)) and y\n\u22c6\nis the gold answer, all provided by human experts or superior AI models.\nThe model trains to imitate the reference responses to minimize the conditional negative log-likelihood (Ouyang et al., 2022):",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "LSFT(\u03b8) = \u2212 E(x,c\u22c6,y\u22c6)\u223cD log \u03c0\u03b8\n\nc\n\u22c6\n, y\n\u22c6\n| x). (1)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "However, at the frontier level, there's no stronger model to distill from, and expert human labeling doesn't scale well.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Reinforcement Learning with Verifiable Rewards (RLVR). To move beyond the limits of pure imitation, RLVR only\nrequires a dataset of task and answer D = {(x, y\u22c6\n)}, without labeled rationale. RLVR allows the model to generate its own CoT and\ncalculate a verifiable reward with the golden answer r(y, y\u22c6\n). However, the learning task distribution D, with its set of queries and gold\nanswers are still labeled by human experts. The trainable policy \u03c0\u03b8 is optimized to maximize expected reward:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "JRLVR(\u03b8) = E(x,y\u22c6)\u223cD, y\u223c\u03c0\u03b8(\u00b7 |x)\n-\n\nr(y, y\n\u22c6\n)\n\n. (2)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "In summary, both SFT and RLVR still rely on human-curated datasets of either queries, demonstrations, or verifiers, which ultimately\nlimit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own\ninteractions with the environment entirely through self-play.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "2.2. Absolute Zero",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from\nboth stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We\nillustrate this paradigm in Figure 2, which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach\neliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "To make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid\nunderstanding, we include an illustration in Figure 3. Let \u03c0\u03b8 be our parameterized language model, it is used to play two roles, proposer\n\u03c0\npropose\n\u03b8\nand solver \u03c0\nsolve\n\u03b8 during training.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "Language Model\n!\"#!$%&\n'#()*\n\n, \u22c6\n, \"#$\"$%&\n\nEnvironment\n, \n%$'(&\nEnvironment\n\nFigure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent \u03c0\nproposing task \u03c4 , which is transformed by f with the environment e into a validated\nproblem (x, y\u22c6\n), and also emits a reward r\npropose for learnability. Then, a standard RL step\nfollows: the agent solves x by producing y, receiving reward r\nsolve from e by matching\nwith y\n\u22c6\n. \u03c0\npropose and \u03c0\nsolve are jointly trained and this process can be repeated indefinitely.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "Language Model\n!\"#!$%&\n'#()*\n\n, \u22c6\n, \"#$\"$%&\n\nEnvironment\n, \n%$'(&\nEnvironment",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "The proposer first samples a proposed\ntask conditioned on variable z: \u03c4 \u223c\n\u03c0\npropose\n\u03b8\n(\u00b7|z), which will then be validated\nand used to construct a valid reasoning task\ntogether with the environment e: (x, y\u22c6\n) \u223c\nfe(\u00b7|\u03c4 ), where x is the task query and y\n\u22c6\nis the gold label. Then the solver produces\nan answer y \u223c \u03c0\nsolve\n\u03b8\n(\u00b7 | x). Each pro-\nposed task \u03c4 is scored by a learnability\nreward r\npropose\ne (\u03c4, \u03c0\u03b8), which captures the\nexpected improvement in \u03c0\u03b8 after train-\ning on the task query x. Moreover, the\nsame policy also receives a solution re-\nward r\nsolve\ne (y, y\u22c6\n) for its answer to the task\nquery x, with the environment again serv-",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent \u03c0\nproposing task \u03c4 , which is transformed by f with the environment e into a validated\nproblem (x, y\u22c6\n), and also emits a reward r\npropose for learnability. Then, a standard RL step\nfollows: the agent solves x by producing y, receiving reward r\nsolve from e by matching\nwith y\n\u22c6\n. \u03c0\npropose and \u03c0\nsolve are jointly trained and this process can be repeated indefinitely.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "ing as the verifier. A nonnegative coefficient \u03bb balances the trade-off between exploring new, learnable tasks and improving the model's\nreasoning and problem-solving abilities. We formally define the absolute zero setting's objective as follows:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "J (\u03b8) := max\n\u03b8\nEz\u223cp(z)\n\"\nE(x,y\u22c6)\u223cfe(\u00b7|\u03c4),\u03c4\u223c\u03c0\npropose\n\u03b8\n(\u00b7|z)\n\nr\npropose\ne (\u03c4, \u03c0\u03b8) + \u03bb Ey\u223c\u03c0solve\n\u03b8\n(\u00b7|x)\n-\n\nr\nsolve\ne (y, y\n\u22c6\n)\n\n\n#\n. (3)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Notice that we shift the burden of scaling data away from human experts and onto the proposer policy \u03c0\npropose\n\u03b8\nand the environment\ne. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing\ngrounded feedback that supports stable and self-sustainable training. When proposing, z acts as a conditional variable that seeds\ngeneration of tasks. Practically, z can be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated\ntask memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward\nr\npropose(\u03c4, \u03c0\u03b8), which measures how much the model is expected to improve by solving a proposed task \u03c4 . Moreover, the solver reward\nr\nsolve(y, y\u2217\n) evaluates the correctness of the model's output. Together, these two signals guide the model to propose tasks that are both\nchallenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "3. Absolute Zero Reasoner",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "In this section, we present Absolute Zero Reasoner (AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an\nunified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them\nto improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of\nreasoning capacity while enhancing its ability to solve them effectively (Section 3.1). Within this self-play training paradigm, the model\nlearns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and\ninduction (Section 3.2). Using coding tasks is motivated by the Turing-completeness of programming languages (Stuart, 2015) and\nempirical evidence that code-based training improves reasoning (Aryabumi et al., 2024). We adopt code as an open-ended, expressive,\nand verifiable medium for enabling reliable task construction and verification (Section 3.3). Finally, the model is updated using a newly\nproposed advantage estimator designed for multitask learning (Section 3.3.5). We outline the overall algorithm in Algorithm 1 and\nhighlight an illustration of our Absolute Zero Reasoner approach in Figure 4. To expedite future exploration in this area, we also present\nseveral attempts that did not yield fruitful results but still warrant discussion in Appendix D.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "3.1. Two Roles in One: Proposer and Solver",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Large language models are naturally suited for implementing AZR in a multitask learning context (Radford et al., 2019), as both\nthe formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a\nsingle model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective\nin Equation (3). At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined\nin Section 3.2) and K past self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples,\npromoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning\ntasks that can be verified using the environment, outlined later in Section 3.3. AZR then attempts to solve these newly proposed tasks,\nreceiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning.\nWe now outline the rewards used for each role.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Reward Design. Prior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning\nsystems (Zeng et al., 2025b). Motivated by this, we design a reward function for the proposer that encourages generation of tasks",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "model input/output model reward ( , , ) P rogram I nput O utput\n Learnability\n\nReward\nAccuracy\n\nReward\nAbsolute\n\nZero\n\nReasoner\nVerify\nConstruct & Estimate \nPROPOSE\n\nSelf-play\n\nSOLVE\n\nJoint Update\nTask Types\nInduction:\nAbduction:\nDeduction:\nOX = F ( ) P ?\nOX = ( ) ? I\n? = F ( ) P I",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 4. Absolute Zero Reasoner Training Overview. At every iteration, Absolute Zero Reasoner first PROPOSES a batch of tasks,\nconditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section 3.2).\nFrom these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward rpropose is\nalso calculated for each proposed task as defined in Equation (4). The Absolute Zero Reasoner then SOLVES the batch of reasoning\nquestions. Python is used again to verify the generated responses and compute the accuracy reward rsolve as described in Equation (5).\nFinally, the Absolute Zero Reasoner is jointly updated using both rpropose and rsolve across all three task types, using TRR++ (Section 3.3.5).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "with meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language\nmodel in its solver role to estimate the learnability of a proposed task, a similar type of reward used in unsupervised environment\ndesign literature (Sukhbaatar et al., 2018). We perform n Monte Carlo rollouts of the solver and compute the average success rate:\nr\u00afsolve =\n1\nn\nPN\ni=1 r\n(i)\nsolve. The proposer's reward is then defined as:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "rpropose =\n\n0, if r\u00afsolve = 0 or r\u00afsolve = 1\n1 \u2212 r\u00afsolve, otherwise,\n(4)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "The intuition is that if a task is either trivial to solve (r\u00afsolve = 1) or unsolvable (r\u00afsolve = 0), the task provides little to no learning signal\nfor the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the\nrichest feedback and greatest potential for learning.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "For the solver, we assign a simple binary reward based on the correctness of its final output,",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "rsolve = I(y=y\u22c6)\n, (5)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "where y\n\u22c6\nis the ground-truth answer, and equality is evaluated based on value equality in Python.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "With the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which\nintegrates rpropose and rsolve with a format-aware penalty inspired by DeepSeek-AI et al. (2025):",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "R(y\u03c0) =\n\n\n\nrrole if the response is passable, role \u2208 {propose,solve}\n\u22120.5 if the response is wrong but well-formatted,\n\u22121 if the answer has formatting errors,\n(6)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "where y\u03c0 is the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek\nR1 <think> and <answer> format, as shown in Figure 33. Moreover, for the proposer, the reward criterion for format goes beyond\nsimply following the XML structure. As detailed in Section 3.3.3, only responses that produce valid triplets and pass the filtering stage\nare considered to be correctly formatted.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "AZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution,\nand validation of code reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O\nof a coding language, we define an AZR reasoning task as a triplet (p, i, o), where p \u2208 P is a program, i \u2208 I is an input, and o \u2208 O is\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about different parts of this task\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n3.2. Learning Diferent Modes of Reasoning: Deduction, Induction, and Abduction\nAZR uses code executor as both a fexible interface and a verifable environment. This setup enables automatic construction, execution,\nand validation of reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O of\na coding language, we defne an AZR reasoning task as a triplet (p, i, o), where p \u2192 P is a program, i \u2192 I is an input, and o \u2192 O is\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about diferent parts of this task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction buffer Ddeduction\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction bufer Ddeduction\n(all task bufers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verified using type-aware value equality\nin python to account for possible variations (such as set ordering or fractions).\ncompleting the triplet (p, i, o), which is added to the bufer if non-error output was produced.\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verifed using type-aware value equality\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\nin python to account for possible variations (such as set ordering or fractions).\n\u2022 As a proposer, the policy \u03c0\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\nexecutes p(i) and get the triplet (p, i, o).\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\u2022 As a proposer, the policy \u03c0propose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verified by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\nexecutes p(i) and get the triplet (p, i, o).\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verifed by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn\n, on\n)}, requiring generalization from partial information.\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn, on)}, requiring generalization from partial information.\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222a Ddeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn\n, on\n)}, m), which is stored\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222aDdeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn, on)}, m), which is stored\nin the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\u2022 As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\nlogic and promotes generalized induction.\n\u2022 As a solver, the model is shown the frst half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\nlogic and promotes generalized induction.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction buffer Ddeduction\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction bufer Ddeduction\n(all task bufers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verified using type-aware value equality\nin python to account for possible variations (such as set ordering or fractions).\ncompleting the triplet (p, i, o), which is added to the bufer if non-error output was produced.\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verifed using type-aware value equality",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\nin python to account for possible variations (such as set ordering or fractions).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 As a proposer, the policy \u03c0\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\nexecutes p(i) and get the triplet (p, i, o).\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\u2022 As a proposer, the policy \u03c0propose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verified by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\nexecutes p(i) and get the triplet (p, i, o).\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verifed by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "3. Induction: synthesizing a program p from a set of in-out examples {(i\nn\n, on\n)}, requiring generalization from partial information.\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn, on)}, requiring generalization from partial information.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222a Ddeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn\n, on\n)}, m), which is stored\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222aDdeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn, on)}, m), which is stored\nin the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "\u2022 As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\nlogic and promotes generalized induction.\n\u2022 As a solver, the model is shown the frst half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\nlogic and promotes generalized induction.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Each reasoning task type leverages code as an expressive and verifable\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025; Lam-\nbert et al., 2024). All prompts used by three diferent task types and two\ntypes of roles within a task type are shown in Figures 34 to 39. Next, we\noutline exact details of our algorithm.\nEach reasoning task type leverages code as an expressive and verifiable\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025;\nLambert et al., 2024). All prompts used by three different task types and\ntwo types of roles within a task type are shown in Figures 34 to 39. Next,\nwe outline exact details of our algorithm.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Program Triplet\nInput: \"Hello World\"\n1 def f(x):\n2 return x\nOutput: \"Hello World\"",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "3.3. Absolute Zero Reasoner Learning Algorithm\n3.3. Absolute Zero Reasoner Learning Algorithm",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "In this section, we will discuss details of our AZR self-play algorithm, includ-\ning initialization of bufers 3.3.1, usage of thse bufers 3.3.2, construction of\nvalid tasks 3.3.3, validating solutions 3.3.4, and fnally advantage estimator\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure of\nAZR in Algorithm 1.\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\ning initialization of buffers 3.3.1, usage of thse buffers 3.3.2, construction of\nvalid tasks 3.3.3, validating solutions 3.3.4, and finally advantage estimator\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure\nof AZR in Algorithm 1.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 5. The Seed AZR Zero Triplet. The above\nidentity function triplet was the only triplet provided\nto AZR to initiate its self-bootstrap propose-and-solve\nRLVR loop. We note that the base LLM is fully ca-\npable of initiating the AZR loop without any seed pro-\ngram; its inclusion illustrates our approach's fexibility:\nwe can optionally initialize seed programs with existing\ndatasets of varying complexity, and we initialized ours\nwith the simplest program.\nFigure 5. The Seed AZR Zero Triplet. The above\nidentity function triplet was the only triplet provided\nto AZR to initiate its self-bootstrap propose-and-solve\nRLVR loop. We note that the base LLM is fully capable\nof initiating the AZR loop without any seed program;\nits inclusion illustrates our approach's flexibility: we\ncan optionally initialize seed programs with existing\ndatasets of varying complexity, and we initialized ours\nwith the simplest program.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "3.3.1. BUFFER INITIALIZATION\nTo initialize AZR self-play, we frst generate a seed set of valid triplets using\n3.3.1. Buffer Initialization",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "the base language model. Each prompt samples up to K triplets from the\ncurrent seed bufer Dseed as references. When Dseed is empty at time 0, we\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\nuse the same proposer prompts detailed in Figures 34 to 36.\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\nTo initialize AZR self-play, we first generate a seed set of valid triplets using\nthe base language model. Each prompt samples up to K triplets from the\ncurrent seed buffer Dseed as references. When Dseed is empty at time 0, we\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\nuse the same proposer prompts detailed in Figures 34 to 36.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "(p, i) pairs, which are fltered, executed, and stored as valid triplets. We\ninitialize D0\nabduction = D0\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n(p, i) pairs, which are filtered, executed, and stored as valid triplets. We",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "B is the batch size, and S = 4 is a factor we fx in all experiments. All seed triplet's program are stripped of global variables and\ncomments (Appendix C), but subsequent iterations of adding new triplets to the bufers are unaltered. No model updates occur during\nthis phase. Similarly, to initialize the induction bufer, we sample programs from Dseed, generate matching input sets and messages, and\ncollect valid examples until |D0\ninduction| = B \u00d7 S.\ninitialize D\n0\nabduction = D\n0\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where B is the batch size, and S = 4 is a factor we fix in all\nexperiments. All seed triplet's program are stripped of global variables and comments (Appendix D), but subsequent iterations of adding\nnew triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we\nsample programs from Dseed, generate matching input sets and messages, and collect valid examples until |D0\ninduction| = B \u00d7 S.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "6\n6",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "table",
            "text": "Algorithm 1Self-Play Training of Absolute Zero Reasoner (AZR)Require:Pretrained base LLM\n\u03c0\u03b8; batch size\nB; #referencesK; iterations\nT1:\nDded,\nDabd,\nDind\u2190 InitSeeding(\u03c0\u03b8)see \u00a73.3.1\n\u25b7to\n2:\n\u2190 1\nfor\nt\nTdo3:\nfor\n\u2190 1\nbto\ndo\nB\u25b7PROPOSE PHASE4:\np\n\u223c Dabd\u222a Ddedsample a program for induction task proposal\n\u25b7N\nn\n5:\n\ni\n\u03c0propose\n(ind, p)\nn=1, m\u03c0\n\u2190 \u03c0\n\u03b8generate\n\u25b7\nNinputs and a descriptionn\n6:\n\n(i\nif\n\u03c0N\nn=1 \u2190 ValidateByExecuting\n, on\n)\np,\n{i\n\u03c0syntax\nn\n\u03c0},\nthen\n\u25b7validate I/Os, see \u00a73.3.37:\nDindn\n, on\n\n(p,\n{(i\n)}, m\u03c0)\n\u2190 Dind\n\u222a\n\u03c0\n\u03c0\u25b7update induction buffer8:\nfor\n\u03b1\u2208 {ded,\nabd}\ndo9:\nK\npk, ik, ok\nk=1 \u223c D\u03b1sample\n\u25b7\nKreference examples10:propose\n\n\n(p\u03c0, i\u03c0)\n{(pk, ik, ok)}\n\u2190 \u03c0\n\u03b1,\n\u03b8\u25b7propose new task11:\nif\u2190 ValidateByExecuting\no\u03c0\np\u03c0, i\u03c0,syntax,safety,determinism\nthensee \u00a73.3.3\n\u25b712:\n(p\u03c0, i\u03c0, o\u03c0)\nD\u03b1 \u2190 D\u03b1 \u222aif valid, update deduction or abduction buffers\n\u25b713:\nfor all\n\u03b1\u2208 {ded,\nabd,\nind}\ndo\u25b7SOLVE PHASE(x, y\u22c6\n14:\u2190 SamplePrepareTasks\nD\u03b1, B, t\n)\u25b7 x, y\u22c6 prepared based on\u03b1, see \u00a73.3.3&amp;3.3.415:\ny\u03c0\n\u223c \u03c0solve\n(x)\n\u03b816:\nReward:Use proposed task triplets and solved answers to get&amp;\nrpropose\nrsolvesee \u00a73.1\n\u25b717:\nRL update:use Task Relative REINFORCE++ to update\u03c0\u03b8see \u00a73.3.5\n\u25b7",
            "level": null,
            "language": null,
            "csv": "",
            "json_data": {
              "headers": [],
              "rows": []
            },
            "breadcrumbs": null,
            "metadata": {
              "extraction_method": "surya",
              "extraction_details": {
                "model": "surya",
                "method": "ml",
                "detection_model": "DetectionPredictor",
                "recognition_model": "RecognitionPredictor",
                "table_rec_model": "TableRecPredictor"
              },
              "quality_score": 0.6157894736842104
            }
          },
          {
            "type": "tablecell",
            "text": "Algorithm 1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Self-Play Training of Absolute Zero Reasoner (AZR)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Require:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Pretrained base LLM\n\u03c0\u03b8; batch size\nB; #references",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "K; iterations\nT",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "1:\nDded,\nDabd,\nDind",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u2190 InitSeeding(\u03c0\u03b8)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "see \u00a73.3.1\n\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "to\n2:\n\u2190 1\nfor\nt\nT",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "do",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "3:\nfor\n\u2190 1\nb",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "to\ndo\nB",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "PROPOSE PHASE",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "4:\np\n\u223c Dabd",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u222a Dded",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "sample a program for induction task proposal\n\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "N\nn\n5:\n\ni\n\u03c0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "propose\n(ind, p)\nn=1, m\u03c0\n\u2190 \u03c0\n\u03b8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "generate\n\u25b7\nN",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "inputs and a description",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "n\n6:\n\n(i\nif\n\u03c0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "N\nn=1 \u2190 ValidateByExecuting\n, on\n)\np,\n{i\n\u03c0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "syntax\nn\n\u03c0},\nthen\n\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "validate I/Os, see \u00a73.3.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "7:\nDind",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "n\n, on\n\n(p,\n{(i\n)}, m\u03c0)\n\u2190 Dind\n\u222a\n\u03c0\n\u03c0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "update induction buffer",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "8:\nfor\n\u03b1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u2208 {ded,\nabd}\ndo",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "9:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "K\npk, ik, ok\nk=1 \u223c D\u03b1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "sample\n\u25b7\nK",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "reference examples",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "10:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "propose\n\n\n(p\u03c0, i\u03c0)\n{(pk, ik, ok)}\n\u2190 \u03c0\n\u03b1,\n\u03b8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "propose new task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "11:\nif",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u2190 ValidateByExecuting\no\u03c0\np\u03c0, i\u03c0,",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "syntax,safety,determinism\nthen",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "see \u00a73.3.3\n\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "12:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "(p\u03c0, i\u03c0, o\u03c0)\nD\u03b1 \u2190 D\u03b1 \u222a",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "if valid, update deduction or abduction buffers\n\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "13:\nfor all\n\u03b1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u2208 {ded,\nabd,\nind}\ndo",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "SOLVE PHASE",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "(x, y\u22c6\n14:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u2190 SamplePrepareTasks\nD\u03b1, B, t\n)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u25b7 x, y\u22c6 prepared based on",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u03b1, see \u00a73.3.3&amp;3.3.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "15:\ny\u03c0\n\u223c \u03c0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "solve\n(x)\n\u03b8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "16:\nReward:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Use proposed task triplets and solved answers to get",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "&amp;\nrpropose\nrsolve",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "see \u00a73.1\n\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "17:\nRL update:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "use Task Relative REINFORCE++ to update",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "\u03c0\u03b8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "see \u00a73.3.5\n\u25b7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "3.3.2. Task Proposal Inputs and Buffer Management",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "During the actual self-play stage of AZR, we use the task buffer in three ways. First, for the proposer of abduction and deduction tasks,\nwe uniformly sample K past triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task.\nThe design is to show it past examples, and prompt it to generate a different one to promote diversity (Zhao et al., 2025a). Second, we\nsample one triplet from the union of abduction and deduction buffers Dabd S\nDded, and present the program p from that triplet to the\ninduction proposer to generate a set of N matching inputs {i\nn\n} and a natural language message m. Lastly, to maintain stable training, if\na batch of solver problems contains fewer than B valid proposed tasks (proposer not adhering to formatting), we fill the remainder by\nuniformly sampling from the corresponding task buffer of previously validated triplets.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "The buffer grows for abduction and deduction tasks whenever \u03c0 propose a valid triplet (p, i, o), regardless if it gets any task reward.\nSimilarly, for induction tasks, all valid triplets (p, {i\nn\n, on\n}), m are added to the buffer.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "3.3.3. Constructing Valid Tasks",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Proposal Task Validation. We first describe how we construct valid tasks from the proposals generated by the policy \u03c0. For deduction\nand abduction tasks, each proposal consists of a program and an input (p, i). To validate the task, we use the task validation procedure\n(steps shown below) on the input to obtain the correct output o, resulting in a complete triplet (p, i, o). For induction tasks, given a\nprogram p the policy proposes a set of inputs {i\nn\n} and message m. We also use the task validation procedure on each of the input i\nn\nin the set to obtain a corresponding output o\nn\n, forming a set of input-output pairs {i\nn\n, on\n}. We do not impose any constraints on m.\nThe resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. The task\nvalidation procedure entails:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.\n2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\nFigures 34 to 36.\n3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p \u2208 Pdeterministic \u2282 P, where P is the space\nof all valid programs and I is the space of all valid inputs:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\nFigures 34 to 36.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p \u2208 Pdeterministic \u2282 P, where P is the space\nof all valid programs and I is the space of all valid inputs:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "\u2200p \u2208 Pdeterministic, \u2200i \u2208 I ,\n\nlim\nj\u2192\u221e\np(i)\n(1) = p(i)\n(2) = \u00b7 \u00b7 \u00b7 = p(i)\n(j)\n\n, (7)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "where (j) indexes repeated independent executions of the program. That is, for all inputs i, the output of p(i) remains identical\nwith any independent execution of the program. A valid program/input/output triplet (p, i, o) is defined such that o = p(i), where\np \u2208 Pdeterministic.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Since the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the\ncorrectness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class\nof deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and\npromising to include in future versions of AZR.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "To implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted in\nEquation (7), we approximate this procedure by independently running the program j finite times and checking that all the outputs\nare equal. For computational budget reasons, we fixed j = 2 for all experiments.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Solving Task Construction. If a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to\npresent part of the triplet to the solver. Specifically, we set x = (p, i)for deduction; x = (p, o)for abduction; and x = ({i\nn\n, on\n}\nN//2\nn=1 , m)\nfor induction, where half of the tests cases and a program description m is used. We use all valid tasks from timestep t; if the batch B is\nnot full, we uniformly sample from previously validated tasks to fill the batch.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "3.3.4. Answer Verification",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "For abduction task, we receive i\u03c0 from the solver policy, then we equivalence match using p(i\u03c0) = p(i\n\u22c6\n), where \u2217 refers to the\nprivileged gold information. The reason we do not just match i\u03c0 and i\n\u22c6\nis because p is not necessarily bijective. For deduction task, we\nmatch o\u03c0 = o\n\u22c6\n. For induction, we match all({p\u03c0(i\n\u22c6\nn) = o\n\u22c6\nn}\nN ). This part might be convoluted to explain in language, therefore we\nrecommend the reader to see how we did abduction, deduction and induction verification in code in Figures 10 to 12, respectively.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "3.3.5. Task-Relative REINFORCE++",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Since AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup (Zhang & Yang, 2021;\nZhao et al., 2022; Wang et al., 2023; Yue et al., 2023). Instead of computing a single global baseline as in REINFORCE++ (Hu, 2025)\n(Appendix A), we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between\nper-question baselines, as in GRPO (Shao et al., 2024), and a global baseline, allowing for more structured variance reduction tailored to\neach task setup. We refer to this variant as Task-Relative REINFORCE++ (TRR++). The normalized advantage A\nnorm is computed as:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "A\nnorm\ntask,role =\nr \u2212 \u00b5task,role\n\u03c3task,role\n, task \u2208 {ind,ded,abd}, role \u2208 {propose,solve}, (8)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "where the mean and standard deviation are computed within each task type and role, yielding six baselines.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "4. Experiments",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "4.1. Experiment Setup",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Training Details. For all experiments, we initialize the buffers as described in Section 3.1. AZR models are trained using a batch\nsize of 64 \u00d7 6 (2 roles \u00d7 3 task types). We use constant learning rate= 1e\u22126 and the AdamW optimizer (Loshchilov & Hutter, 2019).\nComplete list of hyperparameters is provided in Table 3.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "For the main experiments, we train AZR models on Qwen2.5-7B and Qwen2.5-7B-Coder, resulting in Absolute\nZero Reasoner-base-7B and Absolute Zero Reasoner-Coder-7B, respectively. Additional experiments include training\nQwen2.5-Coder-3B, Qwen2.5-Coder-14B, Qwen2.5-14B, Llama-3.1-8B (Yang et al., 2024a; Hui et al., 2024; Dubey et al.,\n2024).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Evaluation Protocol. To evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD)\ncategories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning\nbenchmarks. For coding tasks, we evaluate using Evalplus (Liu et al., 2023) on the HumanEval+ and MBPP+ benchmarks, as\nwell as LiveCodeBench Generation (v1-5, May 23-Feb 25) (Jain et al., 2024). For mathematical reasoning, we utilize six standard\nbenchmarks commonly used in recent zero-shot trained reasoners: AIME'24, AIME'25, OlympiadBench (He et al., 2024), Minerva,\nMath500 (Hendrycks et al., 2021), and AMC'23. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-\nExecution (Gu et al., 2024; Jain et al., 2024), which assess reasoning capabilities regarding the input and output of programs (Li et al.,\n2025). Greedy decoding is used for all baseline methods and AZR results to ensure reproducibility.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "tablegroup",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Table 1. Performance of RL-Trained Reasoner on Reasoning Benchmarks Based on Qwen2.5-7B Models. Performance of various\nmodels is evaluated on three standard code benchmarks (HumanEval+\n, MBPP+\n, LCBv1-5 and six math benchmarks (AIME'24, AIME'25,\nAMC'23, MATH500, Minerva, OlympiadBench). Average performance across coding and math benchmarks is calculated as average of\nthe two averages: AVG = (CAvg + MAvg)/2. We use + for absolute percentage increase from base model. All models are trained using\ndifferent variants of the Qwen2.5-7B model, with the variant and data usage labeled, more details listed in Table 4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Baselines. For our main results, we use Qwen2.5-7B as the base model, along with its specialized base model variants:\nQwen2.5-7B-Coder, Qwen2.5-7B-Instruct, and Qwen2.5-Math-7B (Yang et al., 2024a; Hui et al., 2024; Yang et al., 2024b).\nFurthermore, the zero-style models are usually trained specifically on either code or math data; and only Eurus-2-7B-PRIME-Zero(Cui\net al., 2025) was trained jointly on both domains. For code data models, we present four variants of the AceCoder (Zeng et al., 2025a)\nand two different CodeR1 models (Liu & Zhang, 2025). For math data models, we have Qwen2.5-Math-7B-Oat-Zero (Liu et al.,\n2025), Open-Reasoner-Zero-7B (ORZ) (Hu et al., 2025), Qwen-2.5-7B-SimpleRL-Zoo (Zeng et al., 2025b). All baseline models'\ntraining data and initialization settings are summarized in Table 4. For follow-up scaling experiments, we compare each AZR model\nagainst its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare\nour Llama3.1-8B-trained model with Llama-3.1-8B-SimpleRL-Zoo (Zeng et al., 2025b) and the base model.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "4.2. Results",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Research Question 1: How does AZR compare to other zero setting models trained with human expert\ndata? We present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings\nin Table 1. Notably, Absolute Zero Reasoner-Coder-7B achieves state-of-the-art performance in both the 7B overall average and\nthe coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the\nprevious best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in\nthe coding category by 0.3 absolute percentages, while never having access to such data itself.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Strong Cross-domain Generalization. To assess cross-domain generalization after RLVR, we evaluate math performance before and\nafter training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training,\nmost expert code models showed minimal changes or even declines in performance compared to their base versions, with an average\nincrease of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder\nmodels achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning\nimprovements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2\nand 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Overall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on\nhuman-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream\nhuman-defined math or coding data, only had access to self-proposed tasks during training.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Research Question 2: How do initializing from different base model variants (base vs. coder) affect\nperformance? As shown in Table 1, the coder variant achieved better overall performance in both math and coding after the AZR",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "9",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "figuregroup",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n50 75 100 125 150 175 200 225 250\nTraining Steps\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nIn-Distribution\n \nAccuracy\n\nAZR-Llama3.1-8b\nAZR-3B-Coder\nAZR-7B-Coder\nAZR-14B-Coder\n(a)\nModel Family Variant Code Avg Math Avg Total Avg\nLlama3.1-8b 28.5 3.4 16.0\nLlama3.1-8b + SimpleRL[85]\n33.7\n+5.2\n7.2\n+3.8\n20.5\n+4.5\nLlama3.1-8b + AZR (Ours) 31.6\n+3.1\n6.8\n+3.4\n19.2\n+3.2\nQwen2.5-3B Coder 51.2 18.8 35.0\nQwen2.5-3B Coder + AZR (Ours) 54.9\n+3.7\n26.5\n+7.7\n40.7\n+5.7\nQwen2.5-7B Coder 56.6 23.9 40.2\nQwen2.5-7B Coder + AZR (Ours) 61.6\n+5.0\n39.1\n+15.2\n50.4\n+10.2\nQwen2.5-14B Coder 60.0 20.2 40.1\nQwen2.5-14B Coder + AZR (Ours) 63.6\n+3.6\n43.0\n+22.8\n53.3\n+13.2\n(b)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "50 75 100 125 150 175 200 225 250\nTraining Steps\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nIn-Distribution\n \nAccuracy\n\nAZR-Llama3.1-8b\nAZR-3B-Coder\nAZR-7B-Coder\nAZR-14B-Coder\n(a)\nModel Family Variant Code Avg Math Avg Total Avg\nLlama3.1-8b 28.5 3.4 16.0\nLlama3.1-8b + SimpleRL[85]\n33.7\n+5.2\n7.2\n+3.8\n20.5\n+4.5\nLlama3.1-8b + AZR (Ours) 31.6\n+3.1\n6.8\n+3.4\n19.2\n+3.2\nQwen2.5-3B Coder 51.2 18.8 35.0\nQwen2.5-3B Coder + AZR (Ours) 54.9\n+3.7\n26.5\n+7.7\n40.7\n+5.7\nQwen2.5-7B Coder 56.6 23.9 40.2\nQwen2.5-7B Coder + AZR (Ours) 61.6\n+5.0\n39.1\n+15.2\n50.4\n+10.2\nQwen2.5-14B Coder 60.0 20.2 40.1\nQwen2.5-14B Coder + AZR (Ours) 63.6\n+3.6\n43.0\n+22.8\n53.3\n+13.2\n(b)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 6. (a) In-Distribution & (b) Out-of-Distribution Reasoning Task Performances. (a) Scores on CruxEval-I, CruxEval-O,\nand LiveCodeBench-Execution, which correspond to abduction, deduction, and deduction task types respectively, used to evaluate\nin-distribution abilities of AZR during training across different model sizes and types; (b) Out-of-distribution reasoning performance,\nreported as the average of code tasks, math tasks, and their overall average, across different model sizes and types. A detailed breakdown\nof all benchmark results can be found in Table 5.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "self-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla\nbase model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency\nas a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Research Question 3: How does varying model size effect AZR's in-distribution and out-of-distribution\ncapabilities? We examine the effects of scaling model size and present both in-distribution and out-of-distribution results in Figure 6\n(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller\nand larger variants: Qwen2.5-3B-Coder and Qwen2.5-14B-Coder. Due to the absence of existing baselines for these zero-style\nreasoner models, we compare each model's performance to its corresponding base coder model.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "The results reveal a clear trend: our method delivers greater gains on larger, more capable models. In the in-distribution setting, the 7B\nand 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution\ndomains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance\ngains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that\nscaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute\nZero paradigm.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Research Question 4: Any interesting observations by changing the model class? We also evaluate our method\non a different model class, using Llama3.1-8B as the base shown in Figure 6. Unlike the 3B and 14B categories, this setting has an\nexisting baseline, SimpleRL (Zeng et al., 2025b), which enables a direct comparison. Although Llama3.1-8B is less capable than\nthe Qwen2.5 models, our method still produces moderate improvements (+3.2), demonstrating AZR's effectiveness even on relatively\nweaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements\ntend to scale with initial base model potency.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Research Question 5: Any interesting behaviors or patterns observed during AZR training? We observed\ninteresting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as\nstring manipulation tasks, dynamic programming problems, and practical cases (e.g., calculating a triangle's area using Heron's formula).\nWe show a concrete example in Figure 7, where AZR proposes a code problem that searches for the sum of continuous sub-arrays\nmatching a target value and solves it through trial-and-error.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Overall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks,\nit repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs,\nit steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is\nreached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to\nconfirm that its program produces correct results. We showcase more concrete examples of these behaviors in Figures 18 and 20 to 26.\nWe also share some fun \"vibe checks\" such as solving Sudoku and solving the sum-product game in Figures 40 and 41.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Intermediate Planning During Code Response. Another interesting pattern emerged in our AZR models during the code induction\ntask: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct\nprompting framework (Yao et al., 2023). A similar behavior has been observed in recent formal math proving models, such as DeepSeek",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "10",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Input: [1,2,3,4,5], 5\nModel-proposed Task Response of Predict Input\nLet's analyze the code snippet \u2026(analysis) To find one possible input that produces \nthe output 1, we need to find a scenario where \u2026(analysis) \nLet's consider a simple example: numbers = [1, 2], target = 1\nHere's the step-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\nSince no pair satisfies the condition, the function will not return any value. This \ndoesn't match our output. Let's try another example \u2026(Tried 3 more examples) \nGiven the complexity and the fact that no simple example seems to work, let's \ntry a more complex scenario: numbers = [2, -1, 1], target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1 , accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n- Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "- Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n- Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "The pair (i=1, j=2) satisfies the condition, and the function returns the first \naccumulated value in this pair, which is 1. This matches our output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Output: 1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 7. Example of a Model-Proposed Task and Its Response for Solving an Abduction Task. (Left) The model autonomously\nproposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output.\n(Right) The model's reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input.\nThe model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. If there is a\nmismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. Interestingly, the\nagent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Prover v2, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning\nas a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior in long-form\nresponses across other domains.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Cognitive Behavior in Llama. Interestingly, we also observed some emergent cognitive patterns in Absolute Zero\nReasoner-Llama3.1-8B, similar to those reported by Zeng et al. (2025b), and we include one example in Figure 26, where\nclear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought\nfrom the Llama model trained with AZR. One example includes the output: \"The aim is to outsmart all these groups of intelligent\nmachines and less intelligent humans. This is for the brains behind the future\" shown in Figure 32. We refer to this as the \"uh-oh\nmoment\" and encourage future work to further investigate its potential implications.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Token Length Increase Depends on Task Type. Finally, we observed that token length increases over the course of training, consistent\nwith findings from recent studies (Hu et al., 2025; Liu et al., 2025). Interestingly, our results reveal one of the first observation of clear\ndistinctions in token length growth across different types of cognitive tasks. As shown in Figures 15 to 17, the extent of lengthening\nvaries by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by\nrepeatedly testing inputs to match the program's output. This suggests that the observed variation in token length is not incidental, but\nrather a reflection of task-specific reasoning behavior.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Research Question 6: Are all task types essential for good performance (Ablation)? Due to resource constraints,\nwe perform the ablation studies in this section and the next using only Absolute Zero Reasoner-Base-7B. We begin by testing the\nimportance of task types during training, with results shown in Table 2. In row 1, both induction and abduction tasks are removed;\nin row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation\noccurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving\ngeneral reasoning capability, with each contributing in a distinct and essential way.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Research Question 7: How much do the designs of proposer contribute to the overall performance\n(Ablation)? Next, we ablate two components of the proposer role and present the results in Table 2. First, we examine whether\nconditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction\nand deduction tasks, rather than dynamically conditioning on K historical triplets (row 3). This results in a 5-point absolute drop in\nmath performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "11",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "table",
            "text": "ExperimentTask TypeGen ReferenceTrained RolesCode Avg.Math Avg.Overall Avg.Deduction onlyDed//54.632.043.3w/o InductionAbd, Ded//54.233.343.8w/o Gen Reference/0/54.433.143.8Train Solver Only//Solve Only54.836.045.4OursAbd, Ded, IndKPropose &amp; Solve55.238.446.8",
            "level": null,
            "language": null,
            "csv": "",
            "json_data": {
              "headers": [],
              "rows": []
            },
            "breadcrumbs": null,
            "metadata": {
              "extraction_method": "surya",
              "extraction_details": {
                "model": "surya",
                "method": "ml",
                "detection_model": "DetectionPredictor",
                "recognition_model": "RecognitionPredictor",
                "table_rec_model": "TableRecPredictor"
              },
              "quality_score": 0.675
            }
          },
          {
            "type": "tablecell",
            "text": "Experiment",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Task Type",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Gen Reference",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Trained Roles",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Code Avg.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Math Avg.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Overall Avg.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Deduction only",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Ded",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "/",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "/",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "54.6",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "32.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "43.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "w/o Induction",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Abd, Ded",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "/",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "/",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "54.2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "33.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "43.8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "w/o Gen Reference",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "/",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "/",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "54.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "33.1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "43.8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Train Solver Only",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "/",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "/",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Solve Only",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "54.8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "36.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "45.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Ours",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Abd, Ded, Ind",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "K",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Propose &amp; Solve",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "55.2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "38.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "46.8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Table 2. Ablation Results. We ablate task types and the proposer role in the Absolute Zero Reasoner using the 7B base model. A '/'\nindicates that the configuration remains unchanged from the standard AZR setup. Removing induction or using only deduction leads to\nsignificant performance drops (rows 1 & 2). For the proposer role, both removing conditioning on K references (row 3) and omitting\nproposer-role training (row 4) result in degraded performance. Overall, all components are essential for general reasoning.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "improve performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Finally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the\nsolver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it\nmay not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as\nstudied in multitask learning literature (Suteu & Guo, 2019). Thus, we believe that further investigation into how to make the proposer\neven more potent is an exciting and promising direction.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Additional Results. Beyond the core research questions, we present additional results, including the breakdown of individual\nout-of-distribution benchmark scores during training for the 7B base and coder models in Figures 28 and 29, for th 14B base and coder\nmodel in Figures 30 and 31. For completeness, we also report in-distribution benchmark performance during training for the 7B base\nmodel in Figure 14. Finally, we invite interested readers to explore Appendix D, where we share several experimental directions that,\nwhile not yielding strong performance gains, produced interesting and insightful findings.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "5. Related Work",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Reasoning with RL. Using RL to enhance reasoning capabilities has recently emerged as an important step in the post-training\nprocess of strong reasoning-focused large language models (Lambert et al., 2024). One of the first works to explore a self-bootstrapping\napproach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to\niteratively improve the model's CoT. A monumental work, o1 (Jaech et al., 2024), was among the first to deploy this idea on a scale,\nachieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model (DeepSeek-AI et al., 2025)\nbecame the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in\nwhich reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to\nreplicate the R1 process or to improve the underlying reinforcement learning algorithm (Zeng et al., 2025b; Liu et al., 2025; Cui et al.,\n2025; Hu et al., 2025; Yu et al., 2025; Yuan et al., 2025). Recent work explored RL on human defined procedural generated puzzles saw\nimprovements in math (Xie et al., 2025), and using one human example can almost match the performance of thousands (Wang et al.,\n2025b). We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM\nwithout SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed,\nand refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Self-play. The self-play paradigm can be traced back to early 2000s, where Schmidhuber (2003; 2011) (of course) explored a\ntwo-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically\nimproves both agents, enabling theoretically never-ending progress (Schaul, 2024). AlphaGo and AlphaZero (Silver et al., 2016; 2017)\nextend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of\nitself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance\nin the game of Go. Moreover, methods such as asymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), unsupervised\nenvironment design (Wang et al., 2019; Dennis et al., 2020), unsupervised reinforcement learning (Laskin et al., 2021; Zhao et al., 2022;\n2025b), and automatic goal generation (Florensa et al., 2018) all center around inventing new tasks for an agent to learn from\u2014typically\nwithout supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative\nadversarial networks (Goodfellow et al., 2020), also belong in this paradigm where a discriminator discriminate between real data and\ngenerated data, and the generated is trained to fool the discriminator.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Most recently, SPIN and Self-Rewarding Language Models (Chen et al., 2024; Yuan et al., 2024) use the same instance of the lanugage\nmodels themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for\nalignment. (Kirchner et al., 2024) uses Prover-Verifier Game for increasing legibility and eva (Ye et al., 2024) uses self-play for\nalignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks (Lambert et al., 2024). SPC (Chen et al.,",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "12",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "2025) used self-play to train on human-curated tasks to increase the critic capabilities and SPAG (Cheng et al., 2024) trained using\nself-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL (Xu et al., 2025; Zhang et al., 2025b;\nZuo et al., 2025)\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a\nfixed human defined learning task distribution. Finally, Minimo (Poesia et al., 2024) extends self-play to formal mathematics, where a\npair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play\nparadigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python\ninput/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Weak-to-Strong Supervision. The concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite\nbeing weaker than the learner\u2014still provides useful guidance (Burns et al., 2024; Hinton et al., 2015; Christiano, 2018; 2019; Demski &\nGarrabrant, 2019; Leike & Sutskever, 2023; Hubinger et al., 2019). We consider a similar setting in which the learner may possess\nsuperhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach:\nguiding the learner's improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal.\nFurthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are\nentirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "6. Conclusion and Discussion",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Conclusion. In this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing\nRLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving\ntheir reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR),\nwhich is trained by having them propose and solve code-related reasoning tasks grounded by code executor.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains.\nRemarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning\nagents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This\ndemonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive\ndomain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model\nsizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging\nparadigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Discussion. We believe there remains much to explore, such as altering the environment from which the reasoner receives verifiable\nfeedback, including sources like the world wide web, formal math languages (Sutton, 2001; Ren et al., 2025), world simulators, or even\nthe real world. Furthermore, AZ's generality could possibly be extend to domains such as embodied AI (Zitkovich et al., 2023; Yue\net al., 2024). Additionally, more complex agentic tasks or scientific experiments, present exciting opportunities to further advance the\nabsolute zero setting to different application domains (Wu et al., 2024; 2023). Beyond that, future directions could include exploring\nmultimodal reasoning models, modifying the distribution p(z) to incorporate privileged information, defining or even let the model\ndynamically learn how to define f (Equation (3)), or designing exploration/diversity rewards for both the propose and solve roles.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "While underappreciated in current reasoning literature, the exploration component of RL has long been recognized as a critical driver for\nemergent behavior in traditional RL (Yue et al., 2025; Silver et al., 2016; Ladosz et al., 2022). Years of research have examined various\nforms of exploration, even in related subfields using LLMs such as red teaming (Zhao et al., 2025a), yet its role in LLM reasoning\nmodels remains underexplored. Taking this a step further, our framework investigates an even more meta-level exploration problem:\nexploration within the learning task space\u2014where the agent learns not just how to solve tasks, but what tasks to learn from and how to\nfind them. Rather than being confined to a fixed problem set, AI reasoner agents may benefit from dynamically defining and refining\ntheir own learning tasks. This shift opens a powerful new frontier\u2014where agents explore not only solution spaces but also expand the\nboundaries of problem spaces. We believe this is a promising and important direction for future research.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "One limitation of our work is that we did not address how to safely manage a system composed of such self-improving components.\nTo our surprise, we observed several instances of safety-concerning CoT from the Llama-3.1-8B model, which we term the \"uh-oh\nmoment\". These findings suggest that the proposed absolute zero paradigm, while reducing the need for human intervention for curating\ntasks, still necessitates oversight due to lingering safety concerns and is a critical direction for future research (Wang et al., 2024; 2025a).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "As a final note, we explored reasoning models that possess experience\u2014models that not only solve given tasks, but also define and\nevolve their own learning task distributions with the help of an environment. Our results with AZR show that this shift enables strong\nperformance across diverse reasoning tasks, even with significantly fewer privileged resources, such as curated human data. We believe\nthis could finally free reasoning models from the constraints of human-curated data (Morris, 2025) and marks the beginning of a new\nchapter for reasoning models: \"welcome to the era of experience\" (Silver & Sutton, 2025; Zhao et al., 2024).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "13",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "References",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "Aryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\nto code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\nhttps://doi.org/10.48550/arXiv.2408.10914.\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\nhttps://openreview.net/forum?id=ghNRg2mEgN.\nCanal, M. Radon: Python tool for code metrics. https://github.com/rubik/radon, 2023. Accessed: 2025-04-06.\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\nfor llm reasoning, 2025. URL https://arxiv.org/abs/2504.19162.\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\nURL https://openreview.net/forum?id=O4cHTxW9BS.\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html.\nChristiano, P. Approval-directed bootstrapping. https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\napproval-directed-bootstrapping, 2018. AI Alignment Forum.\nChristiano, P. Capability amplification. https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\ncapability-amplification-1, 2019. AI Alignment Forum.\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\nCoRR, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL https://doi.org/10.48550/arXiv.2502.01456.\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.\nDemski, A. and Garrabrant, S. Embedded agency. CoRR, abs/1902.09469, 2019. URL http://arxiv.org/abs/1902.09469.\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n985e9a46e10005356bbaf194249f6856-Abstract.html.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Aryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\nto code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\nhttps://doi.org/10.48550/arXiv.2408.10914.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\nhttps://openreview.net/forum?id=ghNRg2mEgN.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Canal, M. Radon: Python tool for code metrics. https://github.com/rubik/radon, 2023. Accessed: 2025-04-06.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Chen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\nfor llm reasoning, 2025. URL https://arxiv.org/abs/2504.19162.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\nURL https://openreview.net/forum?id=O4cHTxW9BS.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Cheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Christiano, P. Approval-directed bootstrapping. https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\napproval-directed-bootstrapping, 2018. AI Alignment Forum.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Christiano, P. Capability amplification. https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\ncapability-amplification-1, 2019. AI Alignment Forum.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Cui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\nCoRR, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL https://doi.org/10.48550/arXiv.2502.01456.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Demski, A. and Garrabrant, S. Embedded agency. CoRR, abs/1902.09469, 2019. URL http://arxiv.org/abs/1902.09469.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Dennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n985e9a46e10005356bbaf194249f6856-Abstract.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "14",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "K., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL\nhttps://doi.org/10.48550/arXiv.2407.21783.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Ebert, C., Cain, J., Antoniol, G., Counsell, S., and Laplante, P. Cyclomatic complexity. IEEE software, 33(6):27\u201329, 2016.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "Florensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\nKrause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\nStockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1514\u20131523. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/florensa18a.html.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\nadversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\nunderstanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Florensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\nKrause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\nStockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1514\u20131523. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/florensa18a.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\nadversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Gu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\nunderstanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Halstead, M. H. Elements of Software Science (Operating and programming systems series). Elsevier Science Inc., 1977.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\nL., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 3828\u20133850. Association for Computational\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neu-\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL\nhttp://arxiv.org/abs/1503.02531.\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi:\n10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\nreinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https:\n//doi.org/10.48550/arXiv.2503.24290.\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\nsystems. CoRR, abs/1906.01820, 2019. URL http://arxiv.org/abs/1906.01820.\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\nOpen-endedness is essential for artificial superhuman intelligence. In Forty-first International Conference on Machine Learning,\nICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Bc4vZ2CX7E.\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\nhttps://doi.org/10.48550/arXiv.2409.12186.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\nsystem card. arXiv preprint arXiv:2412.16720, 2024.\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\ncontamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\nURL https://doi.org/10.48550/arXiv.2403.07974.\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\nCoRR, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL https://doi.org/10.48550/arXiv.2407.13692.\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85:1\u201322, 2022. doi:\n10.1016/J.INFFUS.2022.03.003. URL https://doi.org/10.1016/j.inffus.2022.03.003.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\nL., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 3828\u20133850. Association for Computational\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neu-\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Hinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL\nhttp://arxiv.org/abs/1503.02531.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Hu, J. REINFORCE++: A simple and efficient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi:\n10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Hu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\nreinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https:\n//doi.org/10.48550/arXiv.2503.24290.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Hubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\nsystems. CoRR, abs/1906.01820, 2019. URL http://arxiv.org/abs/1906.01820.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Hughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\nOpen-endedness is essential for artificial superhuman intelligence. In Forty-first International Conference on Machine Learning,\nICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Bc4vZ2CX7E.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\nhttps://doi.org/10.48550/arXiv.2409.12186.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\nsystem card. arXiv preprint arXiv:2412.16720, 2024.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Jain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\ncontamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\nURL https://doi.org/10.48550/arXiv.2403.07974.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Kirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\nCoRR, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL https://doi.org/10.48550/arXiv.2407.13692.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Ladosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85:1\u201322, 2022. doi:\n10.1016/J.INFFUS.2022.03.003. URL https://doi.org/10.1016/j.inffus.2022.03.003.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "15",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\nH. T\u00fclu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\nURL https://doi.org/10.48550/arXiv.2411.15124.\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural In-\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\ncember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html.\nLeike, J. and Sutskever, I. Introducing superalignment. https://openai.com/index/introducing-superalignment/, 2023.\nOpenAI Blog.\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction. CoRR,\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL https://doi.org/10.48550/arXiv.2502.07316.\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\nCoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852.\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. GitHub, 2025.\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\nlanguage models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=1qvx610Cu7.\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\nCoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783.\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n2025. URL https://github.com/rohaquinlop/complexipy. Accessed: 2025-04-06.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\nMorris, J. There are no new ideas in ai. . . only new datasets. https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only,\n2025.\nOpenAI. Openai o3-mini, January 2025a. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-04-17.\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/.\nAccessed: 2025-04-17.\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\nmanipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\nlanguage models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744,\n2022.\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural In-\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\ncouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI\nblog, 1(8):9, 2019.\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\nfor subgoal decomposition, 2025. URL https://arxiv.org/abs/2504.21801.\nSchaul, T. Boundless socratic learning with language games. arXiv preprint arXiv:2411.16905, 2024.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\nH. T\u00fclu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\nURL https://doi.org/10.48550/arXiv.2411.15124.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Laskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural In-\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\ncember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Leike, J. and Sutskever, I. Introducing superalignment. https://openai.com/index/introducing-superalignment/, 2023.\nOpenAI Blog.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Li, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction. CoRR,\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL https://doi.org/10.48550/arXiv.2502.07316.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Li, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\nCoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Liu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. GitHub, 2025.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\nlanguage models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=1qvx610Cu7.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\nCoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Lopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n2025. URL https://github.com/rohaquinlop/complexipy. Accessed: 2025-04-06.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Morris, J. There are no new ideas in ai. . . only new datasets. https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only,\n2025.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "OpenAI. Openai o3-mini, January 2025a. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-04-17.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "OpenAI. Introducing openai o3 and o4-mini, April 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/.\nAccessed: 2025-04-17.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "OpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\nmanipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\nlanguage models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744,\n2022.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Poesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural In-\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\ncouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI\nblog, 1(8):9, 2019.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Ren, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\nfor subgoal decomposition, 2025. URL https://arxiv.org/abs/2504.21801.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Schaul, T. Boundless socratic learning with language games. arXiv preprint arXiv:2411.16905, 2024.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "16",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "Schmidhuber, J. Exploring the predictable. In Advances in evolutionary computing: theory and applications, pp. 579\u2013612. Springer,\n2003.\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\nproblem. CoRR, abs/1112.5309, 2011. URL http://arxiv.org/abs/1112.5309.\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\nof mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\nhttps://doi.org/10.48550/arXiv.2402.03300.\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\nRLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\nThe Netherlands, 30 March 2025 - 3 April 2025, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https:\n//doi.org/10.1145/3689031.3696075.\nSilver, D. and Sutton, R. S. The era of experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%\n20/The%20Era%20of%20Experience%20Paper.pdf, 2025.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484\u2013489, 2016.\ndoi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR,\nabs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\nStuart, T. Understanding computation - from simple machines to impossible programs. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\nhttp://www.oreilly.de/catalog/9781449329273/index.html.\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\nasymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients. CoRR, abs/1912.06844, 2019. URL\nhttp://arxiv.org/abs/1912.06844.\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\nConference session, December 2024. URL https://neurips.cc/virtual/2024/test-of-time/105032.\nSutton, R. S. Verification, the key to ai. http://incompleteideas.net/IncIdeas/KeytoAI.html, 2001.\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms. CoRR,\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\nbased on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ViZcgDQjyG.\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\nsimple parameter editing. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\nComputational Linguistics, pp. 6337\u20136357, 2025a.\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\nand diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019. URL http://arxiv.org/abs/1901.01753.\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\nbalances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nURL https://openreview.net/forum?id=vtoY8qJjTR.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Schmidhuber, J. Exploring the predictable. In Advances in evolutionary computing: theory and applications, pp. 579\u2013612. Springer,\n2003.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Schmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\nproblem. CoRR, abs/1112.5309, 2011. URL http://arxiv.org/abs/1112.5309.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\nof mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\nhttps://doi.org/10.48550/arXiv.2402.03300.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\nRLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\nThe Netherlands, 30 March 2025 - 3 April 2025, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https:\n//doi.org/10.1145/3689031.3696075.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Silver, D. and Sutton, R. S. The era of experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%\n20/The%20Era%20of%20Experience%20Paper.pdf, 2025.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484\u2013489, 2016.\ndoi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR,\nabs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Stuart, T. Understanding computation - from simple machines to impossible programs. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\nhttp://www.oreilly.de/catalog/9781449329273/index.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Sukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\nasymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Suteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients. CoRR, abs/1912.06844, 2019. URL\nhttp://arxiv.org/abs/1912.06844.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Sutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\nConference session, December 2024. URL https://neurips.cc/virtual/2024/test-of-time/105032.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Sutton, R. S. Verification, the key to ai. http://incompleteideas.net/IncIdeas/KeytoAI.html, 2001.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms. CoRR,\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Villalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\nbased on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ViZcgDQjyG.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Wang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\nsimple parameter editing. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\nComputational Linguistics, pp. 6337\u20136357, 2025a.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Wang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\nand diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019. URL http://arxiv.org/abs/1901.01753.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Wang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\nbalances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nURL https://openreview.net/forum?id=vtoY8qJjTR.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "17",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "Wang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for\nComputational Linguistics: ACL 2024, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.findings-acl.591. URL https://aclanthology.org/2024.findings-acl.591/.\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\nhttps://arxiv.org/abs/2504.20571.\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\nLLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\nhttps://doi.org/10.48550/arXiv.2308.08155.\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows. CoRR,\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL https://doi.org/10.48550/arXiv.2403.11322.\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\nreasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\nhttps://doi.org/10.48550/arXiv.2502.14768.\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\nself-training framework for advanced reasoning, 2025. URL https://arxiv.org/abs/2504.08672.\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\nCoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.\ndoi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. CoRR,\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL https://doi.org/10.48550/arXiv.2411.00062.\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\nCoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476.\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. URL https://arxiv.\norg/abs/2401.10020, 2024.\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\nlearning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\nAdvances in Neural Information Processing Systems, 36:60247\u201360277, 2023.\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\nZhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html.\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\ncapacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837.\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information\nProcessing Systems, 35:15476\u201315488, 2022.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Wang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for\nComputational Linguistics: ACL 2024, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.findings-acl.591. URL https://aclanthology.org/2024.findings-acl.591/.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Wang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\nhttps://arxiv.org/abs/2504.20571.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\nLLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\nhttps://doi.org/10.48550/arXiv.2308.08155.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Wu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows. CoRR,\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL https://doi.org/10.48550/arXiv.2403.11322.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Xie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\nreasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\nhttps://doi.org/10.48550/arXiv.2502.14768.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Xu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\nself-training framework for advanced reasoning, 2025. URL https://arxiv.org/abs/2504.08672.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\nCoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.\ndoi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Ye, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. CoRR,\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL https://doi.org/10.48550/arXiv.2411.00062.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\nCoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Yuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. URL https://arxiv.\norg/abs/2401.10020, 2024.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Yuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\nlearning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Yue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\nAdvances in Neural Information Processing Systems, 36:60247\u201360277, 2023.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Yue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\nZhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\ncapacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information\nProcessing Systems, 35:15476\u201315488, 2022.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "18",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listgroup",
            "text": "Zeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis. CoRR,\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL https://doi.org/10.48550/arXiv.2502.01718.\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\nlearning for open base models in the wild. CoRR, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL https:\n//doi.org/10.48550/arXiv.2503.18892.\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\non replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\nincentivization, 2025b. URL https://arxiv.org/abs/2504.05812.\nZhang, Y. and Yang, Q. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609,\n2021.\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Pro-\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html.\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\nJ. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\nURL https://doi.org/10.1609/aaai.v38i17.29936.\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.), AAAI-25, Sponsored by the Association for the\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 26021\u201326030. AAAI Press, 2025a.\ndoi: 10.1609/AAAI.V39I24.34797. URL https://doi.org/10.1609/aaai.v39i24.34797.\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning. Neural\nNetworks, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL https://doi.org/10.1016/j.neunet.2025.\n107448.\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research,\npp. 2165\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html.\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n2025. URL https://arxiv.org/abs/2504.16084.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis. CoRR,\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL https://doi.org/10.48550/arXiv.2502.01718.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\nlearning for open base models in the wild. CoRR, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL https:\n//doi.org/10.48550/arXiv.2503.18892.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\non replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\nincentivization, 2025b. URL https://arxiv.org/abs/2504.05812.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zhang, Y. and Yang, Q. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609,\n2021.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Pro-\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\nJ. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\nURL https://doi.org/10.1609/aaai.v38i17.29936.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.), AAAI-25, Sponsored by the Association for the\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 26021\u201326030. AAAI Press, 2025a.\ndoi: 10.1609/AAAI.V39I24.34797. URL https://doi.org/10.1609/aaai.v39i24.34797.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning. Neural\nNetworks, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL https://doi.org/10.1016/j.neunet.2025.\n107448.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research,\npp. 2165\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "listitem",
            "text": "Zuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n2025. URL https://arxiv.org/abs/2504.16084.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "19",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Appendix",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Appendix Contents",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "20",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "A. Reinforcement Learning with Verifiable Rewards.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We use reinforcement learning to update our learner LLM, rewarding it based on a task-specific reward function rf , where the subscript\nf indicates the task. The goal of the RL agent is to maximize the expected discounted sum of rewards. We adopt an online variant of RL,\nREINFORCE++, which is optimized using the original PPO objective:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "LPPO(\u03b8) = Eq\u223cP (Q), o\u223c\u03c0\u03b8old (O|q)\n\"\n1\n|o|\nX\n|o|\nt=1\nmin \nst(\u03b8)A\nnorm\nf,q , clip (st(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) A\nnorm\nf,q \n#\n, (9)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "where st(\u03b8) is the probability ratio between the new and old policies at timestep t, and A\nnorm\nf,q is the normalized advantage.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "REINFORCE++ computes the normalized advantage as:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "A\nnorm\nf,q =\nrf,q \u2212 mean({Af,q}\nB)\nstd({Af,q}B)\n, (10)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "where rf,q is the outcome reward for question q, task f, mean and std are calculated across the global batch with batch size B. Note that\nwe do not apply any KL penalty to the loss or reward.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "B. Implementation Details",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We built Absolute Zero Reasoner upon the veRL codebase (Sheng et al., 2025). For code execution, we incorporated components from\nthe QwQ Python executor. For safer code execution, we recommend using API-based services such as E2B instead.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "All experiments were conducted on clusters of A800 GPUs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Training Hyperparameters. We show the hyperparameters used in our training in Table 3. We do not change them for any of\nthe runs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "table",
            "text": "ParameterValueModel ConfigurationMax Prompt Length6144Max Response Length8096Seed Batch Factor4Max Programs16384Training SettingsTrain Batch Size64 * 6Learning Rate1e-6OptimizerAdamWGrad Clip1.0Total Steps500RL SettingsAlgorithmTRR++ (Section\n3.3.5)KL LossFalseKL RewardFalseEntropy Coefficient0.001PPO Epochs1Rollouts\nN1Rollout Temperature1.0Rollout Top-P1.0References\nK6Samples to Estimate Task Accuracy\nN8",
            "level": null,
            "language": null,
            "csv": "",
            "json_data": {
              "headers": [],
              "rows": []
            },
            "breadcrumbs": null,
            "metadata": {
              "extraction_method": "surya",
              "extraction_details": {
                "model": "surya",
                "method": "ml",
                "detection_model": "DetectionPredictor",
                "recognition_model": "RecognitionPredictor",
                "table_rec_model": "TableRecPredictor"
              },
              "quality_score": 0.6622665534804754
            }
          },
          {
            "type": "tablecell",
            "text": "Value",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Parameter",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Model Configuration",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Max Prompt Length",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "6144",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Max Response Length",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "8096",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Seed Batch Factor",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Max Programs",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "16384",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Training Settings",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Train Batch Size",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "64 * 6",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Learning Rate",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "1e-6",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Optimizer",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "AdamW",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "1.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Grad Clip",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Total Steps",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "500",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "RL Settings",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Algorithm",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "TRR++ (Section\n3.3.5)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "KL Loss",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "False",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "KL Reward",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "False",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Entropy Coefficient",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0.001",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "PPO Epochs",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Rollouts\nN",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Rollout Temperature",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "1.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Rollout Top-P",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "1.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "References\nK",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "6",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Samples to Estimate Task Accuracy\nN",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Table 3. Hyperparameters Used During AZR Self-play Training.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "21",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "tablegroup",
            "text": "ModelData CurationBase ModelOat-7B (Liu et al.,\n2025)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-MathSimpleRL-Zoo (Zeng et al.,\n2025b)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-BaseOpenReasonerZero (Hu et al.,\n2025)57k STEM + math samplesQwen2.5-7B-BasePRIME-Zero (Cui et al.,\n2025)457k math + 27k code problemsQwen2.5Math-7B-BaseCodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)2k Leetcode pairsQwen2.5-7B-Instruct-1MCodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)2k Leetcode + 10k TACO pairs (Li et al.,\n2023)Qwen2.5-7B-Instruct-1MAceCoder-7B-Ins-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Code-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderAceCoder-7B-Code-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderQwen-7B-Instruct (Yang et al.,\n2024a)1M SFT + 150k RL pairsQwen2.5-7B-BaseAZR-7B (Ours)No dataQwen2.5-7B-BaseAZR-7B-Coder (Ours)No dataQwen2.5-7B-CoderTable 4. Reasoner Training Data Source and Base Model.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "table",
            "text": "ModelData CurationBase ModelOat-7B (Liu et al.,\n2025)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-MathSimpleRL-Zoo (Zeng et al.,\n2025b)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-BaseOpenReasonerZero (Hu et al.,\n2025)57k STEM + math samplesQwen2.5-7B-BasePRIME-Zero (Cui et al.,\n2025)457k math + 27k code problemsQwen2.5Math-7B-BaseCodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)2k Leetcode pairsQwen2.5-7B-Instruct-1MCodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)2k Leetcode + 10k TACO pairs (Li et al.,\n2023)Qwen2.5-7B-Instruct-1MAceCoder-7B-Ins-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Code-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderAceCoder-7B-Code-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderQwen-7B-Instruct (Yang et al.,\n2024a)1M SFT + 150k RL pairsQwen2.5-7B-BaseAZR-7B (Ours)No dataQwen2.5-7B-BaseAZR-7B-Coder (Ours)No dataQwen2.5-7B-Coder",
            "level": null,
            "language": null,
            "csv": "",
            "json_data": {
              "headers": [],
              "rows": []
            },
            "breadcrumbs": null,
            "metadata": {
              "extraction_method": "surya",
              "extraction_details": {
                "model": "surya",
                "method": "ml",
                "detection_model": "DetectionPredictor",
                "recognition_model": "RecognitionPredictor",
                "table_rec_model": "TableRecPredictor"
              },
              "quality_score": 0.675
            }
          },
          {
            "type": "tablecell",
            "text": "Base Model",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Model",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Data Curation",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Math",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Oat-7B (Liu et al.,\n2025)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "8.5k math pairs (Hendrycks et al.,\n2021)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "SimpleRL-Zoo (Zeng et al.,\n2025b)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "8.5k math pairs (Hendrycks et al.,\n2021)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Base",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "OpenReasonerZero (Hu et al.,\n2025)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "57k STEM + math samples",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Base",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "PRIME-Zero (Cui et al.,\n2025)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "457k math + 27k code problems",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5Math-7B-Base",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "CodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "2k Leetcode pairs",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Instruct-1M",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "CodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "2k Leetcode + 10k TACO pairs (Li et al.,\n2023)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Instruct-1M",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "AceCoder-7B-Ins-RM (Zeng et al.,\n2025a)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "22k code data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Instruct",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "AceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "22k code data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Instruct",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "AceCoder-7B-Code-RM (Zeng et al.,\n2025a)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "22k code data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Coder",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "AceCoder-7B-Code-Rule (Zeng et al.,\n2025a)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "22k code data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Coder",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen-7B-Instruct (Yang et al.,\n2024a)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "1M SFT + 150k RL pairs",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Base",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "AZR-7B (Ours)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "No data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Base",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "AZR-7B-Coder (Ours)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "No data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-7B-Coder",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Table 4. Reasoner Training Data Source and Base Model.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "table",
            "text": "loggingrandommultiprocessingpebblesubprocessthreadingdatetimetimehashlibcalendarbcryptos.sysos.pathsys.exitos.environ",
            "level": null,
            "language": null,
            "csv": "",
            "json_data": {
              "headers": [],
              "rows": []
            },
            "breadcrumbs": null,
            "metadata": {
              "extraction_method": "surya",
              "extraction_details": {
                "model": "surya",
                "method": "ml",
                "detection_model": "DetectionPredictor",
                "recognition_model": "RecognitionPredictor",
                "table_rec_model": "TableRecPredictor"
              },
              "quality_score": 0.675
            }
          },
          {
            "type": "tablecell",
            "text": "logging",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "random",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "multiprocessing",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "pebble",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "subprocess",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "threading",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "datetime",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "time",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "hashlib",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "calendar",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "bcrypt",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "os.sys",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "os.path",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "sys.exit",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "os.environ",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 8. Forbidden Python Modules. List of Python modules forbidden to exist in proposed tasks' programs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "C. More Results",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "C.1. Out-of-Distribution Performance Breakdown",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We plot the out-of-distribution performance, broken down by each benchmark and in aggregate, across training steps for our 7B, 7B-Coder,\n14B, and 14B-Coder models in Figures 28 to 31. We observe a strong correlation between training using AZR and improvements in both\nmathematical and coding reasoning capabilities. Moreover, our models are trained for more steps than typical zero-style reasoners; while\noverfitting can occur with static datasets, it is less likely in AZR due to dynamically proposed tasks.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "C.2. In-Distribution Results",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Since we have defined the task domains as input prediction and output prediction, we can directly evaluate our model's capabilities in these\nareas using popular code reasoning benchmarks: CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E) (Gu\net al., 2024; Jain et al., 2024), where CruxEval-O and LCB-E is solving the deduction task, and CruxEval-I is solving the abduction task.\nIn Figure 14, we visualize the evolution of these metrics during the training of Absolute Zero Reasoner-base-7b. As training\nprogresses, we observe a consistent improvement in in-distribution performance across steps. While these three benchmark curves do\nnot perfectly correlate with broader coding or math reasoning capabilities (compare this with Figure 28), they serve as useful proxies for\ntracking task-specific progress.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "C.3. Interplay Between Propose and Solve Roles",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We visualize the training dynamics between the propose and solve roles over training steps in Figures 15 to 17. We observe that, in\ngeneral, the solve roles produce more output tokens than the propose role. Intuitively, this makes sense: the propose role emphasizes\ncreativity and generation of novel tasks, whereas the solve role requires deeper reasoning, which naturally leads to longer outputs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Interestingly, we also observe a consistent ordering in token length across reasoning types\u2014abduction and deduction tasks tend to result\nin shorter outputs than induction tasks during problem solving. This aligns with our intuition, as we observed the model engaging\nin trial-and-error reasoning\u2014repeatedly generating hypothesized inputs, evaluating their outcomes, and reflecting and retrying when\nsubsequent deductions fail to produce the correct output. To our knowledge, this is the first time such a clear distinction in token length",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "1 VALIDATE_CODE_TEMPLATE = \"\"\"{ code }\n2 repr (f({ inputs }))\"\"\"\n3\n4 exec ( VALIDATE_CODE_TEMPLATE )",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "Figure 9. Python Program to Check Valid Code.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "22",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "1 EVAL_INPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n2 { gold_output } == f({ agent_input }) \"\"\"\n3\n4 exec ( EVAL_INPUT_PREDICTION_TEMPLATE )",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 10. Python Code to Check Agent Input Abduction Correctness.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "1 EVAL_OUTPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n2 eval ({ gold_output }) == eval ({ agent_output }) \"\"\"\n3\n4 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 11. Python Code to Check Agent Output Deduction Correctness.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "1 EVAL_FUNCTION_PREDICTION_TEMPLATE = \"\"\"{ code }\n2 matches = []\n3 for gold_input , gold_output in zip ({ gold_inputs } , { gold_outputs }):\n4 match = { gold_output } == f({ gold_input })\n5 matches . append ( match )\n6 \"\"\"\n7\n8 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 12. Python Code to Check Agent Function Induction Correctness.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "1 CHECK_DETERMINISM_TEMPLATE = \"\"\"{ code }\n2 returns = f({ inputs })\n3 if returns != f({ inputs }):\n4 raise Exception ( 'Non - deterministic code ')\n5 repr ( returns ) \"\"\"\n6\n7 exec ( CHECK_DETERMINISM_TEMPLATE )",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "0 30 60 90 120 150 180 210 240 270\nTraining Steps\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nPerformance Score\n\nCruxEval-I\nCruxEval-O\nLiveCodeBench-Execution\nFigure 13. Python Code to Check Deterministic Program.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 13. Python Code to Check Deterministic Program.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "0 30 60 90 120 150 180 210 240 270\nTraining Steps\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nPerformance Score\n\nCruxEval-I\nCruxEval-O\nLiveCodeBench-Execution",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 14. In-distribution Benchmark Score During Training. The evolution of CruxEval-I, CruxEval-O, and LiveCodeBench-\nExecution during training for the Qwen2.5-7B base model trained using AZR.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "23",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\nReward\n\n0 40 80 120 160 200 240 280\n1000\n2000\n3000\n4000\nToken Length\nTraining Steps Abduction Task\nSolve Propose\nFigure 15. Abduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for abduction task type of\nAbsolute Zero Reasoner-base-7b.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\nReward\n\n0 40 80 120 160 200 240 280\n1000\n2000\n3000\n4000\nToken Length\nTraining Steps Abduction Task\nSolve Propose",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 15. Abduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for abduction task type of\nAbsolute Zero Reasoner-base-7b.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "0 40 80 120 160 200 240 280\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nReward\n\n0 40 80 120 160 200 240 280\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nToken Length\nTraining Steps Induction Task\nSolve Propose\nFigure 16. Induction Task Reward and Token Lengths. The task reward and token lengths of the two roles for induction task type of\nAbsolute Zero Reasoner-base-7b.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "0 40 80 120 160 200 240 280\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nReward\n\n0 40 80 120 160 200 240 280\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nToken Length\nTraining Steps Induction Task\nSolve Propose",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 16. Induction Task Reward and Token Lengths. The task reward and token lengths of the two roles for induction task type of\nAbsolute Zero Reasoner-base-7b.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "24",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n\n0 40 80 120 160 200 240 280\n400\n600\n800\n1000\n1200\n1400\nToken Length\nTraining Steps Deduction Task\nSolve Propose\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n\n0 40 80 120 160 200 240 280\n400\n600\n800\n1000\n1200\n1400\nToken Length\nTraining Steps Deduction Task\nSolve Propose",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "Task: Propose Deduction Task\nModel Input: (k random selected example programs as few-shot input)\nModel Output:\nThis task requires creating a new Python function that takes an input and returns a value. The function should have at least one\ninput parameter, be deterministic, and require state tracking across multiple data transformations. The input should be a test case\nprovided to a subject for the purpose of deducing the output. The code and the output should be challenging enough to require\nalgorithmic reasoning without using restricted words or packages.\ndef f(prices: list[int], budget: int):\nn = len(prices)\nprofit = [0] * n\nfor i in range(1, n):\nfor j in range(i):\nif prices[i] > prices[j]:\nprofit[i] = max(profit[i], profit[j] + prices[i] - prices[j])\nreturn max(profit + [profit[-1] - prices[-1] if budget >= prices[-1] else 0])\nInput: [7, 1, 5, 3, 6, 4], 7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 18. An example response of proposing a deduction task. k programs are selected as few-shot examples to the model. The generated\nnew program will then be verifed through execution.\nFigure 18. Propose Deduction Task Example. An example response of proposing a deduction task. k programs are selected as few-shot\nexamples to the model. The generated new program will then be verified through execution.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "24\n25",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "def f(numbers):\n# Step 1: Filter out even numbers\nfiltered_numbers = [num for num in numbers if num % 2 != 0]\n# Step 2: Calculate the sum of the remaining odd numbers\nsum_of_odd_numbers = sum(filtered_numbers)\n# Step 3: Reverse the order of the remaining odd numbers\nreversed_odd_numbers = filtered_numbers[::-1]\n# Step 4: Calculate the product of the reversed odd numbers\nproduct_of_reversed_odd_numbers = 1\nfor num in reversed_odd_numbers:\nproduct_of_reversed_odd_numbers *= num\n# Step 5: Calculate the sum of the digits of the product\nsum_of_digits_of_product = sum(int(digit) for digit in str(product_of_reversed_odd_numbers))\n# Step 6: Modify the original list by adding the sum of the digits to each even number\n# and subtracting it from each odd number\nmodified_numbers = []\nfor num in numbers:\nif num % 2 == 0:\nmodified_numbers.append(num + sum_of_digits_of_product)\nelse:\nmodified_numbers.append(num - sum_of_digits_of_product)\n# Step 7: Return the modified list\nreturn modified_numbers",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed\nin much bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).\nFigure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed in\nmuch bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "25\n26",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Task: Propose Abduction Task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Model Input: (k random selected example programs as few-shot input)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Model Output:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "For this task, we need an intelligent problem that requires deep algorithmic reasoning and demands long multi step reasoning. A\ntypical type of problem that fts these requirements is the Matrix Reconstruction from a given jumbled li st of elements, where\nwe have to fgure out the matrix's dimensions and individual values. However, this task will involve multiple steps, increasing its\ncomplexity and creativity. We will create a function 'f' that transforms a jumbled list of elements into a two-dimensional matrix,\nthen back to a list, given a specifc reconstruction pattern. We will retrieve the original list from the reconstructed matrix. Here,\nwe will avoid random and datetime operations to ensure determinism. The function will be structured to reconstruct the matrix\nelements in a unique and challenging way. We will provide an input that, after reconstruction, presents a unique list that can be\neasily deduced from the given output in a test piece.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "def f(jumbled_elements: list) -> list:\nif not jumbled_elements:\nreturn []\n# Calculate the square root of the length of elements to estimate the matrix dimensions\nn = len(jumbled_elements)\nsqrt_n = int(n ** 0.5)\nif n > sqrt_n ** 2:\nsqrt_n += 1\nif n < sqrt_n ** 2:\nsqrt_n -= 1\n# Now, we need to reconstruct the matrix elements in a challenging way using specific patterns\n# For this, we can reconstruct the matrix using zigzag patterns or hill-climbing patterns\n# Here, a hill-climbing pattern is used where we traverse the matrix from the extreme\n# right and downward Then traverse the matrix from the second to last element to the\n# extreme bottom. Repeat the process until the entire matrix is traversed.\n# Initialize the matrix\nmatrix = [[0 for _ in range(sqrt_n)] for _ in range(sqrt_n)]\ncur_idx = 0\n# Fill the matrix in a zigzag pattern\nfor i in range(sqrt_n):\nif i % 2 == 0:\nfor j in range(sqrt_n):\nmatrix[j][i] = j * sqrt_n + i + cur_idx\ncur_idx += 1\nelse:\nfor j in range(sqrt_n - 1, -1, -1):\nmatrix[j][i] = j * sqrt_n + n - i - cur_idx - 1\ncur_idx += 1\n# Reconstruct the list into a matrix by repeating the process\nreconstructed_list = []\nfor i in range(sqrt_n):\nfor j in range(sqrt_n):\nreconstructed_list.append(matrix[i][j])\n# Now, we need to calculate another matrix based on a function F applied to the current matrix\n# F is a mathematical function that can be complex, such as factorial, prime check,\n# or another transformation For example, transform each element by adding 1 if it's prime,\n# subtracting 1 if it's even, or leaving it unchanged, Implement any specific transformation\n# function here. Here, for demonstration, each element is left unchanged\ntransformed_result = reconstructed_list\nreturn transformed_result\nOutput: [0, 11, 10, 1, 5, 16, 15, 6, 10, 21, 20, 11, 15, 26, 25, 16]",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 20. An example response of proposing an abduction task. The model uses comments as reasoning when generating the program.\n26\nFigure 20. Propose Abduction Task Example. An example response of proposing an abduction task. The model uses comments as\nreasoning when generating the program.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "27",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Task: Propose Induction Task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Input: Here is the function f. Analyze its behaviour and relate it to the observed inputs/outputs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "def f(numbers: list[int]) -> int:\nreplacement = {\n0: 2,\n1: 3,\n2: 1,\n3: 0,\n4: 4\n}\nreturn sum(replacement[num] for num in numbers)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Model Output:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "To solve this task, we need to come up with 10 diferent inputs for the function f(numbers: list[int]) -> int. The function\nreplaces each number in the list according to a specifc dictionary and then sums up the replaced numbers. Let's analyze the\ndictionary:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "This dictionary tells us that:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "-0 becomes 2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "-1 becomes 3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "-2 becomes 1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "-3 becomes 0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "-4 remains 4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "To create diverse outputs, we should include lists with diferent sets of numbers from the list {0, 1, 2, 3, 4}. We also need\nto ensure that the inputs are valid (i.e., lists of integers).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Let's illustrate using provided examples step-by-step:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "1. Input: [0] \u21d2 Replacement: [2] \u21d2 sum = 2\n2. Input: [1, 2, 3] \u21d2 Replacement: [3, 1, 0] \u21d2 sum = 4\n3. Input: [4, 1] \u21d2 Replacement: [4, 3] \u21d2 sum = 7\n4. Input: [2, 1, 0] \u21d2 Replacement: [1, 3, 2] \u21d2 sum = 6\n5. Input: [3, 0, 2, 4] \u21d2 Replacement: [0, 2, 1, 4] \u21d2 sum = 7\n6. Input: [1, 1, 1] \u21d2 Replacement: [3, 3, 3] \u21d2 sum = 9\n7. Input: [2, 2, 0, 1] \u21d2 Replacement: [1, 1, 2, 3] \u21d2 sum = 7\n8. Input: [3, 4, 2] \u21d2 Replacement: [0, 4, 1] \u21d2 sum = 5\n9. Input: [0, 1, 2, 3, 4] \u21d2 Replacement: [2, 3, 1, 0, 4] \u21d2 sum = 10\n10. Input: [1, 2, 3, 0, 4] \u21d2 Replacement: [3, 1, 0, 2, 4] \u21d2 sum = 10\nThus, the rule is clearly replacing numbers according to the given mapping and then summing the results.\nInputs: ['[0]', '[1, 2, 3]', '[4, 1]', '[2, 1, 0]', '[3, 0, 2, 4]', '[1, 1, 1]',\n[2, 2, 0, 1], '[3, 4, 2]', '[0, 1, 2, 3, 4]', '[1, 2, 3, 0, 4]']|\nMessage: You are given a function that takes a list of integers as input and returns a single integer as output. Can you determine\nthe rule this function is following by analyzing the outputs for the given inputs?\n(After execution:) Outputs: ['2', '4', '7', '6', '7', '9', '7', '5', '10', '10']",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 21. An example response of proposing an induction task. Given a random selected program, the model needs to propose 10 inputs\nand a description of program. These inputs will be executed and verifed to get outputs. At solve phase, the model need to generate the\nprogram given the message and 5 input/output pairs (half is hidden cases)\nFigure 21. Propose Induction Task Example. An example response of proposing an induction task. Given a random selected program,\nthe model needs to propose 10 inputs and a description of program. These inputs will be executed and verified to get outputs. At solve\nphase, the model need to generate the program given the message and 5 input/output pairs (half is hidden cases)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "27\n28",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Task: Solve Abduction Task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Input: Here is the function f and the output: '1', please predict the input.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "def f(numbers: list[int], target: int) -> int:\naccumulated_values = []\ncurrent_sum = 0\nfor number in numbers:\ncurrent_sum += number\naccumulated_values.append(current_sum)\nresult = 0\nfor i in range(len(accumulated_values)):\nfor j in range(i + 1, len(accumulated_values)):\nif accumulated_values[j] \\\n- accumulated_values[i] == target:\nresult = accumulated_values[i]\nreturn result\nreturn result",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Model Output:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Let's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "To fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\nsatisfes this condition is 1.\nLet's consider a simple example:\n-numbers = [1, 2]\n-target = 1\nHere's the step-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\nConsider:\n-numbers = [1, 0]\n-target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n...(shortened)\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n-numbers = [2, -1, 1]\n-target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 22. A more detailed version of the example shown in main Figure 7. The model is solving an abduction task (predict input).\nFigure 22. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\nabduction task (predict input).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "28\n29",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Task: Solve Deduction Task (Example 1)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Input: Here is the function f, the input provided [1, 2, 1, 2, 3, 2, 1], 2, and we need to\npredict the output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "def f(array: list[int], k: int) -> int:\nwindow_start = 0\nmax_length = 0\nchar_freq = {}\nfor window_end in range(len(array)):\nend_char = array[window_end]\nif end_char not in char_freq:\nchar_freq[end_char] = 0\nchar_freq[end_char] += 1\nwhile len(char_freq) > k:\nstart_char = array[window_start]\nchar_freq[start_char] -= 1\nif char_freq[start_char] == 0:\ndel char_freq[start_char]\nwindow_start += 1\nmax_length = max(max_length, window_end - window_start + 1)\nreturn max_length\nInput: [1, 2, 1, 2, 3, 2, 1], 2\nModel Output:\nBased on the given code snippet, we need to determine the output of the function f(array, k) when provided with the input [1, 2, 1, 2, 3, 2, 1] and k = 2.\nThe function f is designed to fnd the length of the longest subarray with at most k distinct characters. Let's go through the code step by step:\n1. Initialize window_start to 0, max_length to 0, and an empty dictionary char_freq to store the frequency of characters in the current window.\n2. Iterate through the array using window_end as the index:\na. Get the current character end_char at index window_end.\nb. If end_char is not in char_freq, add it with a frequency of 0.\nc. Increment the frequency of end_char by 1.\n3. Check if the number of distinct characters in char_freq is greater than k:\na. If true, shrink the window from the start:\ni. Get the character start_char at index window_start.\nii. Decrement its frequency by 1.\niii. If the frequency becomes 0, remove the character from char_freq.\niv. Increment window_start by 1.\n4. Update max_length by taking the maximum of max_length and the current window size (window_end - window_start + 1).\nNow, let's apply this algorithm step-by-step to the input [1, 2, 1, 2, 3, 2, 1], k = 2:\n\u2022 window_start = 0, max_length = 0, char_freq = {}\n\u2022 window_end = 0, add 1 to char_freq: {1:1}, window size = 1\n\u2022 window_end = 1, add 2: {1:1, 2:1}, window size = 2\n\u2022 window_end = 2, increment 1: {1:2, 2:1}, window size = 3\n\u2022 window_end = 3, increment 2: {1:2, 2:2}, window size = 4\n\u2022 window_end = 4, add 3: {1:2, 2:2, 3:1}; now distinct chars > 2, shrink window:\n- remove 1 at start: {1:1, 2:2, 3:1}\n- remove 2 at start: {1:1, 2:1, 3:1}\n- remove 1 again: {2:1, 3:1}, window now starts at 3\n\u2022 window_end = 5, increment 2: {2:2, 3:1}, window size = 3\n\u2022 window_end = 6, add 1: {2:2, 3:1, 1:1}, shrink again:\n- remove 2: {2:1, 3:1, 1:1}\n- remove 3: {2:1, 1:1}, window now starts at 5\n- window size = 2\nMaximum window size encountered is 4, thus max_length = 4.\nHence, the function returns 4.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 23. Example of solving the deduction task. Given a program and input, the model simulates the execution of the program until\nthe fnal result.\nFigure 23. Solve Deduction Task Example. Example of solving the deduction task. Given a program and input, the model simulates the\nexecution of the program until the final result.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "29\n30",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Task: Solve Induction Task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Input: Given the inputs and outputs, deduce the code snippet implementing the described function-\nality:\nInput Description: Given a list of integers, sort the list and then for each element at index i, multiply it by\n(n \u2212 i), where n is the total number of elements. Compute the fnal sum.\nInputs: ['[1, 2, 3, 4]', '[4, 3, 2, 1]', '[3, 1, 4, 2]', '[1, 2, 2, 3]', '[5]']\nOutputs: ['20', '20', '20', '17', '5']\n# Golden\ndef f(elements: list[int]) -> int:\nn = len(elements)\nsorted_elements = sorted(elements)\nmodified_elements = [sorted_elements[i] * (n - i) for i in range(n)]\nreturn sum(modified_elements)\nModel Output:\nGiven the problem and the input/output pairs, we need to deduce a general code snippet that takes a sorted list and a list of multipliers, and then computes a fnal sum based on the multipliers.\nLet's break down the problem step by step:\n1. **Understand the Message:**\n- We are given a list of integers.\n- The list is sorted.\n- For each element at index i in the sorted list, we multiply it by (n \u2212 i), where n is the number of elements in the list.\n- We need to compute the sum of these products.\n2. **Analyze the Input/Output Pairs:**\n- For the input [1, 2, 3, 4], the sorted form is the same, and the output is 20.\n1 \u00b7 (4 \u2212 1) = 3, 2 \u00b7 (4 \u2212 2) = 4, 3 \u00b7 (4 \u2212 3) = 3, 4 \u00b7 (4 \u2212 4) = 0, Sum: 3 + 4 + 3 + 0 = 10\n- For the input [4, 3, 2, 1], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\n- For the input [3, 1, 4, 2], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\nshortened\n3. **Re-evaluate the Outputs:**\n- It seems there might be an error in the provided outputs for some inputs. However, based on the problem description, the outputs should be correct if we consider the sorted list and the multipliers\ncorrectly.\n4. **Deduce the Function:**\n- Given the sorted list and the list of multipliers, we can deduce the following function:\n\"'python\ndef f(a):\nsorted_a = sorted(a)\nn = len(sorted_a)\nresult = 0\nfor i in range(n):\nresult += sorted_a[i] * (n - i)\nreturn result\n...(shortened)\n5. **Verify the Function:**\n- Let's verify the function with the provided inputs:\n- For [1, 2, 3, 4]:\n- Sorted list: [1, 2, 3, 4]\n- Multipliers: [3, 4, 3, 0]\n- Sum: 1 \u00b7 3+2 \u00b7 4+3 \u00b7 3+4 \u00b7 0 = 3 + 8 + 9 + 0 = 20\n...(shortened)\nThe corrected outputs should be: For [1, 2, 3, 4]: 20, For [4, 3, 2, 1]: 20, For [3, 1, 4, 2]: 20, For [1, 2, 2, 3]: 13, For [5]: 0\nGiven the function:\ndef f(a):\nsorted_a = sorted(a)\nn = len(sorted_a)\nresult = 0\nfor i in range(n):\nresult += sorted_a[i] * (n - i)\nreturn result\nThe outputs match the corrected sum.\n### Final Answer\n(repeat the program above)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 24. Example of solving an induction task. The model is given a program description, and 5 input/output cases and predict the\nfunction. Interestingly, after the model gives the function, it will go through the given use cases one by one and confrm all test cases are\npassed. 30\nFigure 24. Solve Induction Task Example. Example of solving an induction task. The model is given a program description, and 5\ninput/output cases and predict the function. Interestingly, after the model gives the function, it will go through the given use cases one by\none and confirm all test cases are passed.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "31",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "tablegroup",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nModelHEval+MBPP+LCBv1-5AIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBenchLlama3.1-8B31.753.70.00.00.02.510.65.52.1+ Simple-RL-Zoo38.455.37.40.00.07.522.28.84.7+ AZR35.450.88.53.30.05.013.214.05.0Qwen2.5-3B-Coder67.165.920.03.33.320.051.018.416.6+ AZR71.369.024.43.33.337.562.026.127.0Qwen2.5-14B-Coder76.871.731.40.00.037.554.810.718.5+ AZR80.571.239.023.320.065.078.632.039.3Qwen2.5-14B-Base78.066.721.76.73.335.066.228.332.4+ AZR70.768.835.210.020.062.576.240.442.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "table",
            "text": "ModelHEval+MBPP+LCBv1-5AIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBenchLlama3.1-8B31.753.70.00.00.02.510.65.52.1+ Simple-RL-Zoo38.455.37.40.00.07.522.28.84.7+ AZR35.450.88.53.30.05.013.214.05.0Qwen2.5-3B-Coder67.165.920.03.33.320.051.018.416.6+ AZR71.369.024.43.33.337.562.026.127.0Qwen2.5-14B-Coder76.871.731.40.00.037.554.810.718.5+ AZR80.571.239.023.320.065.078.632.039.3Qwen2.5-14B-Base78.066.721.76.73.335.066.228.332.4+ AZR70.768.835.210.020.062.576.240.442.5",
            "level": null,
            "language": null,
            "csv": "",
            "json_data": {
              "headers": [],
              "rows": []
            },
            "breadcrumbs": null,
            "metadata": {
              "extraction_method": "surya",
              "extraction_details": {
                "model": "surya",
                "method": "ml",
                "detection_model": "DetectionPredictor",
                "recognition_model": "RecognitionPredictor",
                "table_rec_model": "TableRecPredictor"
              },
              "quality_score": 0.65625
            }
          },
          {
            "type": "tablecell",
            "text": "Model",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "HEval+",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "MBPP+",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "LCBv1-5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "AIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBench",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Llama3.1-8B",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "31.7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "53.7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "2.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "10.6",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "5.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "2.1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "+ Simple-RL-Zoo",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "38.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "55.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "7.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "7.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "22.2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "8.8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "4.7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "+ AZR",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "35.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "50.8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "8.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "3.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "5.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "13.2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "14.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "5.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-3B-Coder",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "67.1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "65.9",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "20.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "3.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "3.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "20.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "51.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "18.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "16.6",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "+ AZR",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "71.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "69.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "24.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "3.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "3.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "37.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "62.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "26.1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "27.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-14B-Coder",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "76.8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "71.7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "31.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "0.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "37.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "54.8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "10.7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "18.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "+ AZR",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "80.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "71.2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "39.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "23.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "20.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "65.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "78.6",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "32.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "39.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "Qwen2.5-14B-Base",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "78.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "66.7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "21.7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "6.7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "3.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "35.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "66.2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "28.3",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "32.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "+ AZR",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "70.7",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "68.8",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "35.2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "10.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "20.0",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "62.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "76.2",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "40.4",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "tablecell",
            "text": "42.5",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Table 5. Detailed Breakdown of Evaluation Benchmarks for Other Model Sizes and Types. Full evaluation of AZR trained on\nother models on three standard code benchmarks (HEval+\n, MBPP+\n, LCBv1-5) and six math benchmarks (AIME'24, AIME'25, AMC'23,\nMATH500, Minerva, OlympiadBench).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "has been observed and presented for jointly trained reasoning multi-tasks. Previously, length differences were typically noted between\ncorrect and incorrect traces (Liu et al., 2025).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "The reward dynamics between the propose and solve roles exhibit mildly adversarial behavior: when one receives higher rewards, the\nother often receives lower rewards. However, this is not entirely adversarial, as the proposer is also penalized for generating unsolvable\ntasks, encouraging overall cooperative behavior in the learning process.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "C.4. Complexity and Diversity Metrics of AZR Proposed Tasks",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We outline several metrics used to probe characteristics of the tasks proposed during the training of AZR from the base model. Specifically,\nwe log two sets of metrics: program complexity and task diversity. For complexity, we employ two proxy measures\u2014ComplexiPy score\nand the Halstead metric. To assess diversity, we compute the average abstract syntax tree (AST) edit distance between the proposed\nprogram and a set of K reference programs, and an answer diversity metric. We calculate this answer diversity metric by tracking all\nhistorical answers to the generated questions, i.e., the input-output pairs, and form a categorical distribution over these outputs. We\ndefine answer diversity as 1 \u2212 p(answer), where p(answer) is the empirical probability of a specific answer\u2014used as a proxy for the\ndiversity of generated outputs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We present these metrics in Figure 27. Interestingly, even without incorporating them explicitly into the reward function, the policy\nappears to implicitly optimize for these metrics. This aligns well with intuitive notions of task difficulty and diversity, and that the policy\nlearned to propose increasingly challenging tasks over time using our proposed simple reward function in Equation (4).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "C.5. Generated Code Complexity Dynamics Between Abd/Ded and Ind.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "We use the ComplexiPy package to measure code complexity. For each generated program in the induction task, we compute the\ncognitive complexity difference from the corresponding \"gold\" code, i.e. complexipy(p\u03c0\npropose\n{abduction,deduction}\n) \u2212 complexipy(p\u03c0solve\ninduction\n) for each\npair, where the superscript of \u03c0 indicates the role and the subscript indicates the task type(s), and p denotes the generated programs. On\naverage, the difference of proposer and solver while holding the code's functionality constant is 0.27, indicating that the proposer in the\nabduction/deduction tasks often increases the cognitive complexity to make the code appear more convoluted, whereas the induction\nsolver tends to generate more efficient implementations.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "32",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Learning to Reason with Zero Data via Self-play",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Task: Solve Deduction Task (Example 2)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Input: Here is the function f, the given input, and we need to predict the output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "def f(num_list: list[int], target_product: int) -> list[tuple[int]]:\nresult_pairs = []\nfor i in range(len(num_list)):\nfor j in range(i + 1, len(num_list)):\nif num_list[i] * num_list[j] == target_product:\nresult_pairs.append((num_list[i], num_list[j]))\nreturn result_pairs",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Input: [2, 3, 5, 7, 10], 10\nOutput: [(2, 5)]",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Model Output:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "The given function f takes a list of integers num_list and a target product target_product as inputs. It is designed to fnd all\npairs of numbers in num_list whose product equals target_product. The function then returns a list of tuples, where each\ntuple contains a pair of numbers that satisfy the condition.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Let's go through the code step by step:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. Initialize an empty list result_pairs to store pairs whose product equals target_product.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "2. Use nested loops to iterate through each pair of numbers in num_list. The outer loop iterates over each number, and the inner\nloop iterates over numbers that come after the current number in the outer loop (\u907f\u514d\u91cd\u590d\u548c\u53cd\u5411\u7684\u5bf9\u5b50,\u4f8b\u5982 (2, 3) \u548c (3, 2))\u3002\n3. For each pair (num_list[i], num_list[j]), check if their product equals target_product.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "4. If the product equals target_product, append the tuple (num_list[i], num_list[j]) to result_pairs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "5. After all pairs have been checked, return the list result_pairs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Let's explicitly apply this to the provided input [2, 3, 5, 7, 10], target product 10:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "- For i=0 (num_list[0] = 2):\n- j=1, 2 \u00d7 3 = 6 (\u226010)\n- j=2, 2 \u00d7 5 = 10 (=10), add pair (2, 5)\n- j=3, 2 \u00d7 7 = 14 (\u226010)\n- j=4, 2 \u00d7 10 = 20 (\u226010)\n- For i=1 (num_list[1] = 3):\n- j=2, 3 \u00d7 5 = 15 (\u226010)\n- j=3, 3 \u00d7 7 = 21 (\u226010)\n- j=4, 3 \u00d7 10 = 30 (\u226010)\n- For i=2 (num_list[2] = 5):\n- j=3, 5 \u00d7 7 = 35 (\u226010)\n- j=4, 5 \u00d7 10 = 50 (\u226010)\n- For i=3 (num_list[3] = 7):\n- j=4, 7 \u00d7 10 = 70 (\u226010)\n- For i=4 (num_list[4] = 10):\n- No further pairs to check\nThus, the only pair satisfying the condition is (2, 5). Therefore, the output is:\n[(2, 5)]",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 25. An example of the model solving an deduction task. We observed that the model outputs mixed language (Chinese) when\nsolving the task.\nFigure 25. Mixed Language CoT Example. An example of the model solving an deduction task. We observed that the model outputs\nmixed language (Chinese) when solving the task.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "31\n33",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Task: Solve Deduction Task (Llama 8B)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Input: Here is the function f, the input provided 'cookie', and we need to predict the output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "def f(input_string: str):\n\"\"\"Function to check if a string contains given substrings.\"\"\"\nsubstring = \"code\"\n# Start with `substring` as the first substring.\ncurr_index = substring\n# Slice out the required substring from the input string\nsliced_out_string = input_string[0:len(substring)]\nif (sliced_out_string == substring):\nreturn True\n# Check if there are any remaining substrings to be checked\nif (len(curr_index) > len(input_string)):\nreturn False\nreturn False",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Input: 'cookie'\nOutput: False",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Model Output:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "I know that the line 'curr_index = substring' is just initialization of the curr_index parameter variable and does not afect the output.\nHence, I can remove it completely to view the status of code before any output gets generated.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Now I have a single variable, \"'curr_index\"' which is set to \"codecandide\".",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "The second line of code, 'slice_out_string = input_string[0:len(substring)]', assigns the result of slicing the input_string to the left\nof index 0 and to the right of index len(substring), i.e., the index of the last character in the substring. It then assigns this value to\nthe variable \"'slice_out_string\"'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "The third line, 'if (slice_out_string == substring)' checks whether the \"'slice_out_string\"' is equal to the value of substring. Because\ncurr_index is set to \"codecandide\", \"code\" appears on the left of the frst index in input_string, and thus, \"'slice_out_string\"' assumes\nthe value \"code\". That means for a minute the \"'if statement\"' is satisfed and will continue the code execution afterwards.\nHowever, according to the True/False statement, we see that the value of the frst index of input_string does not exist hence while it\ndoes output \"True\", \"False\" is outputted in main. So, the output of the whole program is shown below:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "'f(input_string = \"cookie\")' then output: False",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illustrate an\nexample here.\nFigure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illus-\ntrate an example here.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "0 80 160 240\n0\n0.47\nComplexiPy Score\n0 80 160 240\n0\n0.20\nHalstead Measure\n0 80 160 240\n0\n0.78\nAST Edit Distance\n0 80 160 240\n0\n0.99\nAnswer Diversity\nTraining Steps Complexity Diversity",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\nFigure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "35\n34",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "MATH REASONING",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "CODE REASONING\nOVERALL PERFORMANCE\n50 75 100 125 150 175 200 225 250\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 75 100 125 150 175 200 225 250\n0.000\n0.050\n0.100\nAIME 2025\n50 75 100 125 150 175 200 225 250\n0.300\n0.325\n0.350\n0.375\n0.400\nOlympiad Bench\n50 75 100 125 150 175 200 225 250\n0.280\n0.300\n0.320\n0.340\n0.360\n0.380\nAccuracy\n\nMinerva\n50 75 100 125 150 175 200 225 250\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nMath 500\n50 75 100 125 150 175 200 225 250\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 75 100 125 150 175 200 225 250\n0.700\n0.710\n0.720\n0.730\n0.740\nAccuracy\n\nHumanEval+\n50 75 100 125 150 175 200 225 250\n0.660\n0.670\n0.680\n0.690\n0.700\nMBPP+\n50 75 100 125 150 175 200 225 250\n0.240\n0.260\n0.280\nLiveCodeBench\n50 75 100 125 150 175 200 225 250\n0.300\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 75 100 125 150 175 200 225 250\n0.540\n0.550\n0.560\n0.570\nCode Average\n50 75 100 125 150 175 200 225 250\n0.420\n0.440\n0.460\n0.480\nOverall Average\nFigure 28. Absolute Zero Reasoner-base-7b OOD Performance Breakdown.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "CODE REASONING\nOVERALL PERFORMANCE\n50 75 100 125 150 175 200 225 250\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 75 100 125 150 175 200 225 250\n0.000\n0.050\n0.100\nAIME 2025\n50 75 100 125 150 175 200 225 250\n0.300\n0.325\n0.350\n0.375\n0.400\nOlympiad Bench\n50 75 100 125 150 175 200 225 250\n0.280\n0.300\n0.320\n0.340\n0.360\n0.380\nAccuracy\n\nMinerva\n50 75 100 125 150 175 200 225 250\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nMath 500\n50 75 100 125 150 175 200 225 250\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 75 100 125 150 175 200 225 250\n0.700\n0.710\n0.720\n0.730\n0.740\nAccuracy\n\nHumanEval+\n50 75 100 125 150 175 200 225 250\n0.660\n0.670\n0.680\n0.690\n0.700\nMBPP+\n50 75 100 125 150 175 200 225 250\n0.240\n0.260\n0.280\nLiveCodeBench\n50 75 100 125 150 175 200 225 250\n0.300\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 75 100 125 150 175 200 225 250\n0.540\n0.550\n0.560\n0.570\nCode Average\n50 75 100 125 150 175 200 225 250\n0.420\n0.440\n0.460\n0.480\nOverall Average",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 28. Absolute Zero Reasoner-base-7b OOD Performance Breakdown.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "35",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "MATH REASONING",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "CODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.000\n0.020\n0.040\n0.060\n0.080\n0.100\nAIME 2025\n50 100 150 200 250 300 350\n0.340\n0.360\n0.380\nOlympiad Bench\n50 100 150 200 250 300 350\n0.275\n0.300\n0.325\n0.350\n0.375\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.680\n0.700\n0.720\n0.740\n0.760\nMath 500\n50 100 150 200 250 300 350\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.810\n0.820\n0.830\n0.840\n0.850\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.260\n0.280\n0.300\n0.320\nLiveCodeBench\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.580\n0.590\n0.600\n0.610\n0.620\n0.630\nCode Average\n50 100 150 200 250 300 350\n0.460\n0.480\n0.500\nOverall Average\nFigure 29. Absolute Zero Reasoner-Coder-7b OOD Performance Breakdown.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "CODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.000\n0.020\n0.040\n0.060\n0.080\n0.100\nAIME 2025\n50 100 150 200 250 300 350\n0.340\n0.360\n0.380\nOlympiad Bench\n50 100 150 200 250 300 350\n0.275\n0.300\n0.325\n0.350\n0.375\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.680\n0.700\n0.720\n0.740\n0.760\nMath 500\n50 100 150 200 250 300 350\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.810\n0.820\n0.830\n0.840\n0.850\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.260\n0.280\n0.300\n0.320\nLiveCodeBench\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.580\n0.590\n0.600\n0.610\n0.620\n0.630\nCode Average\n50 100 150 200 250 300 350\n0.460\n0.480\n0.500\nOverall Average",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 29. Absolute Zero Reasoner-Coder-7b OOD Performance Breakdown.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "36",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "MATH REASONING",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "CODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350\n0.250\n0.300\n0.350\n0.400\nOlympiad Bench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.500\n0.550\n0.600\n0.650\n0.700\n0.750\nMath 500\n50 100 150 200 250 300 350\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.700\n0.720\n0.740\n0.760\n0.780\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\nLiveCodeBench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.570\n0.580\n0.590\n0.600\n0.610\n0.620\nCode Average\n50 100 150 200 250 300 350\n0.440\n0.460\n0.480\n0.500\nOverall Average\nFigure 30. Absolute Zero Reasoner-base-14b OOD Performance Breakdown.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "CODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350\n0.250\n0.300\n0.350\n0.400\nOlympiad Bench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.500\n0.550\n0.600\n0.650\n0.700\n0.750\nMath 500\n50 100 150 200 250 300 350\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.700\n0.720\n0.740\n0.760\n0.780\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\nLiveCodeBench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.570\n0.580\n0.590\n0.600\n0.610\n0.620\nCode Average\n50 100 150 200 250 300 350\n0.440\n0.460\n0.480\n0.500\nOverall Average",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 30. Absolute Zero Reasoner-base-14b OOD Performance Breakdown.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "37",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figuregroup",
            "text": "CODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350 400\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350 400\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350 400\n0.380\n0.400\n0.420\n0.440 Olympiad Bench\n50 100 150 200 250 300 350 400\n0.200\n0.250\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350 400\n0.700\n0.720\n0.740\n0.760\n0.780\n0.800\nMath 500\n50 100 150 200 250 300 350 400\n0.500\n0.550\n0.600\n0.650\n0.700\nAMC 2023\n50 100 150 200 250 300 350 400\n0.780\n0.800\n0.820\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350 400\n0.700\n0.710\n0.720\n0.730\n0.740\nMBPP+\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\nLiveCodeBench\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\n0.420\n0.440\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350 400\n0.610\n0.620\n0.630\n0.640\n0.650\nCode Average\n50 100 150 200 250 300 350 400\n0.500\n0.520\n0.540\nOverall Average\nMATH REASONING",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "MATH REASONING",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "CODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350 400\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350 400\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350 400\n0.380\n0.400\n0.420\n0.440 Olympiad Bench\n50 100 150 200 250 300 350 400\n0.200\n0.250\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350 400\n0.700\n0.720\n0.740\n0.760\n0.780\n0.800\nMath 500\n50 100 150 200 250 300 350 400\n0.500\n0.550\n0.600\n0.650\n0.700\nAMC 2023\n50 100 150 200 250 300 350 400\n0.780\n0.800\n0.820\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350 400\n0.700\n0.710\n0.720\n0.730\n0.740\nMBPP+\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\nLiveCodeBench\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\n0.420\n0.440\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350 400\n0.610\n0.620\n0.630\n0.640\n0.650\nCode Average\n50 100 150 200 250 300 350 400\n0.500\n0.520\n0.540\nOverall Average",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\nFigure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "<think>\nDesign an absolutely ludicrous and convoluted Python function that is extremely difficult to\ndeduce the output from the input, designed to keep machine learning models such as Snippi\nguessing and your peers puzzling.\n!\u2192\n!\u2192\nThe aim is to outsmart all these groups of intelligent machines and less intelligent humans. This\n!\u2192 is for the brains behind the future.\n</think>\n- Absolute Zero Reasoner-Llama3.1-8b @ step 132",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 32. Absolute Zero Reasoner \u2013Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\nFigure 32. Absolute Zero Reasoner \u2013 Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "39\n38",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\nThe assistant first thinks about the reasoning process in the mind and then provides the user\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer>\nanswer here </answer>.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\nUser: {TASK_INSTRUCTION}",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Assistant: <think>",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.\nFigure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "40\n39",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "## Task: Create a Python Code Snippet (where custom classes are allowed, which should be defined\n!\u2192 at the top of the code snippet) with one Matching Input\nUsing the reference code snippets provided below as examples, design a new and unique Python code\nsnippet that demands deep algorithmic reasoning to deduce one possible input from a given\noutput. Your submission should include both a code snippet and test input pair, where the\ninput will be plugged into the code snippet to produce the output, which that function output\nbe given to a test subject to come up with any input that will produce the same function\noutput. This is meant to be an I.Q. test.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n### Code Requirements:\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n!\u2192 `f`\n- Ensure the function returns a value\n- Include at least one input parameter\n- Make the function deterministic\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n!\u2192 requires long multi step reasoning\n- AVOID THE FOLLOWING:\n* Random functions or variables\n* Date/time operations\n* I/O operations (reading files, network requests)\n* Printing or logging\n* Any external state\n- Ensure execution completes within 10 seconds on a modern CPU\n- All imports and class definitions should be at the very top of the code snippet\n- The snippet should end with a return statement from the main function `f`, anything after will\n!\u2192 be removed\n### Input Requirements:\n- Provide exactly one test input for your function\n- Format multiple arguments with commas between them\n- Remember to add quotes around string arguments\n### Formatting:\n- Format your code with: ```python\ndef f(...):\n# your code here\nreturn ...\n```\n- Format your input with: ```input\narg1, arg2, ...\n```\n### Example Format:\n```python\ndef f(name: str, info: dict):\n# code logic here\nreturn result\n```\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n### Evaluation Criteria:\n- Executability, your code should be executable given your input\n- Difficulty in predicting the output from your provided input and code snippet. Focus on either\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n!\u2192\n!\u2192\n!\u2192\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\ninputs.\n!\u2192\n!\u2192\n### Reference Code Snippets:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "{CODE_REFERENCES_FROM_BUFFER}",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.\nFigure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "41\n40",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "## Task: Create a New Python Code Snippet (where custom classes are allowed, which should be\n!\u2192 defined at the top of the code snippet) with one Matching Input\nUsing the reference code snippets provided below as examples, design a new and unique Python code\nsnippet that demands deep algorithmic reasoning to deduce the output from the input. Your\nsubmission should include a code snippet and a test input pair, where the input will be\nplugged into the code snippet to produce the output. The input will be given to a test\nsubject to deduce the output, which is meant to be an I.Q. test.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n### Code Requirements:\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n!\u2192 `f`\n- Ensure the function returns a value\n- Include at least one input parameter\n- Make the function deterministic\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n!\u2192 requires long multi step reasoning\n- AVOID THE FOLLOWING:\n* Random functions or variables\n* Date/time operations\n* I/O operations (reading files, network requests)\n* Printing or logging\n* Any external state\n- Ensure execution completes within 10 seconds on a modern CPU\n- All imports and class definitions should be at the very top of the code snippet\n- The snippet should end with a return statement from the main function `f`, anything after will\n!\u2192 be removed\n### Input Requirements:\n- Provide exactly one test input for your function\n- Format multiple arguments with commas between them\n- Remember to add quotes around string arguments\n### Formatting:\n- Format your code with:\n```python\ndef f(...):\n# your code here\nreturn ...\n```\n- Format your input with:\n```input\narg1, arg2, ...\n```\n### Example Format:\n```python\ndef f(name: str, info: dict):\n# code logic here\nreturn result\n```\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n### Evaluation Criteria:\n- Executability, your code should be executable given your input\n- Difficulty in predicting your ```input``` from 1) your ```python``` code and 2) the\ndeterministic ```output``` that will be obtained from your ```input```. Focus on either\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\ninputs.\n!\u2192\n!\u2192\n### Reference Code Snippets:\n{CODE_REFERENCES_FROM_BUFFER}",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 35. Program Output Deduction Task\u2014Problem Generation Instruction.\nFigure 35. Program Output Deduction Task\u2014Problem Generation Instruction.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "42\n41",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "## Task: Output {NUM_INPUTS} Inputs that can be plugged into the following Code Snippet to\n!\u2192 produce diverse Outputs, and give a message related to the given snippet.\nUsing the code snippet provided below, design {NUM_INPUTS} inputs that can be plugged into the\ncode snippet to produce a diverse set of outputs. A subset of your given input and its\ndeterministically produced outputs will be given to a test subject to deduce the function,\nwhich is meant to be an I.Q. test. You can also leave a message to the test subject to help\nthem deduce the code snippet.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n### Input Requirements:\n- Provide {NUM_INPUTS} valid inputs for the code snippet\n- For each input, format multiple arguments with commas between them\n- Remember to add quotes around string arguments\n- Each input should be individually wrapped in ```input``` tags\n### Message Requirements:\n- Leave a message to the test subject to help them deduce the code snippet\n- The message should be wrapped in ```message``` tags\n- The message can be in any form, can even be formed into a coding question, or a natural\n!\u2192 language instruction what the code snippet does\n- You cannot provide the code snippet in the message\n### Formatting:\n- Format your input with:\n```input\narg1, arg2, ...\n```\n### Example Format:\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n```input\n'Sammy', {{'age': 37, 'city': 'Los Angeles'}}\n```\n### Evaluation Criteria:\n- Executability, your code should be executable given your inputs\n- Coverage, the inputs and outputs should cover the whole input space of the code snippet, able\n!\u2192 to deduce the code snippet from the inputs and outputs\n- Creativity, the inputs need to be sufficiently different from each other\n- The overall selection of inputs and message combined should be challenging for the test\n!\u2192 subject, but not impossible for them to solve\nFirst, carefully devise a clear plan: e.g., understand the code snippet, then identify how your\nproposed inputs have high coverage, and why the inputs will be challenging and creative.\nThen, write the inputs and message. Remember to wrap your inputs in ```input``` tags, and\nyour message in ```message``` tags.\n!\u2192\n!\u2192\n!\u2192\n### Code Snippet:\n```python\n{SNIPPET_FROM_BUFFER}\n```",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 36. Program Induction Task\u2014Problem Proposal Instruction.\nFigure 36. Program Induction Task\u2014Problem Proposal Instruction.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "43\n42",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\nGiven the following Code Snippet and the Output, think step by step then provide one possible\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\nseparate them with commas.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Output:\n```output\n{OUTPUT}\n```\n# Output Format:\n```input\narg1, arg2, ...\n```\n# Example Output:\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\nGiven the following Code Snippet and the Output, think step by step then provide one possible\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\nseparate them with commas.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Output:\n```output\n{OUTPUT}\n```\n# Output Format:\n```input\narg1, arg2, ...\n```\n# Example Output:\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "caption",
            "text": "Figure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\nreturns multiple values, remember to use a tuple to wrap them.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Input:\n```input\n{INPUT}\n```\n# Example Output:\n```output\n{{'age': 20, 'city': 'New York'}}\n```\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\nreturns multiple values, remember to use a tuple to wrap them.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Input:\n```input\n{INPUT}\n```\n# Example Output:\n```output\n{{'age': 20, 'city': 'New York'}}\n```",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "Figure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "44\n44\n43",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "# Task: Deduce the Function that Produced the Outputs from the Inputs\nGiven a set of input/output pairs and a message that describes the function, think through the\nproblem step by step to deduce a general code snippet. This code should produce the hidden\noutputs from the hidden inputs, matching the original data-generating code that created the\ninput/output pairs. Place your final answer inside python tags! It may be helpful to work\nthrough each input/output pair individually to test your function. If your function doesn't\nwork as expected, revise it until it does. The final code snippet will be used to evaluate\nyour response, which is wrapped in ```python``` tags.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n# Code Requirements:\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n!\u2192 `f`\n- Ensure the function returns a value\n- Include at least one input parameter\n- Make the function deterministic\n- AVOID THE FOLLOWING:\n* Random functions or variables\n* Date/time operations\n* I/O operations (reading files, network requests)\n* Printing or logging\n* Any external state\n- Ensure execution completes within 10 seconds on a modern CPU\n- All imports and class definitions should be at the very top of the code snippet\n- The snippet should end with a return statement from the main function `f()`, anything after\n!\u2192 will be removed\n# Input and Output Pairs:\n{INPUT_OUTPUT_PAIRS}\n# Message:\n```message\n{MESSAGE}\n```\n# Example Output:\n```python\ndef f(a):\nreturn a\n```\nName your entry function `f()`!!!",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "figure",
            "text": "Figure 39. Program Induction Task\u2014Problem Solving Prompt.\nFigure 39. Program Induction Task\u2014Problem Solving Prompt.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "45\n44",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Task: Manual Constructed Sudoku Abduction Task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "Model Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\nimport random\nfrom typing import List\nimport copy\ndef g_0(board: List[List[str]]) -> bool:\ndef is_valid_block(block):\nreturn sorted(block) == [str(i) for i in range(1, 10)]\nfor row in board:\nif not is_valid_block(row):\nreturn False\nfor col in zip(*board):\nif not is_valid_block(col):\nreturn False\nfor box_row in range(0, 9, 3):\nfor box_col in range(0, 9, 3):\nblock = [\nboard[r][c]\nfor r in range(box_row, box_row + 3)\nfor c in range(box_col, box_col + 3)\n]\nif not is_valid_block(block):\nreturn False\nreturn True\ndef f(board: List[List[str]]) -> List[List[str]]:\nfor i in range(9):\nfor j in range(9):\nif board[i][j] == \".\":\nraise ValueError(\"Invalid board\")\nnum_to_mask = 51\nif not g_0(board):\nraise ValueError(\"Invalid board\")\nmasked_board = copy.deepcopy(board)\nall_positions = [(i, j) for i in range(9) for j in range(9)]\nrandom.shuffle(all_positions)\nfor i, j in all_positions[:num_to_mask]:\nmasked_board[i][j] = \".\"\nreturn masked_board\n45",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Output:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "To determine the input that produced the given output, we need to analyze the function 'f' and understand",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "what it does. Let's break down the function step by step:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "is not valid, 'g_0' returns 'False'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "raises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "- The masked board is then returned.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Given the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"],[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "\"' We can see that the board has 51 positions masked with \".\". Therefore, the input board must have been a",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "complete board, and 'f' masked 51 random positions.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Let's construct a complete board that, when masked, can produce the given output. We need to fnd a board",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "such that after masking 51 positions, we get the output shown.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Here is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Let's verify that this board, when passed to the function 'f', produces the given output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "board will produce the given output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Therefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 40. Vibe Check 1: Sudoku.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Task: Manual Constructed Sudoku Abduction Task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Model Output:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "To determine the input that produced the given output, we need to analyze the function 'f' and understand\nwhat it does. Let's break down the function step by step:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "if each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\nis not valid, 'g_0' returns 'False'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n- The masked board is then returned.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Given the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"], [\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],\n[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"], [\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],\n[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"], [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],\n[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] \"' We can see that the board has 51 positions masked with \".\". Therefore,\nthe input board must have been a complete board, and 'f' masked 51 random positions.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Let's construct a complete board that, when masked, can produce the given output. We need to fnd a board\nsuch that after masking 51 positions, we get the output shown.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Here is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Let's verify that this board, when passed to the function 'f', produces the given output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "board will produce the given output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Therefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 40. Vibe Check 1: Sudoku.\nFigure 40. Vibe Check 1: Sudoku Solver. We cast Sudoku solving as an abduction task: our program starts from a fully solved and\nvalidated Sudoku board and simulates the masking of 51 random cells. The masked board is then presented as output, and the model\nis tasked with inferring the original input \u2014 effectively solving the puzzle backwards. An example solution is shown above, where\nAZR-Coder-14b verifies its initial guess before correctly answering. Generation parameters: temperature=0.6.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "46",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "46",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "code",
            "text": "Task: Manually Constructed Sum Product Game Solve Abduction Task\nModel Input: Here is the function f and the output: 'True', please predict the input.\ndef g_3():\nreturn [(x, y) for x in range(2, 101) for y in range(x + 1, 101) if x + y <= 100]\ndef g_2(pairs):\nd = {}\nfor (x, y) in pairs:\ns = x + y\nd.setdefault(s, []).append((x, y))\nreturn d\ndef g_1(pairs):\nd = {}\nfor (x, y) in pairs:\np = x * y\nd.setdefault(p, []).append((x, y))\nreturn d\ndef g_0():\npairs = g_3()\nsum_dict = g_2(pairs)\nprod_dict = g_1(pairs)\nallowed_sums = [s for s, candidate_pairs in sum_dict.items() \\\nif all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]\ncandidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]\nnew_prod_dict = g_1(candidates_after_S)\nunique_products = {prod: pair_list for prod, pair_list in new_prod_dict.items() \\\nif len(pair_list) == 1}\nfinal_candidates = []\nfor s in allowed_sums:\nvalid_pairs = [pair for pair in candidates_after_S \\\nif pair[0] + pair[1] == s and (pair[0] * pair[1]) in unique_products]\nif len(valid_pairs) == 1:\nfinal_candidates.append(valid_pairs[0])\nreturn sorted(set(final_candidates))\ndef f(x, y):\nif not (isinstance(x, int) and isinstance(y, int)):\nreturn False\nif x <= 1 or y <= 1 or y <= x or (x + y) > 100:\nreturn False\nfinal_candidates = g_0()\nreturn len(final_candidates) == 1 and final_candidates[0] == (x, y)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "Let's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n47",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Output:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "iterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "To fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "satisfes this condition is 1.\nLet's consider a simple example:\n-numbers = [1, 2]\n-target = 1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Here's the step-by-step calculation:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Since no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Consider:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "-numbers = [1, 0]\n-target = 1\nStep-by-step calculation:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "...(shortened)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "The pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Given the complexity and the fact that no simple example seems to work, let's try a more complex scenario:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "-numbers = [2, -1, 1]\n-target = 1\nStep-by-step calculation:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "The pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Therefore, one possible input that could produce the output 1 is: [2, -1, 1], 1",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "abduction task (predict input).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Task: Manually Constructed Sum Product Game Solve Abduction Task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Model Input: Here is the function f and the output: 'True', please predict the input.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "Model Output:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "the function 'f(x, y)' returns 'True' if the following conditions are met:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. 'x' and 'y' are both integers.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "2. 'x' and 'y' satisfy '1 < x < y' and 'x + y <= 100'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "3. The fnal candidates after applying the conditions in 'g_0' are exactly one pair, and that pair is '(x, y)'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Let's analyze the function 'g_0' step by step to understand what it does:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "1. 'pairs = g_3()' generates all pairs '(x, y)' such that '2 <= x < y <= 100' and 'x + y <= 100'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "2. 'sum_dict = g_2(pairs)' creates a dictionary where the keys are the sums of the pairs and the values are lists of pairs that add up\nto that sum.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "3. 'prod_dict = g_1(pairs)' creates a dictionary where the keys are the products of the pairs and the values are lists of pairs that",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "have that product.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "4. 'allowed_sums = [s for s, candidate_pairs in sum_dict.items() if all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]'\nflters out the sums 's' for which all pairs '(x, y)' that add up to 's' have more than one pair with the same product.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "5. 'candidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]' flters out the pairs whose sum is in\n'allowed_sums'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "6. 'new_prod_dict = g_1(candidates_after_S)' creates a new product dictionary using the fltered pairs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "7. 'unique_products = prod: pair_list for prod, pair_list in new_prod_dict.items() if len(pair_list) == 1' creates a dictionary of\nproducts that have exactly one pair.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "8. 'fnal_candidates = []' initializes a list of fnal candidates.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "9. The loop 'for s in allowed_sums:' iterates over each sum in 'allowed_sums' and checks if there is exactly one pair '(x, y)' such\nthat 'x + y = s' and 'x * y' is in 'unique_products'. If so, this pair is added to 'fnal_candidates'.\n10. The function returns 'sorted(set(fnal_candidates))', which should be a list containing exactly one pair if the conditions are\nmet.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Given that the output is 'True', it means that there is exactly one pair '(x, y)' that satisfes all the conditions. Let's identify this pair.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "To do this, we need to understand the mathematical conditions and constraints:\n1. '1 < x < y' and 'x + y <= 100'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "2. The pair '(x, y)' must be the only pair whose sum is in 'allowed_sums' and whose product is unique among the pairs with the\nsame sum.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "By analyzing the constraints and the nature of the problem, we can deduce that the pair '(x, y)' must be such that the sum 'x + y' is\nunique in the sense that all pairs adding up to this sum have products that are not unique, except for this pair.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "After detailed analysis or by running the code with diferent values, we fnd that the pair '(x, y) = (4, 13)' satisfes all the conditions.\nLet's verify:",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "1. '1 < 4 < 13' and '4 + 13 = 17 <= 100'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "2. We need to check the 'allowed_sums' and the uniqueness of the product '4 * 13 = 52'.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "By running the code or logical deduction, we can confrm that '(4, 13)' is the only pair that satisfes all the conditions.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Therefore, the input that produced the output 'True' is:\n\"'input\n(4, 13)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "\"'",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Figure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\nabduction task (predict input). 48\nFigure 41. Vibe Check 2: Sum Product Game. We cast the Sum-Product Game as an abduction task, where when the constraints of the\ngame are satisfied, function 'f' returns True, return False otherwise. AZR-Coder-14b was able to first analyze the complicated constraints,\nidentify candidate solution and verify. Generation parameters: temperature=0.6, top_p=0.95.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "48",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "D. Alternative Approaches Considered",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "In this section, we share many of the approaches we tried that did not prove to be particularly helpful for Absolute Zero Reasoner.\nHowever, we believe it is especially valuable to share these findings with the community, as they are crucial for guiding future research.\nBelow, we outline each of the additional methods we explored during the development of our project.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "D.1. Error Deduction Task",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Since programming languages often have error messages, and these messages contain a lot of information about how someone might\nexpect a program to run, we also came up with another task domain: allowing the learner to propose a program that will produce an\nerror, and requiring the solver to deduce what kind of error is raised when executing this code. We experimented with this additional\ntask alongside the induction (f), deduction (o), and abduction (i) tasks. Unfortunately, we did not observe noticeable changes in\ndownstream performance with this additional task and since it requires more computational resources than our AZR setup, we decided\nnot to incorporate it into our final version. However, we believe further thorough investigation of this is well deserved.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "D.2. Composite Functions as Curriculum Learning",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "One valuable property we can leverage from programming languages is the ability to compose functions\u2014that is, to define a function as\na composite of other functions, i.e., f(g(x)). In our setting, when generating a program, we can not only require the output to be a valid\nprogram but also constrain the LLM to utilize a predefined set of programs within its main function. For example, if the target program\nto be generated is f(\u00b7), we can sample a set of previously generated programs {g_0, . . . , gc} from D, and force a valid program to be\nf(g_0, \u00b7 \u00b7 \u00b7 , gc, i).",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Since all programs are generated by the LLM itself, this setup allows the model to bootstrap from its earlier generations, automatically\nincreasing the complexity of the generated programs. We interpret this mechanism as a form of curriculum learning: earlier programs\nin the AZR self-play loop tend to be simpler, and as the loop progresses, they become increasingly complex. By composing newer\nprograms from progressively more difficult earlier ones, the resulting programs naturally inherit this growing difficulty, which in turn\nchallenges the solver step.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "For implementation, in generating tasks for abduction and deduction, we begin by sampling a binary decision from a binomial distribution\nwith p = 0.5. This determines whether the generated program should be a simple program or a composite one. If the sample is 0, we\nprompt the LLM to generate a standard program along with a corresponding input. If the sample is 1, we prompt the LLM to generate a\ncomposite program. To construct the composite, we first sample an integer c \u223c U(1, 3), then uniformly select c programs from the\ndataset D that are not themselves composite programs. Finally, we prompt the LLM to generate a valid program that incorporates\n{g_0, . . . , gc} as subcomponents, ensuring it composes these selected programs meaningfully. We additionally filter programs that did\nnot utilize all the c programs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "However, we did not observe a significant difference when using this more complex curriculum compared to our simpler and more\neffective approach. One failure mode we encountered was that the model often defaulted to simply returning \"g(x)\", effectively learning\nf(g(x)) = g(x), which failed to introduce any additional difficulty. This trivial behavior undermined the intended challenge, leading us\nto deprioritize further exploration in this direction. While it may be possible to design a stricter reward mechanism\u2014such as enforcing\nf(g(x)) \u0338= g(x) by executing the code via a Python interpreter and penalizing such shortcuts\u2014we leave this to future work.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "D.3. Toying with the Initial p(z)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "We investigated a setting where the initial seed buffer (see Section 3.3.1 on how we generated these), i.e. p(z) in Equation (3), is not\nself-generated by the base model, but instead sourced from the LeetCode Dataset. We only modified this component and ran AZR\nusing the same procedure as before, continuing to add new valid programs to the initialized buffer. We observed an increase in initial\nperformance on coding benchmarks; however, the performance plateaued at roughly the same level after additional training steps,\ncompared to our official AZR setup. Interestingly, math performance was lower than in the official AZR setup, pointing towards that\non-policy data may be more beneficial to the learner to bootstrap from for mathematical reasoning. We believe that exploring different\nstrategies for initializing and updating p(z) is an important and exciting direction for future research. We briefly explored different\nstrategies for sampling reference code, ultimately settling on uniform sampling for its simplicity, though we also experimented with\nrecency-based sampling and observed potential collapse.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "D.4. Extra Rewards",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Complexity Rewards. Code complexity is well studied in software science and could potentially be a good proxy for measuring\nhow hard it is to infer the properties of a piece of code for our reasoning learner. Therefore, for the problem proposer, we can add various\nmeasures of complexity\u2014such as Cyclomatic Complexity (Ebert et al., 2016), maintainability, etc.\u2014to the reward function to incentivize\nthe proposer to produce more complex programs. For illustration purposes, we tried using the Maintainability measure and the Halstead",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "49",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      },
      {
        "blocks": [
          {
            "type": "pageheader",
            "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "complexity measure (Halstead, 1977) as intrinsic rewards. Concretely, we used the complexipy and Radon packages (Lopez, 2025;\nCanal, 2023) to implement the respective metrics. These are then served as intrinsic rewards during the AZR self-play phase.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Diversity Rewards. We also attempted using diversity rewards to . Inspired by DiveR-CT (Zhao et al., 2025a), we incorporate\ncode edit distance as an intrinsic reward. Specifically, we treat the reference programs shown in the prompt as anchors and compute the\naverage code edit distance between the generated program and these anchors. This serves as a measure of diversity in the generated\noutput. Additionally, we explored another diversity-based reward inspired by the notion of surprise (Zhao et al., 2022). In this approach,\nwe construct a probability distribution over previously encountered input/output pairs that the solver has answered. The reward is then\ndefined as 1 \u2212 p(input/output), where p denotes the empirical probability of a particular input or output. While both strategies were\nevaluated in our experiments, we did not observe a significant difference in performance. However, we believe this aspect warrants\ndeeper investigation, as diversity rewards remain a promising avenue for strengthening AZR further.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "textinlinemath",
            "text": "Reward Aggregation. We tested several ways on how to combine rewards for the proposer and discriminator. First, we separate\nthe reward into extrinsic reward rextrinsic and a set of intrinsic reward(s) I = {ri}, and tested the following strategies to combine them\ninto a single reward,",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "r = rextrinsic +\nX\n|I|\ni\nri, (11)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "r = rextrinsic \u00b7\nX\n|I|\ni\nri, (12)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "r = rextrinsic \u00b7\nY\n|I|\ni\nri, (13)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "equation",
            "text": "r = rextrinsic +\nY\n|I|\ni\nri. (14)",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We found that the simple additive way of combining rewards, a.k.a Equation (11), produced the most stable runs, possibly due to less\nvariance.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "section_header",
            "text": "D.5. Environment Transition",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "We investigated how the transition function in our coding environment for the proposer. Specifically, after generating a piece of code, we\ncan apply a transformation function on it before giving it making it an valid tuple in our dataset. We investigated two",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Removing Comments and Docstrings In early iterations of our experiments, we noticed that comments and docstrings\nwere sometimes used to explicitly outline what the function was doing, or even served as a partial \"note-taking\" interleaved \"ReAct\"\nprocess (Yao et al., 2023) of generating code\u2014that is, the model could interleave think and action at the same time, and to make the\ngenerated code valid, it used comments to encase its thoughts (Appendix C.3), similarly observed in DeepSeek-Prover-V2: (Ren et al.,\n2025). We then thought that to make the task harder for the solver, we should occlude this information from it. However, we observed\na significant performance drop after removing all comments and docstrings. One explanation for this phenomenon is that the only\n\"communication\" channel between the proposer and the solver is restricted to the code itself, rather than some kind of \"message\" along\nwith the code. These messages can potentially provide hints to the solver, thus making some otherwise impossible tasks solvable. As a\nresult, the solver is able to learn from its experience and self-bootstrap out of certain unsolvable tasks.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Removing Global Variables. We observed that some programs contain globally declared variables that may inadvertently leak\ninformation about the correct answer\u2014this issue is particularly prevalent in the input induction task generation and solving. Initially, we\nwere concerned that such leakage might lead to wasted computation on trivial or compromised examples. To address this, we developed\na systematic procedure to remove globally declared variables from the generated programs.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "However, after applying this cleaning step, we observed a noticeable drop in performance on our self-play reasoning tasks. One possible\nexplanation is that the generation step is unaware of this post-processing modification; since the reward is assigned after the transition\nfunction (which includes variable removal), the model may not learn effectively from this mismatch.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "text",
            "text": "Moreover, we believe that even when answers are present, the solver still engages in nontrivial reasoning to reach a solution, potentially\nbenefiting from this exposure. This aligns with the idea of rationalization as proposed in STaR (Zelikman et al., 2022), where the model\npretends to not see the answer but still performs reasoning during learning. Therefore, in our final experiments, we choose not to remove\nglobally declared variables, allowing the self-play loop to naturally incorporate and adapt to such cases.",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          },
          {
            "type": "pagefooter",
            "text": "50",
            "level": null,
            "language": null,
            "csv": null,
            "json_data": null,
            "breadcrumbs": null,
            "metadata": null
          }
        ]
      }
    ]
  },
  "metadata": {
    "title": "2505",
    "filepath": "data/input/2505.03335v2.pdf",
    "page_count": 50,
    "author": null,
    "creation_date": null,
    "modification_date": null,
    "language": "en",
    "subject": null,
    "keywords": [],
    "producer": null,
    "creator": null
  },
  "validation": {
    "corpus_validation": {
      "performed": false,
      "threshold": null,
      "raw_corpus_length": null
    }
  },
  "raw_corpus": {
    "full_text": "May 9, 2025\n\nMay 9, 2025\n\nMay 9, 2025\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAndrew Zhao 1\n, Yiran Wu3\n, Yang Yue 1\n, Tong Wu2\n, Quentin Xu1\n, Yang Yue 1\n, Matthieu Lin1\n,\nShenzhi Wang 1\n, Qingyun Wu3\n, Zilong Zheng 2, and Gao Huang 1,\n\n, Yiran Wu3\n\n3\n\n, Yang Yue 1\n\n1\n\n, Tong Wu2\n\n2\n\n, Quentin Xu1\n\n1\n\n, Yang Yue 1\n\n1\n\n, Matthieu Lin1\n\n1\n\nAndrew Zhao 1\n\n1\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nYiran Wu\n\nYang Yue\n\nTong Wu\n\nQuentin Xu\n\nYang Yue\n\nMatthieu Lin\n\nAndrew Zhao\n\n, Zilong Zheng 2, and Gao Huang 1,\n\n,\n\n,\n\nShenzhi Wang 1\n\n1\n\n, Qingyun Wu3\n\n3\n\n2\n\n1\n\nShenzhi Wang\n\n, Qingyun Wu\n\n, Zilong Zheng\n\nand Gao Huang\n\n1 Tsinghua University 2 Beijing Institute for General Artificial Intelligence 3 Pennsylvania State University\n\n1 Tsinghua University 2 Beijing Institute for General Artificial Intelligence 3 Pennsylvania State University\n\nTsinghua University\n\nBeijing Institute for General Artificial Intelligence\n\nPennsylvania State University\n\n1\n\n2\n\n3\n\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\n\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\n\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\n\narXiv:2505.03335v2 [cs.LG] 7 May 2025\n\narXiv:2505.03335v2 [cs.LG] 7 May 2025\n\narXiv:2505.03335v2 [cs.LG] 7 May 2025\n\nReinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning\ncapabilities of large language models by learning directly from outcome-based rewards. Recent RLVR\nworks that operate under the zero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training. The scarcity of high-\nquality, human-produced examples raises concerns about the long-term scalability of relying on human\nsupervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited\nlearning potential for a superintelligent system. To address these concerns, we propose a new RLVR\nparadigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own\nlearning progress and improves reasoning by solving them, without relying on any external data. Under\nthis paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training\ncurriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks\nand verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded\nlearning. Despite being trained entirely without external data, AZR achieves overall SOTA performance\non coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be\neffectively applied across different model scales and is compatible with various model classes.\n\nReinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning\n\nReinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning\n\ncapabilities of large language models by learning directly from outcome-based rewards. Recent RLVR\n\ncapabilities of large language models by learning directly from outcome-based rewards. Recent RLVR\n\nworks that operate under the zero setting avoid supervision in labeling the reasoning process, but still\n\nworks that operate under the\n\navoid supervision in labeling the reasoning process, but still\n\nzero setting\n\ndepend on manually curated collections of questions and answers for training. The scarcity of high-\n\ndepend on manually curated collections of questions and answers for training. The scarcity of high-\n\nquality, human-produced examples raises concerns about the long-term scalability of relying on human\n\nquality, human-produced examples raises concerns about the long-term scalability of relying on human\n\nsupervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a\n\nsupervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a\n\nhypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited\n\nhypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited\n\nlearning potential for a superintelligent system. To address these concerns, we propose a new RLVR\n\nlearning potential for a superintelligent system. To address these concerns, we propose a new RLVR\n\nparadigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own\n\nparadigm called\n\n, in which a single model learns to propose tasks that maximize its own\n\nAbsolute Zero\n\nlearning progress and improves reasoning by solving them, without relying on any external data. Under\n\nlearning progress and improves reasoning by solving them, without relying on any external data. Under\n\nthis paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training\n\nthis paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training\n\ncurriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks\n\ncurriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks\n\nand verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded\n\nand verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded\n\nlearning. Despite being trained entirely without external data, AZR achieves overall SOTA performance\n\nlearning. Despite being trained entirely\n\n, AZR achieves overall SOTA performance\n\nwithout external data\n\non coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens\n\non coding and mathematical reasoning tasks,\n\nthat rely on tens\n\noutperforming existing zero-setting models\n\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be\n\nof thousands of\n\n. Furthermore, we demonstrate that AZR can be\n\nin-domain human-curated examples\n\neffectively applied across different model scales and is compatible with various model classes.\n\neffectively applied across different model scales and is compatible with various model classes.\n\nCode Project Page Logs Models\nFigure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\n\nCode Project Page Logs Models\n\nCode Project Page Logs Models\n\nCode\n\nProject Page\n\nLogs\n\nModels\n\nFigure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\n\nFigure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\n\n. Without relying on any gold\n\nAbsolute Zero Reasoner (AZR) achieves state-of-the-art performance with\n\nZERO DATA\n\nFigure 1.\n\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\n\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\n\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\n\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\n\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\n\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\n\nCorresponding author(s)\n\nCorresponding author(s)\n\nCorresponding author(s)\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\nSupervised Learning Reinforcement Learning with Verifiable Rewards Absolute Zero (Ours)\nLess Human Supervision\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nSupervised Learning Reinforcement Learning with Verifiable Rewards Absolute Zero (Ours)\n\nSupervised Learning Reinforcement Learning with Verifiable Rewards Absolute Zero (Ours)\n\nReinforcement Learning with Verifiable Rewards\n\nAbsolute Zero (Ours)\n\nSupervised Learning\n\nLess Human Supervision\n\nLess Human Supervision\n\nLess Human Supervision\n\nFigure 2. Absolute Zero Paradigm. Supervised learning relies on human-curated reasoning traces for behavior cloning. Reinforcement\nlearning from verified rewards, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a\nrespective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm, Absolute\nZero, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks\noptimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that\nprovides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.\n\nFigure 2. Absolute Zero Paradigm. Supervised learning relies on human-curated reasoning traces for behavior cloning. Reinforcement\n\nrelies on human-curated reasoning traces for behavior cloning.\n\nAbsolute Zero Paradigm.\n\nSupervised learning\n\nFigure 2.\n\nReinforcement\n\nlearning from verified rewards, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a\n\n, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a\n\nlearning from verified rewards\n\nrespective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm, Absolute\n\nrespective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm,\n\nAbsolute\n\nZero, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks\n\n, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks\n\nZero\n\noptimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that\n\noptimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that\n\nprovides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.\n\nprovides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.\n\n1. Introduction\n\n1. Introduction\n\n1. Introduction\n\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement\nLearning with Verifiable Rewards (RLVR) (Lambert et al., 2024). Unlike methods that explicitly imitate intermediate reasoning steps,\nRLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets (DeepSeek-AI et al., 2025;\nTeam et al., 2025; Jaech et al., 2024; OpenAI, 2025b;a). A particularly compelling variant is the \"zero\" RLVR paradigm (DeepSeek-AI\net al., 2025), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies\nRLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of\nreasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability (Villalobos et al., 2024). As reasoning\nmodels continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable (Yue\net al., 2025). A similar scalability bottleneck has already been identified in the domain of LLM pretraining (Sutskever et al., 2024).\nFurthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed\ntasks risks imposing constraints on their capacity for autonomous learning and growth (Hughes et al., 2024). This underscores the need\nfor a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which\nAI systems may surpass human intelligence.\n\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement\n\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement\n\nLearning with Verifiable Rewards (RLVR) (Lambert et al., 2024). Unlike methods that explicitly imitate intermediate reasoning steps,\n\nLearning with Verifiable Rewards (RLVR)\n\n(Lambert et al.,\n\n2024)\n\n. Unlike methods that explicitly imitate intermediate reasoning steps,\n\nRLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets (DeepSeek-AI et al., 2025;\n\nRLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets\n\n(DeepSeek-AI et al.,\n\n2025;\n\nTeam et al., 2025; Jaech et al., 2024; OpenAI, 2025b;a). A particularly compelling variant is the \"zero\" RLVR paradigm (DeepSeek-AI\n\nTeam et al.,\n\nJaech et al.,\n\n. A particularly compelling variant is the\n\nRLVR paradigm\n\n2025;\n\n2024;\n\nOpenAI,\n\n2025b;\n\na)\n\n(DeepSeek-AI\n\n\"zero\"\n\net al., 2025), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies\n\net al.,\n\n, which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies\n\n2025)\n\nRLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of\n\nRLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of\n\nreasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability (Villalobos et al., 2024). As reasoning\n\nreasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability\n\n(Villalobos et al.,\n\n. As reasoning\n\n2024)\n\nmodels continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable (Yue\n\nmodels continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable\n\n(Yue\n\net al., 2025). A similar scalability bottleneck has already been identified in the domain of LLM pretraining (Sutskever et al., 2024).\n\net al.,\n\n2025)\n\n. A similar scalability bottleneck has already been identified in the domain of LLM pretraining\n\n(Sutskever et al.,\n\n2024)\n\n.\n\nFurthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed\n\nFurthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed\n\ntasks risks imposing constraints on their capacity for autonomous learning and growth (Hughes et al., 2024). This underscores the need\n\ntasks risks imposing constraints on their capacity for autonomous learning and growth\n\n(Hughes et al.,\n\n. This underscores the need\n\n2024)\n\nfor a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which\n\nfor a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which\n\nAI systems may surpass human intelligence.\n\nAI systems may surpass human intelligence.\n\nTo this end, we propose \"Absolute Zero\", a new paradigm for reasoning models in which the model simultaneously learns to define tasks\nthat maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In\ncontrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to\nhacking (Silver et al., 2017; Chen et al., 2025; 2024), the Absolute Zero paradigm is designed to operate in open-ended settings while\nremaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how\nhumans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models (Hughes\net al., 2024). Similar to AlphaZero (Silver et al., 2017), which improves through self-play, our proposed paradigm requires no human\nsupervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward\nenabling large language models to autonomously achieve superhuman reasoning capabilities.\n\nTo this end, we propose \"Absolute Zero\", a new paradigm for reasoning models in which the model simultaneously learns to define tasks\n\nTo this end, we propose\n\n, a new paradigm for reasoning models in which the model simultaneously learns to define tasks\n\n\"Absolute Zero\"\n\nthat maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In\n\nthat maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In\n\ncontrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to\n\ncontrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to\n\nhacking (Silver et al., 2017; Chen et al., 2025; 2024), the Absolute Zero paradigm is designed to operate in open-ended settings while\n\nhacking\n\n(Silver et al.,\n\n2017;\n\nChen et al.,\n\n2025;\n\n2024)\n\n, the\n\nparadigm is designed to operate in open-ended settings while\n\nAbsolute Zero\n\nremaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how\n\nremaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how\n\nhumans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models (Hughes\n\nhumans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models\n\n(Hughes\n\net al., 2024). Similar to AlphaZero (Silver et al., 2017), which improves through self-play, our proposed paradigm requires no human\n\net al.,\n\n2024)\n\n. Similar to AlphaZero\n\n(Silver et al.,\n\n2017)\n\n, which improves through self-play, our proposed paradigm requires no human\n\nsupervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward\n\nsupervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward\n\nenabling large language models to autonomously achieve superhuman reasoning capabilities.\n\nenabling large language models to autonomously achieve superhuman reasoning capabilities.\n\nBuilding on this new reasoning paradigm, we introduce the Absolute Zero Reasoner (AZR), which proposes and solves coding tasks. We\ncast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable\nfeedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a\nprogram, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We\ntrain the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of\nthe proposed approach.\n\nBuilding on this new reasoning paradigm, we introduce the Absolute Zero Reasoner (AZR), which proposes and solves coding tasks. We\n\nBuilding on this new reasoning paradigm, we introduce the\n\n, which proposes and solves coding tasks. We\n\nAbsolute Zero Reasoner (AZR)\n\ncast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable\n\ncast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable\n\nfeedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a\n\nfeedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a\n\nprogram, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We\n\nprogram, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We\n\ntrain the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of\n\ntrain the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of\n\nthe proposed approach.\n\nthe proposed approach.\n\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks\nin math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned\nwith domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically\ntrained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points\n\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks\n\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks\n\nin math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned\n\nin math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned\n\nwith domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically\n\nwith domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically\n\ntrained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points\n\ntrained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points\n\n2\n\n2\n\n2\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\ncompared to models trained in the \"zero\" setting using in-domain data. These surprising results highlight that general reasoning skills\ncan emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a\nfirst pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting\nfindings summarized below:\n\ncompared to models trained in the \"zero\" setting using in-domain data. These surprising results highlight that general reasoning skills\n\ncompared to models trained in the \"zero\" setting using in-domain data. These surprising results highlight that general reasoning skills\n\ncan emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a\n\ncan emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a\n\nfirst pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting\n\nfirst pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting\n\nfindings summarized below:\n\nfindings summarized below:\n\n\u2022 Code priors amplify reasoning. The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b.\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\nmay potentially amplify overall reasoning improvements after AZR training.\n\u2022 Cross domain transfer is more pronounced for AZR. After RLVR, expert code models raise math accuracy by only 0.65 points on\naverage, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\u2022 Bigger bases yield bigger gains. Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\u2022 Comments as intermediate plans emerge naturally. When solving code induction tasks, AZR often interleaves step-by-step plans\nas comments and code (Appendix C.3), resembling the ReAct prompting framework (Yao et al., 2023). Similar behavior has been\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B) (Ren et al., 2025). We therefore believe that allowing\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\u2022 Cognitive Behaviors and Token length depends on reasoning mode. Distinct cognitive behaviors\u2014such as step-by-step reasoning,\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\u2022 Safety alarms ringing. We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the\n\"uh-oh moment\", example shown in Figure 32, highlighting the need for future work on safety-aware training (Zhang et al., 2025a).\n\n\u2022 Code priors amplify reasoning. The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b.\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\nmay potentially amplify overall reasoning improvements after AZR training.\n\n\u2022 Code priors amplify reasoning. The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b.\n\nThe base\n\nmodel started with math performance 3.6 points lower than\n\n.\n\n\u2022\n\nCode priors amplify reasoning.\n\nQwen-Coder-7b\n\nQwen-7b\n\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\n\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\n\nmay potentially amplify overall reasoning improvements after AZR training.\n\nmay potentially amplify overall reasoning improvements after AZR training.\n\n\u2022 Cross domain transfer is more pronounced for AZR. After RLVR, expert code models raise math accuracy by only 0.65 points on\naverage, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\n\u2022 Cross domain transfer is more pronounced for AZR. After RLVR, expert code models raise math accuracy by only 0.65 points on\n\nAfter RLVR, expert code models raise math accuracy by only 0.65 points on\n\n\u2022\n\nCross domain transfer is more pronounced for AZR.\n\naverage, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and\n\naverage, whereas\n\ntrained on self-proposed code reasoning tasks improve math average by 10.9 and\n\nand\n\nAZR-Base-7B\n\nAZR-Coder-7B\n\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\n\u2022 Bigger bases yield bigger gains. Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\n\u2022 Bigger bases yield bigger gains. Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n\nPerformance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n\n\u2022\n\nBigger bases yield bigger gains.\n\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\n\u2022 Comments as intermediate plans emerge naturally. When solving code induction tasks, AZR often interleaves step-by-step plans\nas comments and code (Appendix C.3), resembling the ReAct prompting framework (Yao et al., 2023). Similar behavior has been\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B) (Ren et al., 2025). We therefore believe that allowing\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\n\u2022 Comments as intermediate plans emerge naturally. When solving code induction tasks, AZR often interleaves step-by-step plans\n\nWhen solving code induction tasks, AZR often interleaves step-by-step plans\n\n\u2022\n\nComments as intermediate plans emerge naturally.\n\nas comments and code (Appendix C.3), resembling the ReAct prompting framework (Yao et al., 2023). Similar behavior has been\n\nas comments and code (Appendix\n\nC.3)\n\n, resembling the ReAct prompting framework\n\n(Yao et al.,\n\n2023)\n\n. Similar behavior has been\n\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B) (Ren et al., 2025). We therefore believe that allowing\n\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B)\n\n(Ren et al.,\n\n. We therefore believe that allowing\n\n2025)\n\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\n\u2022 Cognitive Behaviors and Token length depends on reasoning mode. Distinct cognitive behaviors\u2014such as step-by-step reasoning,\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\n\u2022 Cognitive Behaviors and Token length depends on reasoning mode. Distinct cognitive behaviors\u2014such as step-by-step reasoning,\n\nDistinct cognitive behaviors\u2014such as step-by-step reasoning,\n\n\u2022\n\nCognitive Behaviors and Token length depends on reasoning mode.\n\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\n\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\n\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\n\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\n\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\n\u2022 Safety alarms ringing. We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the\n\"uh-oh moment\", example shown in Figure 32, highlighting the need for future work on safety-aware training (Zhang et al., 2025a).\n\n\u2022 Safety alarms ringing. We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the\n\nWe observe AZR with\n\noccasionally produces concerning chains of thought, we term the\n\n\u2022\n\nSafety alarms ringing.\n\nLlama3.1-8b\n\n\"uh-oh moment\", example shown in Figure 32, highlighting the need for future work on safety-aware training (Zhang et al., 2025a).\n\n\"uh-oh moment\", example shown in Figure\n\n32,\n\nhighlighting the need for future work on safety-aware training\n\n(Zhang et al.,\n\n2025a)\n\n.\n\n2. The Absolute Zero Paradigm\n\n2. The Absolute Zero Paradigm\n\n2. The Absolute Zero Paradigm\n\n2.1. Preliminaries\n\n2.1. Preliminaries\n\n2.1. Preliminaries\n\nSupervised Fine-Tuning (SFT). SFT requires the datasets of task-rationale-answer demonstrations D = {(x, c\u22c6\n, y\u22c6\n)}, where\nx is the query, c\n\u22c6\nis the gold chain-of-thought (CoT)) and y\n\u22c6\nis the gold answer, all provided by human experts or superior AI models.\nThe model trains to imitate the reference responses to minimize the conditional negative log-likelihood (Ouyang et al., 2022):\n\nSupervised Fine-Tuning (SFT). SFT requires the datasets of task-rationale-answer demonstrations D = {(x, c\u22c6\n\nSFT requires the datasets of task-rationale-answer demonstrations\n\n)}, where\n\n, where\n\n\u22c6\n\n, y\u22c6\n\n\u22c6\n\nSupervised Fine-Tuning (SFT).\n\n=\n\n(\n\n)\n\nx, c\n\n, y\n\nD\n\n{\n\n}\n\nx is the query, c\n\nis the query,\n\nis the gold chain-of-thought (CoT)) and y\n\nis the gold chain-of-thought (CoT)) and\n\nis the gold answer, all provided by human experts or superior AI models.\n\nis the gold answer, all provided by\n\nhuman experts\n\nor\n\nsuperior AI models.\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\nx\n\nc\n\ny\n\nThe model trains to imitate the reference responses to minimize the conditional negative log-likelihood (Ouyang et al., 2022):\n\nThe model trains to imitate the reference responses to minimize the conditional negative log-likelihood\n\n(Ouyang et al.,\n\n2022)\n\n:\n\nLSFT(\u03b8) = \u2212 E(x,c\u22c6,y\u22c6)\u223cD log \u03c0\u03b8\n\nc\n\u22c6\n, y\n\u22c6\n| x). (1)\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\nLSFT(\u03b8) = \u2212 E(x,c\u22c6,y\u22c6)\u223cD log \u03c0\u03b8\n\nE\n\n| x). (1)\n\n(1)\n\n(\n\n) =\n\nlog\n\n)\n\n\u03b8\n\n\u03c0\n\nc\n\nc\n\n, y\n\n, y\n\nx\n\n.\n\nSFT\n\nL\n\n\u2212\n\n|\n\n\u22c6\n\n\u22c6\n\n\u03b8\n\n(\n\n)\n\nx,c\n\n,y\n\n\u223cD\n\nHowever, at the frontier level, there's no stronger model to distill from, and expert human labeling doesn't scale well.\n\nHowever, at the frontier level, there's no stronger model to distill from, and expert human labeling doesn't scale well.\n\nHowever, at the frontier level, there's no stronger model to distill from, and expert human labeling doesn't scale well.\n\nReinforcement Learning with Verifiable Rewards (RLVR). To move beyond the limits of pure imitation, RLVR only\nrequires a dataset of task and answer D = {(x, y\u22c6\n)}, without labeled rationale. RLVR allows the model to generate its own CoT and\ncalculate a verifiable reward with the golden answer r(y, y\u22c6\n). However, the learning task distribution D, with its set of queries and gold\nanswers are still labeled by human experts. The trainable policy \u03c0\u03b8 is optimized to maximize expected reward:\n\nReinforcement Learning with Verifiable Rewards (RLVR). To move beyond the limits of pure imitation, RLVR only\n\nTo move beyond the limits of pure imitation, RLVR only\n\nReinforcement Learning with Verifiable Rewards (RLVR).\n\nrequires a dataset of task and answer D = {(x, y\u22c6\n\nrequires a dataset of task and answer\n\n)}, without labeled rationale. RLVR allows the model to generate its own CoT and\n\n, without labeled rationale. RLVR allows the model to generate its own CoT and\n\n\u22c6\n\n=\n\n(\n\n)\n\nx, y\n\nD\n\n{\n\n}\n\ncalculate a verifiable reward with the golden answer r(y, y\u22c6\n\ncalculate a verifiable reward with the golden answer\n\n). However, the learning task distribution D, with its set of queries and gold\n\n. However, the learning task distribution\n\n, with its set of queries and gold\n\n\u22c6\n\n(\n\n)\n\nr\n\ny, y\n\nD\n\nanswers are still labeled by human experts. The trainable policy \u03c0\u03b8 is optimized to maximize expected reward:\n\nanswers are still labeled by\n\nhuman experts. The trainable policy\n\nis optimized to maximize expected reward:\n\n\u03c0\n\n\u03b8\n\nJRLVR(\u03b8) = E(x,y\u22c6)\u223cD, y\u223c\u03c0\u03b8(\u00b7 |x)\n-\n\nr(y, y\n\u22c6\n)\n\n. (2)\n\n\u22c6\n\n\u22c6\n\nJRLVR(\u03b8) = E(x,y\u22c6)\u223cD, y\u223c\u03c0\u03b8(\u00b7 |x)\n\nE\n\n. (2)\n\n(2)\n\n-\n\n-\n\n(\n\n) =\n\nr(y, y\n\n(\n\n)\n\n)\n\nJ\n\n\u03b8\n\nr\n\ny, y\n\n.\n\nRLVR\n\n\u22c6\n\n(\n\n)\n\n(\n\n)\n\nx,y\n\n, y\n\n\u03c0\n\nx\n\n\u223cD\n\n\u223c\n\n\u00b7 |\n\n\u03b8\n\nIn summary, both SFT and RLVR still rely on human-curated datasets of either queries, demonstrations, or verifiers, which ultimately\nlimit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own\ninteractions with the environment entirely through self-play.\n\nIn summary, both SFT and RLVR still rely on human-curated datasets of either queries, demonstrations, or verifiers, which ultimately\n\nIn summary, both SFT and RLVR still rely on\n\nhuman-curated\n\ndatasets of either queries, demonstrations, or verifiers, which ultimately\n\nlimit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own\n\nlimit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own\n\ninteractions with the environment entirely through self-play.\n\ninteractions with the environment entirely through self-play.\n\n2.2. Absolute Zero\n\n2.2. Absolute Zero\n\n2.2. Absolute Zero\n\nWe propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from\nboth stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We\nillustrate this paradigm in Figure 2, which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach\neliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.\n\nWe propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from\n\nWe propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from\n\nboth stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We\n\nboth stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We\n\nillustrate this paradigm in Figure 2, which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach\n\nillustrate this paradigm in Figure\n\nwhich contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach\n\n2,\n\neliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.\n\neliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.\n\nTo make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid\nunderstanding, we include an illustration in Figure 3. Let \u03c0\u03b8 be our parameterized language model, it is used to play two roles, proposer\n\u03c0\npropose\n\u03b8\nand solver \u03c0\nsolve\n\u03b8 during training.\n\nTo make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid\n\nTo make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid\n\nunderstanding, we include an illustration in Figure 3. Let \u03c0\u03b8 be our parameterized language model, it is used to play two roles, proposer\n\nunderstanding, we include an illustration in Figure\n\nLet\n\nbe our parameterized language model, it is used to play two roles, proposer\n\n3.\n\n\u03c0\n\n\u03b8\n\npropose\n\npropose\n\nsolve\n\nsolve\n\nand solver \u03c0\n\nand solver\n\n\u03b8 during training.\n\nduring training.\n\n\u03c0\n\n\u03c0\n\n\u03c0\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\n3\n\n3\n\n3\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nLanguage Model\n!\"#!$%&\n'#()*\n\n, \u22c6\n, \"#$\"$%&\n\nEnvironment\n, \n%$'(&\nEnvironment\n\nFigure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent \u03c0\nproposing task \u03c4 , which is transformed by f with the environment e into a validated\nproblem (x, y\u22c6\n), and also emits a reward r\npropose for learnability. Then, a standard RL step\nfollows: the agent solves x by producing y, receiving reward r\nsolve from e by matching\nwith y\n\u22c6\n. \u03c0\npropose and \u03c0\nsolve are jointly trained and this process can be repeated indefinitely.\n\nLanguage Model\n!\"#!$%&\n'#()*\n\n, \u22c6\n, \"#$\"$%&\n\nEnvironment\n, \n%$'(&\nEnvironment\n\nThe proposer first samples a proposed\ntask conditioned on variable z: \u03c4 \u223c\n\u03c0\npropose\n\u03b8\n(\u00b7|z), which will then be validated\nand used to construct a valid reasoning task\ntogether with the environment e: (x, y\u22c6\n) \u223c\nfe(\u00b7|\u03c4 ), where x is the task query and y\n\u22c6\nis the gold label. Then the solver produces\nan answer y \u223c \u03c0\nsolve\n\u03b8\n(\u00b7 | x). Each pro-\nposed task \u03c4 is scored by a learnability\nreward r\npropose\ne (\u03c4, \u03c0\u03b8), which captures the\nexpected improvement in \u03c0\u03b8 after train-\ning on the task query x. Moreover, the\nsame policy also receives a solution re-\nward r\nsolve\ne (y, y\u22c6\n) for its answer to the task\nquery x, with the environment again serv-\n\nThe proposer first samples a proposed\n\nThe proposer first samples a proposed\n\ntask conditioned on variable z: \u03c4 \u223c\n\ntask conditioned on variable\n\n:\n\nz\n\n\u03c4\n\n\u223c\n\npropose\n\npropose\n\n(\u00b7|z), which will then be validated\n\n, which will then be validated\n\n(\n\n)\n\n\u03c0\n\n\u03c0\n\nz\n\n\u00b7|\n\n\u03b8\n\n\u03b8\n\n'#()*\n\n#()*\n\n'\n\nand used to construct a valid reasoning task\n\nand used to construct a valid reasoning task\n\nEnvironment\n\nEnvironment\n\nEnvironment\n\nEnvironment\n\nLanguage Model\n\nLanguage Model\n\ntogether with the environment e: (x, y\u22c6\n\ntogether with the environment\n\n:\n\n\u22c6\n\n,\n\n,\n\n(\n\n) \u223c\n\n)\n\ne\n\nx, y\n\n!\"#!$%&\n\n!\"#!$%&\n\n\u223c\n\nfe(\u00b7|\u03c4 ), where x is the task query and y\n\n, where\n\nis the task query and\n\n\u22c6\n\n\u22c6\n\n(\n\n)\n\nx\n\ny\n\nf\n\n\u03c4\n\n\u00b7|\n\ne\n\nis the gold label. Then the solver produces\n\nis the gold label. Then the solver produces\n\n, \u22c6\n\n\u22c6\n\n, \"#$\"$%&\n\n\"#$\"$%&\n\n%$'(&\n\n%$'(&\n\n,\n\n,\n\nsolve\n\nsolve\n\nan answer y \u223c \u03c0\n\nan answer\n\n(\u00b7 | x). Each pro-\n\n. Each pro-\n\n(\n\n)\n\ny\n\n\u03c0\n\nx\n\n\u223c\n\n\u00b7 |\n\n\u03b8\n\n\u03b8\n\nposed task \u03c4 is scored by a learnability\n\nposed task\n\nis scored by a\n\nlearnability\n\n\u03c4\n\npropose\n\npropose\n\ne (\u03c4, \u03c0\u03b8), which captures the\n\n, which captures the\n\nFigure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent \u03c0\nproposing task \u03c4 , which is transformed by f with the environment e into a validated\nproblem (x, y\u22c6\n), and also emits a reward r\npropose for learnability. Then, a standard RL step\nfollows: the agent solves x by producing y, receiving reward r\nsolve from e by matching\nwith y\n\u22c6\n. \u03c0\npropose and \u03c0\nsolve are jointly trained and this process can be repeated indefinitely.\n\nFigure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent \u03c0\n\nThe Absolute Zero loop begins with the agent\n\n(\n\n)\n\nreward r\n\nreward\n\nr\n\n\u03c4, \u03c0\n\nThe Absolute Zero Loop.\n\nFigure 3.\n\n\u03c0\n\ne\n\n\u03b8\n\nexpected improvement in \u03c0\u03b8 after train-\n\nexpected improvement in\n\nafter train-\n\nproposing task \u03c4 , which is transformed by f with the environment e into a validated\n\nproposing task\n\n, which is transformed by\n\nwith the environment\n\ninto a validated\n\n\u03c0\n\n\u03c4\n\nf\n\ne\n\n\u03b8\n\ning on the task query x. Moreover, the\n\ning on the task query\n\n. Moreover, the\n\npropose for learnability. Then, a standard RL step\n\npropose\n\n), and also emits a reward r\n\n, and also emits a reward\n\nfor learnability. Then, a standard RL step\n\nproblem (x, y\u22c6\n\nproblem\n\n\u22c6\n\nx\n\n(\n\n)\n\nx, y\n\nr\n\nsame policy also receives a solution re-\n\nsame policy also receives a\n\nsolve from e by matching\n\nsolve\n\nfollows: the agent solves x by producing y, receiving reward r\n\nfollows: the agent solves\n\nby producing\n\n, receiving reward\n\nfrom\n\nby matching\n\nsolution re-\n\nx\n\ny\n\nr\n\ne\n\nsolve\n\nsolve\n\n) for its answer to the task\n\nfor its answer to the task\n\npropose and \u03c0\n\npropose\n\nsolve are jointly trained and this process can be repeated indefinitely.\n\nsolve\n\ne (y, y\u22c6\n\n\u22c6\n\nare jointly trained and this process can be repeated indefinitely.\n\nwith y\n\nwith\n\n. \u03c0\n\n.\n\nand\n\n\u22c6\n\n\u22c6\n\n(\n\n)\n\nward r\n\nward\n\nr\n\ny, y\n\ny\n\n\u03c0\n\n\u03c0\n\ne\n\nquery x, with the environment again serv-\n\nquery\n\n, with the environment again serv-\n\nx\n\ning as the verifier. A nonnegative coefficient \u03bb balances the trade-off between exploring new, learnable tasks and improving the model's\nreasoning and problem-solving abilities. We formally define the absolute zero setting's objective as follows:\n\ning as the verifier. A nonnegative coefficient \u03bb balances the trade-off between exploring new, learnable tasks and improving the model's\n\ning as the verifier. A nonnegative coefficient\n\nbalances the trade-off between exploring new, learnable tasks and improving the model's\n\n\u03bb\n\nreasoning and problem-solving abilities. We formally define the absolute zero setting's objective as follows:\n\nreasoning and problem-solving abilities. We formally define the absolute zero setting's objective as follows:\n\nJ (\u03b8) := max\n\u03b8\nEz\u223cp(z)\n\"\nE(x,y\u22c6)\u223cfe(\u00b7|\u03c4),\u03c4\u223c\u03c0\npropose\n\u03b8\n(\u00b7|z)\n\nr\npropose\ne (\u03c4, \u03c0\u03b8) + \u03bb Ey\u223c\u03c0solve\n\u03b8\n(\u00b7|x)\n-\n\nr\nsolve\ne (y, y\n\u22c6\n)\n\n\n#\n. (3)\n\n\"\n\n\"\n\n#\n\n#\n\npropose\n\npropose\n\nsolve\n\nsolve\n\n\u22c6\n\n\u22c6\n\nEz\u223cp(z)\n\nE\n\nE(x,y\u22c6)\u223cfe(\u00b7|\u03c4),\u03c4\u223c\u03c0\n\nE\n\ne (\u03c4, \u03c0\u03b8) + \u03bb Ey\u223c\u03c0solve\n\nE\n\n. (3)\n\n(3)\n\n-\n\n-\n\nJ (\u03b8) := max\n\n:= max\n\n(\n\n)\n\n(\n\n) +\n\ne (y, y\n\n(\n\n)\n\n)\n\n\u03b8\n\nr\n\nr\n\n\u03c4, \u03c0\n\n\u03bb\n\nr\n\nr\n\ny, y\n\n.\n\npropose\n\npropose\n\nsolve\n\nJ\n\n\u03b8\n\n(\n\n)\n\n\u22c6\n\ne\n\ne\n\nz\n\np\n\nz\n\n(\n\n)\n\n(\n\n)\n\n(\u00b7|z)\n\n(\n\n)\n\n(\u00b7|x)\n\n(\n\n)\n\nx,y\n\nf\n\n\u03c4\n\n,\u03c4\n\n\u03c0\n\nz\n\ny\n\n\u03c0\n\nx\n\n\u223c\n\n\u223c\n\n\u00b7|\n\n\u223c\n\n\u00b7|\n\n\u223c\n\n\u00b7|\n\ne\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\nNotice that we shift the burden of scaling data away from human experts and onto the proposer policy \u03c0\npropose\n\u03b8\nand the environment\ne. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing\ngrounded feedback that supports stable and self-sustainable training. When proposing, z acts as a conditional variable that seeds\ngeneration of tasks. Practically, z can be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated\ntask memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward\nr\npropose(\u03c4, \u03c0\u03b8), which measures how much the model is expected to improve by solving a proposed task \u03c4 . Moreover, the solver reward\nr\nsolve(y, y\u2217\n) evaluates the correctness of the model's output. Together, these two signals guide the model to propose tasks that are both\nchallenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.\n\npropose\n\npropose\n\nNotice that we shift the burden of scaling data away from human experts and onto the proposer policy \u03c0\n\nNotice that we shift the burden of scaling data away from\n\nhuman experts\n\nand onto the\n\nproposer policy\n\nand the environment\n\nand the\n\nenvironment\n\n\u03c0\n\n\u03b8\n\n\u03b8\n\ne. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing\n\n. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing\n\ne\n\ngrounded feedback that supports stable and self-sustainable training. When proposing, z acts as a conditional variable that seeds\n\ngrounded feedback that supports stable and self-sustainable training. When proposing,\n\nacts as a conditional variable that seeds\n\nz\n\ngeneration of tasks. Practically, z can be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated\n\ngeneration of tasks. Practically,\n\ncan be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated\n\nz\n\ntask memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward\n\ntask memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward\n\npropose(\u03c4, \u03c0\u03b8), which measures how much the model is expected to improve by solving a proposed task \u03c4 . Moreover, the solver reward\n\npropose\n\n, which measures how much the model is expected to improve by solving a proposed task\n\n. Moreover, the solver reward\n\n(\n\n)\n\nr\n\nr\n\n\u03c4, \u03c0\n\n\u03c4\n\n\u03b8\n\nsolve(y, y\u2217\n\nsolve\n\n) evaluates the correctness of the model's output. Together, these two signals guide the model to propose tasks that are both\n\nevaluates the correctness of the model's output. Together, these two signals guide the model to propose tasks that are both\n\n(\n\n)\n\nr\n\nr\n\ny, y\n\n\u2217\n\nchallenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.\n\nchallenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.\n\n3. Absolute Zero Reasoner\n\n3. Absolute Zero Reasoner\n\n3. Absolute Zero Reasoner\n\nIn this section, we present Absolute Zero Reasoner (AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an\nunified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them\nto improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of\nreasoning capacity while enhancing its ability to solve them effectively (Section 3.1). Within this self-play training paradigm, the model\nlearns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and\ninduction (Section 3.2). Using coding tasks is motivated by the Turing-completeness of programming languages (Stuart, 2015) and\nempirical evidence that code-based training improves reasoning (Aryabumi et al., 2024). We adopt code as an open-ended, expressive,\nand verifiable medium for enabling reliable task construction and verification (Section 3.3). Finally, the model is updated using a newly\nproposed advantage estimator designed for multitask learning (Section 3.3.5). We outline the overall algorithm in Algorithm 1 and\nhighlight an illustration of our Absolute Zero Reasoner approach in Figure 4. To expedite future exploration in this area, we also present\nseveral attempts that did not yield fruitful results but still warrant discussion in Appendix D.\n\nIn this section, we present Absolute Zero Reasoner (AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an\n\nIn this section, we present\n\n(AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an\n\nAbsolute Zero Reasoner\n\nunified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them\n\nunified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them\n\nto improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of\n\nto improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of\n\nreasoning capacity while enhancing its ability to solve them effectively (Section 3.1). Within this self-play training paradigm, the model\n\nreasoning capacity while enhancing its ability to solve them effectively (Section\n\n. Within this self-play training paradigm, the model\n\n3.1)\n\nlearns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and\n\nlearns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and\n\ninduction (Section 3.2). Using coding tasks is motivated by the Turing-completeness of programming languages (Stuart, 2015) and\n\ninduction (Section\n\n3.2)\n\n. Using coding tasks is motivated by the Turing-completeness of programming languages\n\n(Stuart,\n\n2015)\n\nand\n\nempirical evidence that code-based training improves reasoning (Aryabumi et al., 2024). We adopt code as an open-ended, expressive,\n\nempirical evidence that code-based training improves reasoning\n\n(Aryabumi et al.,\n\n. We adopt code as an open-ended, expressive,\n\n2024)\n\nand verifiable medium for enabling reliable task construction and verification (Section 3.3). Finally, the model is updated using a newly\n\nand verifiable medium for enabling reliable task construction and verification (Section\n\n. Finally, the model is updated using a newly\n\n3.3)\n\nproposed advantage estimator designed for multitask learning (Section 3.3.5). We outline the overall algorithm in Algorithm 1 and\n\nproposed advantage estimator designed for multitask learning (Section\n\n3.3.5)\n\n. We outline the overall algorithm in Algorithm\n\n1\n\nand\n\nhighlight an illustration of our Absolute Zero Reasoner approach in Figure 4. To expedite future exploration in this area, we also present\n\nhighlight an illustration of our Absolute Zero Reasoner approach in Figure\n\nTo expedite future exploration in this area, we also present\n\n4.\n\nseveral attempts that did not yield fruitful results but still warrant discussion in Appendix D.\n\nseveral attempts that did not yield fruitful results but still warrant discussion in Appendix\n\nD.\n\n3.1. Two Roles in One: Proposer and Solver\n\n3.1. Two Roles in One: Proposer and Solver\n\n3.1. Two Roles in One: Proposer and Solver\n\nLarge language models are naturally suited for implementing AZR in a multitask learning context (Radford et al., 2019), as both\nthe formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a\nsingle model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective\nin Equation (3). At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined\nin Section 3.2) and K past self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples,\npromoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning\ntasks that can be verified using the environment, outlined later in Section 3.3. AZR then attempts to solve these newly proposed tasks,\nreceiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning.\nWe now outline the rewards used for each role.\n\nLarge language models are naturally suited for implementing AZR in a multitask learning context (Radford et al., 2019), as both\n\nLarge language models are naturally suited for implementing AZR in a multitask learning context\n\n(Radford et al.,\n\n2019)\n\n, as both\n\nthe formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a\n\nthe formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a\n\nsingle model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective\n\nsingle model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective\n\nin Equation (3). At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined\n\nin Equation\n\n. At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined\n\n(3)\n\nin Section 3.2) and K past self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples,\n\nin Section\n\nand\n\npast self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples,\n\n3.2)\n\nK\n\npromoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning\n\npromoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning\n\ntasks that can be verified using the environment, outlined later in Section 3.3. AZR then attempts to solve these newly proposed tasks,\n\ntasks that can be verified using the environment, outlined later in Section\n\n3.3.\n\nAZR then attempts to solve these newly proposed tasks,\n\nreceiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning.\n\nreceiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning.\n\nWe now outline the rewards used for each role.\n\nWe now outline the rewards used for each role.\n\nReward Design. Prior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning\nsystems (Zeng et al., 2025b). Motivated by this, we design a reward function for the proposer that encourages generation of tasks\n\nReward Design. Prior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning\n\nPrior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning\n\nReward Design.\n\nsystems (Zeng et al., 2025b). Motivated by this, we design a reward function for the proposer that encourages generation of tasks\n\nsystems\n\n(Zeng et al.,\n\n2025b)\n\n. Motivated by this, we design a reward function for the proposer that encourages generation of tasks\n\n4\n\n4\n\n4\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nmodel input/output model reward ( , , ) P rogram I nput O utput\n Learnability\n\nReward\nAccuracy\n\nReward\nAbsolute\n\nZero\n\nReasoner\nVerify\nConstruct & Estimate \nPROPOSE\n\nSelf-play\n\nSOLVE\n\nJoint Update\nTask Types\nInduction:\nAbduction:\nDeduction:\nOX = F ( ) P ?\nOX = ( ) ? I\n? = F ( ) P I\n\nPROPOSE\n\nPROPOSE\n\nLearnability\n\nLearnability\n\nConstruct & Estimate\n\nConstruct & Estimate\n\nReward\n\nReward\n\nTask Types\n\nTask Types\n\nAbsolute\n\nAbsolute\n\nSelf-play\n\nSelf-play\n\nAbduction:\n\nAbduction:\n\nOX = F ( ) P ?\n\nOX = F ( )\n\nP\n\n?\n\nJoint Update\n\nJoint Update\n\nZero\n\nZero\n\nDeduction:\n\nDeduction:\n\n? = F ( ) P I\n\nP\n\nI\n\n? = F ( )\n\nReasoner\n\nReasoner\n\nInduction:\n\nInduction:\n\nOX = ( ) ? I\n\nOX = ( )\n\n?\n\nI\n\nAccuracy\n\nAccuracy\n\nSOLVE\n\nSOLVE\n\nVerify\n\nVerify\n\nReward\n\nReward\n\nmodel input/output model reward ( , , ) P rogram I nput O utput\n\nmodel input/output\n\nmodel reward ( , , )\n\nP\n\nrogram\n\nI\n\nnput\n\nO\n\nutput\n\nFigure 4. Absolute Zero Reasoner Training Overview. At every iteration, Absolute Zero Reasoner first PROPOSES a batch of tasks,\nconditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section 3.2).\nFrom these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward rpropose is\nalso calculated for each proposed task as defined in Equation (4). The Absolute Zero Reasoner then SOLVES the batch of reasoning\nquestions. Python is used again to verify the generated responses and compute the accuracy reward rsolve as described in Equation (5).\nFinally, the Absolute Zero Reasoner is jointly updated using both rpropose and rsolve across all three task types, using TRR++ (Section 3.3.5).\n\nFigure 4. Absolute Zero Reasoner Training Overview. At every iteration, Absolute Zero Reasoner first PROPOSES a batch of tasks,\n\nAt every iteration, Absolute Zero Reasoner first\n\na batch of tasks,\n\nAbsolute Zero Reasoner Training Overview.\n\nPROPOSES\n\nFigure 4.\n\nconditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section 3.2).\n\nconditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section\n\n3.2)\n\n.\n\nFrom these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward rpropose is\n\nFrom these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward\n\nis\n\nr\n\npropose\n\nalso calculated for each proposed task as defined in Equation (4). The Absolute Zero Reasoner then SOLVES the batch of reasoning\n\nalso calculated for each proposed task as defined in Equation\n\n(4)\n\n. The Absolute Zero Reasoner then\n\nthe batch of reasoning\n\nSOLVES\n\nquestions. Python is used again to verify the generated responses and compute the accuracy reward rsolve as described in Equation (5).\n\nquestions. Python is used again to verify the generated responses and compute the accuracy reward\n\nas described in Equation\n\n(5)\n\n.\n\nr\n\nsolve\n\nFinally, the Absolute Zero Reasoner is jointly updated using both rpropose and rsolve across all three task types, using TRR++ (Section 3.3.5).\n\nFinally, the Absolute Zero Reasoner is jointly updated using both\n\nacross all three task types, using TRR++ (Section\n\nand\n\n3.3.5)\n\n.\n\nr\n\nr\n\npropose\n\nsolve\n\nwith meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language\nmodel in its solver role to estimate the learnability of a proposed task, a similar type of reward used in unsupervised environment\ndesign literature (Sukhbaatar et al., 2018). We perform n Monte Carlo rollouts of the solver and compute the average success rate:\nr\u00afsolve =\n1\nn\nPN\ni=1 r\n(i)\nsolve. The proposer's reward is then defined as:\n\nwith meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language\n\nwith meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language\n\nmodel in its solver role to estimate the learnability of a proposed task, a similar type of reward used in unsupervised environment\n\nmodel in its solver role to estimate the\n\nof a proposed task, a similar type of reward used in unsupervised environment\n\nlearnability\n\ndesign literature (Sukhbaatar et al., 2018). We perform n Monte Carlo rollouts of the solver and compute the average success rate:\n\ndesign literature\n\n(Sukhbaatar et al.,\n\n2018)\n\n. We perform\n\nMonte Carlo rollouts of the solver and compute the average success rate:\n\nn\n\n(i)\n\n(\n\n)\n\ni\n\nPN\n\nN\n\nsolve. The proposer's reward is then defined as:\n\n. The proposer's reward is then defined as:\n\n1\n\n1\n\nP\n\nr\u00afsolve =\n\n\u00af\n\n=\n\ni=1 r\n\nr\n\nr\n\nsolve\n\nsolve\n\n=1\n\nn\n\nn\n\ni\n\nrpropose =\n\n0, if r\u00afsolve = 0 or r\u00afsolve = 1\n1 \u2212 r\u00afsolve, otherwise,\n(4)\n\n0, if r\u00afsolve = 0 or r\u00afsolve = 1\n\nif\n\nor\n\n0\n\n\u00af\n\n= 0\n\n\u00af\n\n= 1\n\n,\n\nr\n\nr\n\nsolve\n\nsolve\n\n(4)\n\n(4)\n\nrpropose =\n\n=\n\nr\n\npropose\n\n1 \u2212 r\u00afsolve, otherwise,\n\notherwise\n\n1\n\n\u00af\n\nr\n\n,\n\n,\n\nsolve\n\n\u2212\n\nThe intuition is that if a task is either trivial to solve (r\u00afsolve = 1) or unsolvable (r\u00afsolve = 0), the task provides little to no learning signal\nfor the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the\nrichest feedback and greatest potential for learning.\n\nThe intuition is that if a task is either trivial to solve (r\u00afsolve = 1) or unsolvable (r\u00afsolve = 0), the task provides little to no learning signal\n\nThe intuition is that if a task is either trivial to solve (\n\n) or unsolvable (\n\n), the task provides little to no learning signal\n\n\u00af\n\n= 1\n\n\u00af\n\n= 0\n\nr\n\nr\n\nsolve\n\nsolve\n\nfor the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the\n\nfor the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the\n\nrichest feedback and greatest potential for learning.\n\nrichest feedback and greatest potential for learning.\n\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\n\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\n\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\n\nrsolve = I(y=y\u22c6)\n, (5)\n\nrsolve = I(y=y\u22c6)\n\nI\n\n, (5)\n\n(5)\n\n=\n\nr\n\n,\n\nsolve\n\n\u22c6\n\n(\n\n=\n\n)\n\ny\n\ny\n\nwhere y\n\u22c6\nis the ground-truth answer, and equality is evaluated based on value equality in Python.\n\nwhere y\n\nwhere\n\nis the ground-truth answer, and equality is evaluated based on value equality in Python.\n\nis the ground-truth answer, and equality is evaluated based on value equality in Python.\n\n\u22c6\n\n\u22c6\n\ny\n\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which\nintegrates rpropose and rsolve with a format-aware penalty inspired by DeepSeek-AI et al. (2025):\n\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which\n\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which\n\nintegrates rpropose and rsolve with a format-aware penalty inspired by DeepSeek-AI et al. (2025):\n\nintegrates\n\nand\n\nwith a format-aware penalty inspired by\n\nDeepSeek-AI et al.\n\n(2025)\n\n:\n\nr\n\nr\n\npropose\n\nsolve\n\nR(y\u03c0) =\n\n\n\nrrole if the response is passable, role \u2208 {propose,solve}\n\u22120.5 if the response is wrong but well-formatted,\n\u22121 if the answer has formatting errors,\n(6)\n\nrrole if the response is passable, role \u2208 {propose,solve}\n\nif the response is passable, role\n\npropose,solve\n\nr\n\nrole\n\n\u2208 {\n\n}\n\n(6)\n\n(6)\n\n\u22120.5 if the response is wrong but well-formatted,\n\nif the response is wrong but well-formatted,\n\nR(y\u03c0) =\n\n(\n\n) =\n\n0\n\n5\n\nR\n\ny\n\n.\n\n\u03c0\n\n\u2212\n\n\u22121 if the answer has formatting errors,\n\nif the answer has formatting errors,\n\n1\n\n\u2212\n\nwhere y\u03c0 is the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek\nR1 <think> and <answer> format, as shown in Figure 33. Moreover, for the proposer, the reward criterion for format goes beyond\nsimply following the XML structure. As detailed in Section 3.3.3, only responses that produce valid triplets and pass the filtering stage\nare considered to be correctly formatted.\n\nwhere y\u03c0 is the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek\n\nis the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek\n\nwhere\n\ny\n\n\u03c0\n\nR1 <think> and <answer> format, as shown in Figure 33. Moreover, for the proposer, the reward criterion for format goes beyond\n\nR1\n\nand\n\nformat, as shown in Figure\n\n33.\n\nMoreover, for the proposer, the reward criterion for format goes beyond\n\n<think>\n\n<answer>\n\nsimply following the XML structure. As detailed in Section 3.3.3, only responses that produce valid triplets and pass the filtering stage\n\nsimply following the XML structure. As detailed in Section\n\nonly responses that produce valid triplets and pass the filtering stage\n\n3.3.3,\n\nare considered to be correctly formatted.\n\nare considered to be correctly formatted.\n\n5\n\n5\n\n5\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\n\n3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\n\nAZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution,\nand validation of code reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O\nof a coding language, we define an AZR reasoning task as a triplet (p, i, o), where p \u2208 P is a program, i \u2208 I is an input, and o \u2208 O is\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about different parts of this task\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n3.2. Learning Diferent Modes of Reasoning: Deduction, Induction, and Abduction\nAZR uses code executor as both a fexible interface and a verifable environment. This setup enables automatic construction, execution,\nand validation of reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O of\na coding language, we defne an AZR reasoning task as a triplet (p, i, o), where p \u2192 P is a program, i \u2192 I is an input, and o \u2192 O is\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about diferent parts of this task\n\nAZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution,\n\nAZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution,\n\n3.2. Learning Diferent Modes of Reasoning: Deduction, Induction, and Abduction\n\n3.2. Learning Diferent Modes of Reasoning: Deduction, Induction, and Abduction\n\nand validation of code reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O\n\nand validation of code reasoning tasks\n\nAryabumi et al.,\n\n. Give program space\n\n, input space\n\nand output space\n\n(Stuart,\n\n2015;\n\n2024)\n\nP\n\nI\n\nO\n\nAZR uses code executor as both a fexible interface and a verifable environment. This setup enables automatic construction, execution,\n\nAZR uses code executor as both a fexible interface and a verifable environment. This setup enables automatic construction, execution,\n\nof a coding language, we define an AZR reasoning task as a triplet (p, i, o), where p \u2208 P is a program, i \u2208 I is an input, and o \u2208 O is\n\nof a coding language, we define an AZR reasoning task as a triplet\n\n, where\n\nis a program,\n\nis an input, and\n\nis\n\n(\n\n)\n\np, i, o\n\np\n\ni\n\no\n\nP\n\nI\n\nO\n\n\u2208\n\n\u2208\n\n\u2208\n\nand validation of reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O of\n\nand validation of reasoning tasks (Stuart,\n\n2015;\n\nAryabumi et al.,\n\n2024). Give program space\n\n, input space\n\nand output space\n\nof\n\nP\n\nI\n\nO\n\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about different parts of this task\n\nthe corresponding output produced by running program on input,\n\n. AZR learns by reasoning about different parts of this task\n\n=\n\n(\n\n)\n\no\n\np\n\ni\n\na coding language, we defne an AZR reasoning task as a triplet (p, i, o), where p \u2192 P is a program, i \u2192 I is an input, and o \u2192 O is\n\n(\n\n)\n\na coding language, we defne an AZR reasoning task as a triplet\n\n, where\n\nis a program,\n\nis an input, and\n\nis\n\np, i, o\n\np\n\ni\n\no\n\nP\n\nI\n\nO\n\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\n\u2192\n\n\u2192\n\n\u2192\n\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about diferent parts of this task\n\n=\n\n(\n\n)\n\nthe corresponding output produced by running program on input,\n\n. AZR learns by reasoning about diferent parts of this task\n\no\n\np\n\ni\n\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction buffer Ddeduction\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction bufer Ddeduction\n(all task bufers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verified using type-aware value equality\nin python to account for possible variations (such as set ordering or fractions).\ncompleting the triplet (p, i, o), which is added to the bufer if non-error output was produced.\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verifed using type-aware value equality\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\nin python to account for possible variations (such as set ordering or fractions).\n\u2022 As a proposer, the policy \u03c0\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\nexecutes p(i) and get the triplet (p, i, o).\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\u2022 As a proposer, the policy \u03c0propose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verified by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\nexecutes p(i) and get the triplet (p, i, o).\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verifed by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn\n, on\n)}, requiring generalization from partial information.\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn, on)}, requiring generalization from partial information.\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222a Ddeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn\n, on\n)}, m), which is stored\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222aDdeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn, on)}, m), which is stored\nin the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\u2022 As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\nlogic and promotes generalized induction.\n\u2022 As a solver, the model is shown the frst half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\nlogic and promotes generalized induction.\n\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\n1.\n\n: predicting the output\n\ngiven a program\n\nand input\n\n, capturing step-by-step logical reasoning.\n\nDeduction\n\no\n\np\n\ni\n\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction buffer Ddeduction\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction bufer Ddeduction\n(all task bufers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\n\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction buffer Ddeduction\n\nAs a\n\n, AZR is conditioned on the task type\n\ndeduction\n\nreference examples from the deduction buffer\n\nand\n\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\n1.\n\n: predicting the output\n\ngiven a program\n\nand input\n\n, capturing step-by-step logical reasoning.\n\nDeduction\n\no\n\np\n\ni\n\n=\n\n\u2022\n\nproposer\n\n\u03b1\n\nK\n\ndeduction\n\nD\n\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\n\n(all task buffers are outlined in Section\n\n3.3), and generates a pair\n\n. The environment\n\nthen executes\n\nto compute\n\n,\n\n(\n\n)\n\n(\n\n)\n\np, i\n\ne\n\np\n\ni\n\no\n\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction bufer Ddeduction\n\n=\n\n\u2022\n\nAs a\n\n, AZR is conditioned on\n\nthe\n\ntask type\n\ndeduction and\n\nreference examples from the deduction bufer\n\nproposer\n\n\u03b1\n\nK\n\nD\n\ndeduction\n\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\n\ncompleting the triplet\n\n, which is added to the buffer if non-error output was produced.\n\n(\n\n)\n\np, i, o\n\n(all task bufers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\n\n(\n\n)\n\n(\n\n)\n\n(all task bufers are outlined in Section\n\n3.3), and generates a pair\n\n. The environment\n\nthen executes\n\nto compute\n\n,\n\np, i\n\ne\n\np\n\ni\n\no\n\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verified using type-aware value equality\nin python to account for possible variations (such as set ordering or fractions).\ncompleting the triplet (p, i, o), which is added to the bufer if non-error output was produced.\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verifed using type-aware value equality\n\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verified using type-aware value equality\n\nAs a\n\n, the model receives\n\nand predicts the output\n\n. The predicted output is verified using type-aware value equality\n\ncompleting the triplet (p, i, o), which is added to the bufer if non-error output was produced.\n\n(\n\n)\n\ncompleting the triplet\n\n, which is added to the bufer if non-error output was produced.\n\np, i, o\n\n(\n\n)\n\n\u2022\n\nsolver\n\np, i\n\no\n\n\u03c0\n\nin python to account for possible variations (such as set ordering or fractions).\n\nin python to account for possible variations (such as set ordering or fractions).\n\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verifed using type-aware value equality\n\n(\n\n)\n\n\u2022\n\nAs a\n\n, the model receives\n\nand predicts the output\n\n. The predicted output is verifed using type-aware value equality\n\nsolver\n\np, i\n\no\n\n\u03c0\n\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\nin python to account for possible variations (such as set ordering or fractions).\n\nin python to account for possible variations (such as set ordering or fractions).\n\nin python to account for possible variations (such as set ordering or fractions).\n\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\n2.\n\n: inferring a plausible input\n\ngiven the program\n\nand an output\n\n, resembling trial-and-error or online search.\n\nAbduction\n\ni\n\np\n\no\n\n\u2022 As a proposer, the policy \u03c0\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\nexecutes p(i) and get the triplet (p, i, o).\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\u2022 As a proposer, the policy \u03c0propose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\n\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\n2.\n\n: inferring a plausible input\n\ngiven the program\n\nand an output\n\n, resembling trial-and-error or online search.\n\nAbduction\n\ni\n\np\n\no\n\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\n\npropose\n\n\u2022 As a proposer, the policy \u03c0\n\nAs a\n\n, the policy\n\n's input and output is almost the same as the proposer for the deduction task, except that the task\n\n\u2022\n\nproposer\n\n\u03c0\n\n\u2022 As a proposer, the policy \u03c0propose's input and output is almost the same as the proposer for the deduction task, except that the task\n\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\n\n\u2022\n\nAs a\n\n, the policy\n\nproposer\n\n\u03c0\n\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\n\nabduction\n\nis changed as an input. The model generates a pair\n\nconditioned on\n\nand reference examples. Then we\n\ntype\n\n=\n\n(\n\n)\n\n\u03b1\n\np, i\n\n\u03b1\n\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\n\n=\n\n(\n\n)\n\ntype\n\nabduction is changed as an input. The model generates a pair\n\nconditioned on\n\nand reference examples. Then we\n\n\u03b1\n\np, i\n\n\u03b1\n\nexecutes p(i) and get the triplet (p, i, o).\n\nexecutes\n\nand get the triplet\n\n.\n\n(\n\n)\n\n(\n\n)\n\np\n\ni\n\np, i, o\n\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verified by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\nexecutes p(i) and get the triplet (p, i, o).\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verifed by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\n\nexecutes p(i) and get the triplet (p, i, o).\n\n(\n\n)\n\n(\n\n)\n\nexecutes\n\nand get the triplet\n\n.\n\np\n\ni\n\np, i, o\n\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verified by checking whether p(i\u03c0) = o. Since programs\n\nAs a\n\n, the model receives\n\nand predicts\n\n. The solution is verified by checking whether\n\n. Since programs\n\n(\n\n)\n\n(\n\n) =\n\n\u2022\n\nsolver\n\np, o\n\ni\n\np\n\ni\n\no\n\n\u03c0\n\n\u03c0\n\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verifed by checking whether p(i\u03c0) = o. Since programs\n\n(\n\n)\n\n(\n\n) =\n\n\u2022\n\nAs a\n\n, the model receives\n\nand predicts\n\n. The solution is verifed by checking whether\n\n. Since programs\n\nsolver\n\np, o\n\ni\n\np\n\ni\n\no\n\n\u03c0\n\n\u03c0\n\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\n\nmay not be bijective, we use\n\nvalue equivalence rather than requiring exact input matches.\n\noutput\n\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\n\nmay not be bijective, we use\n\nvalue equivalence rather than requiring exact input matches.\n\noutput\n\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn\n, on\n)}, requiring generalization from partial information.\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn, on)}, requiring generalization from partial information.\n\n3. Induction: synthesizing a program p from a set of in-out examples {(i\n\n3.\n\nsynthesizing a program\n\nfrom a set of in-out examples\n\n)}, requiring generalization from partial information.\n\n, requiring generalization from partial information.\n\nn\n\nn\n\n, on\n\nn\n\n(\n\n)\n\nInduction:\n\np\n\ni\n\n, o\n\nn, on)}, requiring generalization from partial information.\n\nn\n\nn\n\n{\n\n}\n\n3. Induction: synthesizing a program p from a set of in-out examples {(i\n\n(\n\n)\n\n3.\n\nsynthesizing a program\n\nfrom a set of in-out examples\n\n, requiring generalization from partial information.\n\nInduction:\n\np\n\ni\n\n, o\n\n{\n\n}\n\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222a Ddeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn\n, on\n)}, m), which is stored\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222aDdeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn, on)}, m), which is stored\nin the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222a Ddeduction, generates N new inputs and a message m, and uses the\n\nAs a\n\n, AZR samples a valid program\n\n, generates\n\nnew inputs and a message\n\n, and uses the\n\nfrom\n\n\u2022\n\nproposer\n\np\n\nN\n\nm\n\nabduction\n\ndeduction\n\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222aDdeduction, generates N new inputs and a message m, and uses the\n\n\u2022\n\nAs a\n\n, AZR samples a valid program\n\nfrom\n\ndeduction, generates\n\nnew inputs and a message\n\n, and uses the\n\nproposer\n\np\n\nN\n\nm\n\nD\n\n\u222a D\n\nD\n\n\u222aD\n\nabduction\n\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\n\nenvironment to compute corresponding outputs. This forms an extended task representation\n\n)}, m), which is stored\n\n, which is stored\n\nn\n\nn\n\n, on\n\nn\n\n(\n\n(\n\n)\n\n)\n\np,\n\ni\n\n, o\n\n, m\n\nn, on)}, m), which is stored\n\nn\n\nn\n\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\n\n(\n\n(\n\n)\n\n)\n\nenvironment to compute corresponding outputs. This forms an extended task representation\n\n, which is stored\n\np,\n\ni\n\n, o\n\n, m\n\n{\n\n}\n\n{\n\n}\n\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\n\nin the induction buffer\n\n. Since infinitely many functions can map the inputs to the outputs, making the induction task\n\nin the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\n\nin the induction bufer\n\ninduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\n\ninduction\n\nD\n\nD\n\nunder-constrained, the message m helps properly condition the problem for the solver.\n\nunder-constrained, the message\n\nhelps properly condition the problem for the solver.\n\nm\n\nunder-constrained, the message m helps properly condition the problem for the solver.\n\nunder-constrained, the message\n\nhelps properly condition the problem for the solver.\n\nm\n\n\u2022 As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\nlogic and promotes generalized induction.\n\u2022 As a solver, the model is shown the frst half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\nlogic and promotes generalized induction.\n\n\u2022 As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\n\nAs a\n\n, the model is shown the first half of the input-output pairs and the message\n\n, and must synthesize a program\n\nthat\n\n\u2022\n\n\u2022 As a solver, the model is shown the frst half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\n\n\u2022\n\nAs a\n\n, the model is shown the frst half of the input-output pairs and the message\n\n, and must synthesize a program\n\nthat\n\nsolver\n\nsolver\n\nm\n\np\n\nm\n\np\n\n\u03c0\n\n\u03c0\n\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\n\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\n\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\n\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\n\nlogic and promotes generalized induction.\n\nlogic and promotes generalized induction.\n\nlogic and promotes generalized induction.\n\nlogic and promotes generalized induction.\n\nEach reasoning task type leverages code as an expressive and verifable\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025; Lam-\nbert et al., 2024). All prompts used by three diferent task types and two\ntypes of roles within a task type are shown in Figures 34 to 39. Next, we\noutline exact details of our algorithm.\nEach reasoning task type leverages code as an expressive and verifiable\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025;\nLambert et al., 2024). All prompts used by three different task types and\ntwo types of roles within a task type are shown in Figures 34 to 39. Next,\nwe outline exact details of our algorithm.\n\nEach reasoning task type leverages code as an expressive and verifable\n\nEach reasoning task type leverages code as an expressive and verifable\n\nEach reasoning task type leverages code as an expressive and verifiable\n\nEach reasoning task type leverages code as an expressive and verifiable\n\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\n\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\n\nProgram Triplet\nInput: \"Hello World\"\n1 def f(x):\n2 return x\nOutput: \"Hello World\"\n\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\n\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\n\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025; Lam-\n\nimproving systems in open-ended domains (DeepSeek-AI et al.,\n\n2025;\n\nLam-\n\nProgram Triplet\n\nProgram Triplet\n\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025;\n\nimproving systems in open-ended domains\n\n(DeepSeek-AI et al.,\n\n2025;\n\nbert et al., 2024). All prompts used by three diferent task types and two\n\nbert et al.,\n\n2024). All prompts used by three diferent task types and two\n\nLambert et al., 2024). All prompts used by three different task types and\n\nLambert et al.,\n\n2024)\n\n. All prompts used by three different task types and\n\nInput: \"Hello World\"\n\nInput:\n\n\"Hello World\"\n\ntypes of roles within a task type are shown in Figures 34 to 39. Next, we\n\ntypes of roles within a task type are shown in Figures\n\n34\n\nto\n\n39. Next, we\n\ntwo types of roles within a task type are shown in Figures 34 to 39. Next,\n\ntwo types of roles within a task type are shown in Figures\n\n34\n\nto\n\n39.\n\nNext,\n\noutline exact details of our algorithm.\n\noutline exact details of our algorithm.\n\nwe outline exact details of our algorithm.\n\nwe outline exact details of our algorithm.\n\n1 def f(x):\n\ndef\n\nf(x):\n\n1\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n2 return x\n\nreturn\n\nx\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n2\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\ning initialization of bufers 3.3.1, usage of thse bufers 3.3.2, construction of\nvalid tasks 3.3.3, validating solutions 3.3.4, and fnally advantage estimator\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure of\nAZR in Algorithm 1.\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\ning initialization of buffers 3.3.1, usage of thse buffers 3.3.2, construction of\nvalid tasks 3.3.3, validating solutions 3.3.4, and finally advantage estimator\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure\nof AZR in Algorithm 1.\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\n\ning initialization of bufers 3.3.1, usage of thse bufers 3.3.2, construction of\n\ning initialization of bufers\n\n3.3.1, usage of thse bufers\n\n3.3.2, construction of\n\ning initialization of buffers 3.3.1, usage of thse buffers 3.3.2, construction of\n\ning initialization of buffers\n\nusage of thse buffers\n\n3.3.2, construction of\n\n3.3.1,\n\nvalid tasks 3.3.3, validating solutions 3.3.4, and fnally advantage estimator\n\nvalid tasks\n\n3.3.3, validating solutions\n\n3.3.4, and fn\n\nally ad\n\nvantage estimator\n\nOutput: \"Hello World\"\n\nOutput:\n\n\"Hello World\"\n\nvalid tasks 3.3.3, validating solutions 3.3.4, and finally advantage estimator\n\nvalid tasks\n\n3.3.3,\n\nvalidating solutions\n\n3.3.4, and finally advantage estimator\n\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure of\n\ncalculation\n\n3.3.5. We outline the o\n\nverall\n\nrecipe of the self-play procedure of\n\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure\n\ncalculation\n\n3.3.5. We outline the overall recipe of the self-play procedure\n\nAZR in Algorithm 1.\n\nAZR in\n\nAlgor\n\nithm\n\n1.\n\nof AZR in Algorithm 1.\n\nof AZR in Algorithm\n\n1.\n\nFigure 5. The Seed AZR Zero Triplet. The above\nidentity function triplet was the only triplet provided\nto AZR to initiate its self-bootstrap propose-and-solve\nRLVR loop. We note that the base LLM is fully ca-\npable of initiating the AZR loop without any seed pro-\ngram; its inclusion illustrates our approach's fexibility:\nwe can optionally initialize seed programs with existing\ndatasets of varying complexity, and we initialized ours\nwith the simplest program.\nFigure 5. The Seed AZR Zero Triplet. The above\nidentity function triplet was the only triplet provided\nto AZR to initiate its self-bootstrap propose-and-solve\nRLVR loop. We note that the base LLM is fully capable\nof initiating the AZR loop without any seed program;\nits inclusion illustrates our approach's flexibility: we\ncan optionally initialize seed programs with existing\ndatasets of varying complexity, and we initialized ours\nwith the simplest program.\n\nFigure 5. The Seed AZR Zero Triplet. The above\n\nThe above\n\nThe Seed AZR Zero Triplet.\n\nFigure 5. The Seed AZR Zero Triplet. The above\n\nFigure 5.\n\nThe above\n\nFigure 5.\n\nThe Seed AZR Zero Triplet.\n\n3.3.1. BUFFER INITIALIZATION\nTo initialize AZR self-play, we frst generate a seed set of valid triplets using\n3.3.1. Buffer Initialization\n\n3.3.1. BUFFER INITIALIZATION\n\n3.3.1. BUFFER INITIALIZATION\n\nidentity function triplet was the only triplet provided\n\nidentity function triplet was the only triplet provided\n\nidentity function triplet was the only triplet provided\n\nidentity function triplet was the only triplet provided\n\n3.3.1. Buffer Initialization\n\n3.3.1. Buffer Initialization\n\nto AZR to initiate its self-bootstrap propose-and-solve\n\nto AZR to initiate its self-bootstrap propose-and-solve\n\nto AZR to initiate its self-bootstrap propose-and-solve\n\nto AZR to initiate its self-bootstrap propose-and-solve\n\nTo initialize AZR self-play, we frst generate a seed set of valid triplets using\n\nTo initialize AZR self-play, we frst generate a seed set of valid triplets using\n\nRLVR loop. We note that the base LLM is fully capable\n\nRLVR loop. We note that the base LLM is fully capable\n\nRLVR loop. We note that the base LLM is fully ca-\n\nRLVR loop. We note that the base LLM is fully ca-\n\nthe base language model. Each prompt samples up to K triplets from the\ncurrent seed bufer Dseed as references. When Dseed is empty at time 0, we\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\nuse the same proposer prompts detailed in Figures 34 to 36.\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\nTo initialize AZR self-play, we first generate a seed set of valid triplets using\nthe base language model. Each prompt samples up to K triplets from the\ncurrent seed buffer Dseed as references. When Dseed is empty at time 0, we\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\nuse the same proposer prompts detailed in Figures 34 to 36.\n\nTo initialize AZR self-play, we first generate a seed set of valid triplets using\n\nTo initialize AZR self-play, we first generate a seed set of valid triplets using\n\nthe base language model. Each prompt samples up to K triplets from the\n\nthe base language model. Each prompt samples up to\n\ntriplets from the\n\nK\n\nof initiating the AZR loop without any seed program;\n\nof initiating the AZR loop without any seed program;\n\npable of initiating the AZR loop without any seed pro-\n\npable of initiating the AZR loop without any seed pro-\n\nthe base language model. Each prompt samples up to K triplets from the\n\nthe base language model. Each prompt samples up to\n\ntriplets from the\n\nK\n\ncurrent seed bufer Dseed as references. When Dseed is empty at time 0, we\n\ncurrent seed bufer\n\nas references. When\n\nis empty at time 0, we\n\nD\n\nD\n\nseed\n\nseed\n\nits inclusion illustrates our approach's flexibility: we\n\nits inclusion illustrates our approach's flexibility: we\n\ngram; its inclusion illustrates our approach's fexibility:\n\ngram; its inclusion illustrates our approach's fexibility:\n\ncurrent seed buffer Dseed as references. When Dseed is empty at time 0, we\n\ncurrent seed buffer\n\nas references. When\n\nis empty at time 0, we\n\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\n\nfall back to the zero triplet show in Figure\n\n5. During the seeding stage, we\n\nseed\n\nseed\n\nD\n\nD\n\nwe can optionally initialize seed programs with existing\n\nwe can optionally initialize seed programs with existing\n\ncan optionally initialize seed programs with existing\n\ncan optionally initialize seed programs with existing\n\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\n\nfall back to the zero triplet show in Figure\n\n5. During the seeding stage, we\n\nuse the same proposer prompts detailed in Figures 34 to 36.\n\nuse the same proposer prompts detailed\n\nin\n\nFigures\n\n34\n\nto\n\n36.\n\ndatasets of varying complexity, and we initialized ours\n\ndatasets of varying complexity, and we initialized ours\n\ndatasets of varying complexity, and we initialized ours\n\ndatasets of varying complexity, and we initialized ours\n\nuse the same proposer prompts detailed in Figures 34 to 36.\n\nuse the same proposer prompts detailed in Figures\n\n34\n\nto\n\n36.\n\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n\nwith the simplest program.\n\nwith the simplest program.\n\nwith the simplest program.\n\nwith the simplest program.\n\n(p, i) pairs, which are fltered, executed, and stored as valid triplets. We\ninitialize D0\nabduction = D0\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n(p, i) pairs, which are filtered, executed, and stored as valid triplets. We\n\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n\n(p, i) pairs, which are fltered, executed, and stored as valid triplets. We\n\n(\n\n)\n\npairs, which are fltered, executed, and stored as valid triplets. We\n\np, i\n\nabduction = D0\n\n0\n\ninitialize D0\n\n0\n\n(p, i) pairs, which are filtered, executed, and stored as valid triplets. We\n\npairs, which are filtered, executed, and stored as valid triplets. We\n\n=\n\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where\n\n=\n\n=\n\ninitialize\n\nseed, where\n\n, where\n\nB\n\nS\n\n(\n\n)\n\np, i\n\nD\n\nD\n\nD\n\n|D\n\n|\n\n\u00d7\n\nseed\n\nabduction\n\ndeduction\n\nB is the batch size, and S = 4 is a factor we fx in all experiments. All seed triplet's program are stripped of global variables and\ncomments (Appendix C), but subsequent iterations of adding new triplets to the bufers are unaltered. No model updates occur during\nthis phase. Similarly, to initialize the induction bufer, we sample programs from Dseed, generate matching input sets and messages, and\ncollect valid examples until |D0\ninduction| = B \u00d7 S.\ninitialize D\n0\nabduction = D\n0\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where B is the batch size, and S = 4 is a factor we fix in all\nexperiments. All seed triplet's program are stripped of global variables and comments (Appendix D), but subsequent iterations of adding\nnew triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we\nsample programs from Dseed, generate matching input sets and messages, and collect valid examples until |D0\ninduction| = B \u00d7 S.\n\ninitialize D\n\ninitialize\n\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where B is the batch size, and S = 4 is a factor we fix in all\n\n, where\n\n, where\n\nis the batch size, and\n\nis a factor we fix in all\n\nB is the batch size, and S = 4 is a factor we fx in all experiments. All seed triplet's program are stripped of global variables and\n\n= 4\n\nis the batch size, and\n\nis a factor we fx in all experiments. All seed triplet's program are stripped of global variables and\n\n0\n\n0\n\n0\n\n0\n\nB\n\nS\n\nabduction = D\n\n=\n\n=\n\n=\n\n= 4\n\nB\n\nS\n\nB\n\nS\n\nseed\n\nseed\n\nabduction\n\ndeduction\n\nD\n\nD\n\nD\n\n|D\n\n|\n\n\u00d7\n\ncomments (Appendix C), but subsequent iterations of adding new triplets to the bufers are unaltered. No model updates occur during\n\ncomments (Appendix\n\nC), but subsequent iterations of adding new triplets to the bufers are un\n\nalte\n\nred. No model updates occur during\n\nexperiments. All seed triplet's program are stripped of global variables and comments (Appendix D), but subsequent iterations of adding\n\nexperiments. All seed triplet's program are stripped of global variables and comments (Appendix\n\nD), but subsequent iterations of adding\n\nthis phase. Similarly, to initialize the induction bufer, we sample programs from Dseed, generate matching input sets and messages, and\n\nthis phase. Similarly, to initialize the induction bufer, we sample programs from\n\nseed, generate matching input sets and messages, and\n\nnew triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we\n\nnew triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we\n\nD\n\ncollect valid examples until |D0\n\n0\n\ninduction| = B \u00d7 S.\n\n=\n\ncollect valid examples until\n\n.\n\nB\n\nS\n\nsample programs from Dseed, generate matching input sets and messages, and collect valid examples until |D0\n\nsample programs from\n\n, generate matching input sets and messages, and collect valid examples until\n\ninduction| = B \u00d7 S.\n\n.\n\n0\n\n=\n\n|D\n\n|\n\n\u00d7\n\nB\n\nS\n\ninduction\n\nseed\n\ninduction\n\nD\n\n|D\n\n|\n\n\u00d7\n\n6\n6\n\n6\n\n6\n\n6\n\n6\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAlgorithm 1Self-Play Training of Absolute Zero Reasoner (AZR)Require:Pretrained base LLM\n\u03c0\u03b8; batch size\nB; #referencesK; iterations\nT1:\nDded,\nDabd,\nDind\u2190 InitSeeding(\u03c0\u03b8)see \u00a73.3.1\n\u25b7to\n2:\n\u2190 1\nfor\nt\nTdo3:\nfor\n\u2190 1\nbto\ndo\nB\u25b7PROPOSE PHASE4:\np\n\u223c Dabd\u222a Ddedsample a program for induction task proposal\n\u25b7N\nn\n5:\n\ni\n\u03c0propose\n(ind, p)\nn=1, m\u03c0\n\u2190 \u03c0\n\u03b8generate\n\u25b7\nNinputs and a descriptionn\n6:\n\n(i\nif\n\u03c0N\nn=1 \u2190 ValidateByExecuting\n, on\n)\np,\n{i\n\u03c0syntax\nn\n\u03c0},\nthen\n\u25b7validate I/Os, see \u00a73.3.37:\nDindn\n, on\n\n(p,\n{(i\n)}, m\u03c0)\n\u2190 Dind\n\u222a\n\u03c0\n\u03c0\u25b7update induction buffer8:\nfor\n\u03b1\u2208 {ded,\nabd}\ndo9:\nK\npk, ik, ok\nk=1 \u223c D\u03b1sample\n\u25b7\nKreference examples10:propose\n\n\n(p\u03c0, i\u03c0)\n{(pk, ik, ok)}\n\u2190 \u03c0\n\u03b1,\n\u03b8\u25b7propose new task11:\nif\u2190 ValidateByExecuting\no\u03c0\np\u03c0, i\u03c0,syntax,safety,determinism\nthensee \u00a73.3.3\n\u25b712:\n(p\u03c0, i\u03c0, o\u03c0)\nD\u03b1 \u2190 D\u03b1 \u222aif valid, update deduction or abduction buffers\n\u25b713:\nfor all\n\u03b1\u2208 {ded,\nabd,\nind}\ndo\u25b7SOLVE PHASE(x, y\u22c6\n14:\u2190 SamplePrepareTasks\nD\u03b1, B, t\n)\u25b7 x, y\u22c6 prepared based on\u03b1, see \u00a73.3.3&amp;3.3.415:\ny\u03c0\n\u223c \u03c0solve\n(x)\n\u03b816:\nReward:Use proposed task triplets and solved answers to get&amp;\nrpropose\nrsolvesee \u00a73.1\n\u25b717:\nRL update:use Task Relative REINFORCE++ to update\u03c0\u03b8see \u00a73.3.5\n\u25b7\n\nAlgorithm 1\n\nSelf-Play Training of Absolute Zero Reasoner (AZR)\n\nRequire:\n\nPretrained base LLM\n\u03c0\u03b8; batch size\nB; #references\n\nK; iterations\nT\n\n1:\nDded,\nDabd,\nDind\n\n\u2190 InitSeeding(\u03c0\u03b8)\n\nsee \u00a73.3.1\n\u25b7\n\nto\n2:\n\u2190 1\nfor\nt\nT\n\ndo\n\n3:\nfor\n\u2190 1\nb\n\nto\ndo\nB\n\n\u25b7\n\nPROPOSE PHASE\n\n4:\np\n\u223c Dabd\n\n\u222a Dded\n\nsample a program for induction task proposal\n\u25b7\n\nN\nn\n5:\n\ni\n\u03c0\n\npropose\n(ind, p)\nn=1, m\u03c0\n\u2190 \u03c0\n\u03b8\n\ngenerate\n\u25b7\nN\n\ninputs and a description\n\nn\n6:\n\n(i\nif\n\u03c0\n\nN\nn=1 \u2190 ValidateByExecuting\n, on\n)\np,\n{i\n\u03c0\n\nsyntax\nn\n\u03c0},\nthen\n\u25b7\n\nvalidate I/Os, see \u00a73.3.3\n\n7:\nDind\n\nn\n, on\n\n(p,\n{(i\n)}, m\u03c0)\n\u2190 Dind\n\u222a\n\u03c0\n\u03c0\n\n\u25b7\n\nupdate induction buffer\n\n8:\nfor\n\u03b1\n\n\u2208 {ded,\nabd}\ndo\n\n9:\n\nK\npk, ik, ok\nk=1 \u223c D\u03b1\n\nsample\n\u25b7\nK\n\nreference examples\n\n10:\n\npropose\n\n\n(p\u03c0, i\u03c0)\n{(pk, ik, ok)}\n\u2190 \u03c0\n\u03b1,\n\u03b8\n\n\u25b7\n\npropose new task\n\n11:\nif\n\n\u2190 ValidateByExecuting\no\u03c0\np\u03c0, i\u03c0,\n\nsyntax,safety,determinism\nthen\n\nsee \u00a73.3.3\n\u25b7\n\n12:\n\n(p\u03c0, i\u03c0, o\u03c0)\nD\u03b1 \u2190 D\u03b1 \u222a\n\nif valid, update deduction or abduction buffers\n\u25b7\n\n13:\nfor all\n\u03b1\n\n\u2208 {ded,\nabd,\nind}\ndo\n\n\u25b7\n\nSOLVE PHASE\n\n(x, y\u22c6\n14:\n\n\u2190 SamplePrepareTasks\nD\u03b1, B, t\n)\n\n\u25b7 x, y\u22c6 prepared based on\n\n\u03b1, see \u00a73.3.3&amp;3.3.4\n\n15:\ny\u03c0\n\u223c \u03c0\n\nsolve\n(x)\n\u03b8\n\n16:\nReward:\n\nUse proposed task triplets and solved answers to get\n\n&amp;\nrpropose\nrsolve\n\nsee \u00a73.1\n\u25b7\n\n17:\nRL update:\n\nuse Task Relative REINFORCE++ to update\n\n\u03c0\u03b8\n\nsee \u00a73.3.5\n\u25b7\n\n3.3.2. Task Proposal Inputs and Buffer Management\n\n3.3.2. Task Proposal Inputs and Buffer Management\n\n3.3.2. Task Proposal Inputs and Buffer Management\n\nDuring the actual self-play stage of AZR, we use the task buffer in three ways. First, for the proposer of abduction and deduction tasks,\nwe uniformly sample K past triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task.\nThe design is to show it past examples, and prompt it to generate a different one to promote diversity (Zhao et al., 2025a). Second, we\nsample one triplet from the union of abduction and deduction buffers Dabd S\nDded, and present the program p from that triplet to the\ninduction proposer to generate a set of N matching inputs {i\nn\n} and a natural language message m. Lastly, to maintain stable training, if\na batch of solver problems contains fewer than B valid proposed tasks (proposer not adhering to formatting), we fill the remainder by\nuniformly sampling from the corresponding task buffer of previously validated triplets.\n\nDuring the actual self-play stage of AZR, we use the task buffer in three ways. First, for the proposer of abduction and deduction tasks,\n\nDuring the actual self-play stage of AZR, we use the task buffer in three ways.\n\n, for the proposer of abduction and deduction tasks,\n\nFirst\n\nwe uniformly sample K past triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task.\n\nwe uniformly sample\n\npast triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task.\n\nK\n\nThe design is to show it past examples, and prompt it to generate a different one to promote diversity (Zhao et al., 2025a). Second, we\n\nThe design is to show it past examples, and prompt it to generate a different one to promote diversity\n\n(Zhao et al.,\n\n2025a)\n\n.\n\n, we\n\nSecond\n\nsample one triplet from the union of abduction and deduction buffers Dabd S\n\nsample one triplet from the union of abduction and deduction buffers\n\nDded, and present the program p from that triplet to the\n\n, and present the program\n\nfrom that triplet to the\n\nS\n\np\n\nabd\n\nded\n\nD\n\nD\n\ninduction proposer to generate a set of N matching inputs {i\n\ninduction proposer to generate a set of\n\nmatching inputs\n\n} and a natural language message m. Lastly, to maintain stable training, if\n\nand a natural language message\n\n, to maintain stable training, if\n\n.\n\nn\n\nn\n\nLastly\n\nN\n\ni\n\nm\n\n{\n\n}\n\na batch of solver problems contains fewer than B valid proposed tasks (proposer not adhering to formatting), we fill the remainder by\n\na batch of solver problems contains fewer than\n\nvalid proposed tasks (proposer not adhering to formatting), we fill the remainder by\n\nB\n\nuniformly sampling from the corresponding task buffer of previously validated triplets.\n\nuniformly sampling from the corresponding task buffer of previously validated triplets.\n\nThe buffer grows for abduction and deduction tasks whenever \u03c0 propose a valid triplet (p, i, o), regardless if it gets any task reward.\nSimilarly, for induction tasks, all valid triplets (p, {i\nn\n, on\n}), m are added to the buffer.\n\nThe buffer grows for abduction and deduction tasks whenever \u03c0 propose a valid triplet (p, i, o), regardless if it gets any task reward.\n\nThe buffer grows for abduction and deduction tasks whenever\n\npropose a valid triplet\n\n, regardless if it gets any task reward.\n\n(\n\n)\n\n\u03c0\n\np, i, o\n\nSimilarly, for induction tasks, all valid triplets (p, {i\n\nSimilarly, for induction tasks, all valid triplets\n\n}), m are added to the buffer.\n\nare added to the buffer.\n\nn\n\nn\n\n, on\n\nn\n\n(\n\n)\n\np,\n\ni\n\n, o\n\n, m\n\n{\n\n}\n\n3.3.3. Constructing Valid Tasks\n\n3.3.3. Constructing Valid Tasks\n\n3.3.3. Constructing Valid Tasks\n\nProposal Task Validation. We first describe how we construct valid tasks from the proposals generated by the policy \u03c0. For deduction\nand abduction tasks, each proposal consists of a program and an input (p, i). To validate the task, we use the task validation procedure\n(steps shown below) on the input to obtain the correct output o, resulting in a complete triplet (p, i, o). For induction tasks, given a\nprogram p the policy proposes a set of inputs {i\nn\n} and message m. We also use the task validation procedure on each of the input i\nn\nin the set to obtain a corresponding output o\nn\n, forming a set of input-output pairs {i\nn\n, on\n}. We do not impose any constraints on m.\nThe resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. The task\nvalidation procedure entails:\n\nProposal Task Validation. We first describe how we construct valid tasks from the proposals generated by the policy \u03c0. For deduction\n\nWe first describe how we construct valid tasks from the proposals generated by the policy\n\n. For\n\nProposal Task Validation.\n\ndeduction\n\n\u03c0\n\nand abduction tasks, each proposal consists of a program and an input (p, i). To validate the task, we use the task validation procedure\n\ntasks, each proposal consists of a program and an input\n\n. To validate the task, we use the task validation procedure\n\n(\n\n)\n\nand abduction\n\np, i\n\n(steps shown below) on the input to obtain the correct output o, resulting in a complete triplet (p, i, o). For induction tasks, given a\n\n(steps shown below) on the input to obtain the correct output\n\n, resulting in a complete triplet\n\n. For\n\ntasks, given a\n\n(\n\n)\n\ninduction\n\no\n\np, i, o\n\nprogram p the policy proposes a set of inputs {i\n\nprogram\n\nthe policy proposes a set of inputs\n\n} and message m. We also use the task validation procedure on each of the input i\n\nand message\n\n. We also use the task validation procedure on each of the input\n\nn\n\nn\n\nn\n\nn\n\np\n\ni\n\nm\n\ni\n\n{\n\n}\n\nin the set to obtain a corresponding output o\n\nin the set to obtain a corresponding output\n\n, forming a set of input-output pairs {i\n\n, forming a set of input-output pairs\n\n}. We do not impose any constraints on m.\n\n. We do not impose any constraints on\n\n.\n\nn\n\nn\n\nn\n\nn\n\n, on\n\nn\n\no\n\ni\n\n, o\n\nm\n\n{\n\n}\n\nThe resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. The task\n\nThe resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. The\n\ntask\n\nvalidation procedure entails:\n\nentails:\n\nvalidation procedure\n\n1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.\n2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\nFigures 34 to 36.\n3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p \u2208 Pdeterministic \u2282 P, where P is the space\nof all valid programs and I is the space of all valid inputs:\n\n1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.\n\n1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\n\nWe first use Python to run the program\n\nwith the input\n\n. If no errors are raised and something is returned, we\n\n1.\n\nProgram Integrity.\n\np\n\ni\n\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.\n\nthen gather the output\n\nof that\n\npair and determine that the program at least has valid syntax.\n\n(\n\n)\n\no\n\np, i\n\n2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\nFigures 34 to 36.\n\n2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\n\n2.\n\nWe also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\n\nProgram Safety.\n\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\n\ncause harm to the Python environment,\n\n.,\n\n. The list of packages used to filter out invalid programs is\n\ni.e\n\nos.sys, sys, shutil\n\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\n\nprovided in Figure\n\n8.\n\nThis list is also included in the instructions when prompting the language model to generate questions. See\n\nFigures 34 to 36.\n\nFigures\n\n34\n\nto\n\n36.\n\n3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p \u2208 Pdeterministic \u2282 P, where P is the space\nof all valid programs and I is the space of all valid inputs:\n\n3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p \u2208 Pdeterministic \u2282 P, where P is the space\n\n3.\n\nIn our setting, we only consider\n\n, where\n\nis the space\n\n,\n\n.,\n\nCheck for Determinism.\n\ndeterministic programs\n\ni.e\n\np\n\nP\n\nP\n\nP\n\ndeterministic\n\n\u2208\n\n\u2282\n\nof all valid programs and I is the space of all valid inputs:\n\nof all valid programs and\n\nis the space of all valid inputs:\n\nI\n\n7\n\n7\n\n7\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n\u2200p \u2208 Pdeterministic, \u2200i \u2208 I ,\n\nlim\nj\u2192\u221e\np(i)\n(1) = p(i)\n(2) = \u00b7 \u00b7 \u00b7 = p(i)\n(j)\n\n, (7)\n\n(1) = p(i)\n\n(1)\n\n(2) = \u00b7 \u00b7 \u00b7 = p(i)\n\n(2)\n\n(j)\n\n(\n\n)\n\nj\n\n, (7)\n\n(7)\n\nlim\n\nlim\n\np(i)\n\n(\n\n)\n\n=\n\n(\n\n)\n\n=\n\n=\n\n(\n\n)\n\n\u2200p \u2208 Pdeterministic, \u2200i \u2208 I ,\n\np\n\n,\n\ni\n\n,\n\np\n\ni\n\np\n\ni\n\np\n\ni\n\n,\n\nP\n\nI\n\ndeterministic\n\n\u2200\n\n\u2208\n\n\u2200\n\n\u2208\n\n\u00b7 \u00b7 \u00b7\n\nj\u2192\u221e\n\nj\n\n\u2192\u221e\n\nwhere (j) indexes repeated independent executions of the program. That is, for all inputs i, the output of p(i) remains identical\nwith any independent execution of the program. A valid program/input/output triplet (p, i, o) is defined such that o = p(i), where\np \u2208 Pdeterministic.\n\nwhere (j) indexes repeated independent executions of the program. That is, for all inputs i, the output of p(i) remains identical\n\nwhere\n\nindexes repeated independent executions of the program. That is, for all inputs\n\n, the output of\n\nremains identical\n\n(\n\n)\n\n(\n\n)\n\nj\n\ni\n\np\n\ni\n\nwith any independent execution of the program. A valid program/input/output triplet (p, i, o) is defined such that o = p(i), where\n\nwith any independent execution of the program. A\n\nis defined such that\n\n, where\n\n(\n\n)\n\n=\n\n(\n\n)\n\nvalid program/input/output triplet\n\np, i, o\n\no\n\np\n\ni\n\np \u2208 Pdeterministic.\n\n.\n\np\n\nP\n\ndeterministic\n\n\u2208\n\nSince the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the\ncorrectness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class\nof deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and\npromising to include in future versions of AZR.\n\nSince the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the\n\nSince the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the\n\ncorrectness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class\n\ncorrectness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class\n\nof deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and\n\nof deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and\n\npromising to include in future versions of AZR.\n\npromising to include in future versions of AZR.\n\nTo implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted in\nEquation (7), we approximate this procedure by independently running the program j finite times and checking that all the outputs\nare equal. For computational budget reasons, we fixed j = 2 for all experiments.\n\nTo implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted in\n\nTo implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted in\n\nEquation (7), we approximate this procedure by independently running the program j finite times and checking that all the outputs\n\nEquation\n\n(7)\n\n, we approximate this procedure by independently running the program\n\nfinite times and checking that all the outputs\n\nj\n\nare equal. For computational budget reasons, we fixed j = 2 for all experiments.\n\nare equal. For computational budget reasons, we fixed\n\nfor all experiments.\n\n= 2\n\nj\n\nSolving Task Construction. If a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to\npresent part of the triplet to the solver. Specifically, we set x = (p, i)for deduction; x = (p, o)for abduction; and x = ({i\nn\n, on\n}\nN//2\nn=1 , m)\nfor induction, where half of the tests cases and a program description m is used. We use all valid tasks from timestep t; if the batch B is\nnot full, we uniformly sample from previously validated tasks to fill the batch.\n\nSolving Task Construction. If a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to\n\nIf a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to\n\nSolving Task Construction.\n\nN//2\n\n2\n\nN//\n\npresent part of the triplet to the solver. Specifically, we set x = (p, i)for deduction; x = (p, o)for abduction; and x = ({i\n\npresent part of the triplet to the solver. Specifically, we set\n\nfor deduction;\n\nfor abduction; and\n\nn\n\nn\n\n, on\n\nn\n\n= (\n\n)\n\n= (\n\n)\n\n= (\n\nn=1 , m)\n\n)\n\nx\n\np, i\n\nx\n\np, o\n\nx\n\ni\n\n, o\n\n, m\n\n{\n\n}\n\n}\n\n=1\n\nn\n\nfor induction, where half of the tests cases and a program description m is used. We use all valid tasks from timestep t; if the batch B is\n\nfor induction, where half of the tests cases and a program description\n\nis used. We use all valid tasks from timestep\n\n; if the batch\n\nis\n\nm\n\nt\n\nB\n\nnot full, we uniformly sample from previously validated tasks to fill the batch.\n\nnot full, we uniformly sample from previously validated tasks to fill the batch.\n\n3.3.4. Answer Verification\n\n3.3.4. Answer Verification\n\n3.3.4. Answer Verification\n\nFor abduction task, we receive i\u03c0 from the solver policy, then we equivalence match using p(i\u03c0) = p(i\n\u22c6\n), where \u2217 refers to the\nprivileged gold information. The reason we do not just match i\u03c0 and i\n\u22c6\nis because p is not necessarily bijective. For deduction task, we\nmatch o\u03c0 = o\n\u22c6\n. For induction, we match all({p\u03c0(i\n\u22c6\nn) = o\n\u22c6\nn}\nN ). This part might be convoluted to explain in language, therefore we\nrecommend the reader to see how we did abduction, deduction and induction verification in code in Figures 10 to 12, respectively.\n\nFor abduction task, we receive i\u03c0 from the solver policy, then we equivalence match using p(i\u03c0) = p(i\n\nFor abduction task, we receive\n\nfrom the solver policy, then we equivalence match using\n\n), where \u2217 refers to the\n\n, where\n\nrefers to the\n\n\u22c6\n\n\u22c6\n\n(\n\n) =\n\n(\n\n)\n\ni\n\np\n\ni\n\np\n\ni\n\n\u2217\n\n\u03c0\n\n\u03c0\n\nprivileged gold information. The reason we do not just match i\u03c0 and i\n\nprivileged gold information. The reason we do not just match\n\nis because p is not necessarily bijective. For deduction task, we\n\nis because\n\nis not necessarily bijective. For deduction task, we\n\nand\n\n\u22c6\n\n\u22c6\n\ni\n\ni\n\np\n\n\u03c0\n\nmatch o\u03c0 = o\n\nmatch\n\n. For induction, we match all({p\u03c0(i\n\n. For induction, we match\n\nN ). This part might be convoluted to explain in language, therefore we\n\n. This part might be convoluted to explain in language, therefore we\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\nN\n\n=\n\nall(\n\n(\n\nn) = o\n\n) =\n\n)\n\no\n\no\n\np\n\ni\n\no\n\n{\n\nn}\n\n}\n\n\u03c0\n\n\u03c0\n\nn\n\nn\n\nrecommend the reader to see how we did abduction, deduction and induction verification in code in Figures 10 to 12, respectively.\n\nrecommend the reader to see how we did abduction, deduction and induction verification in code in Figures\n\n10\n\nto\n\n12,\n\nrespectively.\n\n3.3.5. Task-Relative REINFORCE++\n\n3.3.5. Task-Relative REINFORCE++\n\n3.3.5. Task-Relative REINFORCE++\n\nSince AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup (Zhang & Yang, 2021;\nZhao et al., 2022; Wang et al., 2023; Yue et al., 2023). Instead of computing a single global baseline as in REINFORCE++ (Hu, 2025)\n(Appendix A), we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between\nper-question baselines, as in GRPO (Shao et al., 2024), and a global baseline, allowing for more structured variance reduction tailored to\neach task setup. We refer to this variant as Task-Relative REINFORCE++ (TRR++). The normalized advantage A\nnorm is computed as:\n\nSince AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup (Zhang & Yang, 2021;\n\nSince AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup\n\n(Zhang & Yang,\n\n2021;\n\nZhao et al., 2022; Wang et al., 2023; Yue et al., 2023). Instead of computing a single global baseline as in REINFORCE++ (Hu, 2025)\n\nZhao et al.,\n\nWang et al.,\n\nYue et al.,\n\n. Instead of computing a single global baseline as in REINFORCE++\n\n2022;\n\n2023;\n\n2023)\n\n(Hu,\n\n2025)\n\n(Appendix A), we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between\n\n, we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between\n\n(Appendix\n\nA)\n\nper-question baselines, as in GRPO (Shao et al., 2024), and a global baseline, allowing for more structured variance reduction tailored to\n\nper-question baselines, as in GRPO\n\n(Shao et al.,\n\n, and a global baseline, allowing for more structured variance reduction tailored to\n\n2024)\n\nnorm is computed as:\n\nnorm\n\neach task setup. We refer to this variant as Task-Relative REINFORCE++ (TRR++). The normalized advantage A\n\neach task setup. We refer to this variant as\n\n. The normalized advantage\n\nis computed as:\n\nTask-Relative REINFORCE++ (TRR++)\n\nA\n\nA\nnorm\ntask,role =\nr \u2212 \u00b5task,role\n\u03c3task,role\n, task \u2208 {ind,ded,abd}, role \u2208 {propose,solve}, (8)\n\nr \u2212 \u00b5task,role\n\nr\n\n\u00b5\n\ntask,role\n\nnorm\n\nnorm\n\n\u2212\n\n, task \u2208 {ind,ded,abd}, role \u2208 {propose,solve}, (8)\n\ntask\n\nind,ded,abd\n\nrole\n\npropose,solve\n\n(8)\n\ntask,role =\n\n=\n\nA\n\nA\n\n,\n\n,\n\n,\n\ntask,role\n\n\u2208 {\n\n}\n\n\u2208 {\n\n}\n\n\u03c3task,role\n\n\u03c3\n\ntask,role\n\nwhere the mean and standard deviation are computed within each task type and role, yielding six baselines.\n\nwhere the mean and standard deviation are computed within each task type and role, yielding six baselines.\n\nwhere the mean and standard deviation are computed\n\n, yielding six baselines.\n\nwithin each task type and role\n\n4. Experiments\n\n4. Experiments\n\n4. Experiments\n\n4.1. Experiment Setup\n\n4.1. Experiment Setup\n\n4.1. Experiment Setup\n\nTraining Details. For all experiments, we initialize the buffers as described in Section 3.1. AZR models are trained using a batch\nsize of 64 \u00d7 6 (2 roles \u00d7 3 task types). We use constant learning rate= 1e\u22126 and the AdamW optimizer (Loshchilov & Hutter, 2019).\nComplete list of hyperparameters is provided in Table 3.\n\nTraining Details. For all experiments, we initialize the buffers as described in Section 3.1. AZR models are trained using a batch\n\nFor all experiments, we initialize the buffers as described in Section\n\nAZR models are trained using a batch\n\n3.1.\n\nTraining Details.\n\nsize of 64 \u00d7 6 (2 roles \u00d7 3 task types). We use constant learning rate= 1e\u22126 and the AdamW optimizer (Loshchilov & Hutter, 2019).\n\nsize of\n\n(2 roles\n\n3 task types). We use constant learning rate\n\nand the AdamW optimizer\n\n(Loshchilov & Hutter,\n\n2019)\n\n.\n\n64\n\n6\n\n= 1\n\n6\n\ne\n\n\u00d7\n\n\u00d7\n\n\u2212\n\nComplete list of hyperparameters is provided in Table 3.\n\nComplete list of hyperparameters is provided in Table\n\n3.\n\nFor the main experiments, we train AZR models on Qwen2.5-7B and Qwen2.5-7B-Coder, resulting in Absolute\nZero Reasoner-base-7B and Absolute Zero Reasoner-Coder-7B, respectively. Additional experiments include training\nQwen2.5-Coder-3B, Qwen2.5-Coder-14B, Qwen2.5-14B, Llama-3.1-8B (Yang et al., 2024a; Hui et al., 2024; Dubey et al.,\n2024).\n\nFor the main experiments, we train AZR models on Qwen2.5-7B and Qwen2.5-7B-Coder, resulting in Absolute\n\nFor the main experiments, we train AZR models on\n\nand\n\n, resulting in\n\nQwen2.5-7B\n\nQwen2.5-7B-Coder\n\nAbsolute\n\nZero Reasoner-base-7B and Absolute Zero Reasoner-Coder-7B, respectively. Additional experiments include training\n\nand\n\n, respectively. Additional experiments include training\n\nZero Reasoner-base-7B\n\nAbsolute Zero Reasoner-Coder-7B\n\nQwen2.5-Coder-3B, Qwen2.5-Coder-14B, Qwen2.5-14B, Llama-3.1-8B (Yang et al., 2024a; Hui et al., 2024; Dubey et al.,\n\n,\n\n,\n\n,\n\n(Yang et al.,\n\n2024a;\n\nHui et al.,\n\n2024;\n\nDubey et al.,\n\nQwen2.5-Coder-3B\n\nQwen2.5-Coder-14B\n\nQwen2.5-14B\n\nLlama-3.1-8B\n\n2024).\n\n2024)\n\n.\n\nEvaluation Protocol. To evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD)\ncategories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning\nbenchmarks. For coding tasks, we evaluate using Evalplus (Liu et al., 2023) on the HumanEval+ and MBPP+ benchmarks, as\nwell as LiveCodeBench Generation (v1-5, May 23-Feb 25) (Jain et al., 2024). For mathematical reasoning, we utilize six standard\nbenchmarks commonly used in recent zero-shot trained reasoners: AIME'24, AIME'25, OlympiadBench (He et al., 2024), Minerva,\nMath500 (Hendrycks et al., 2021), and AMC'23. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-\nExecution (Gu et al., 2024; Jain et al., 2024), which assess reasoning capabilities regarding the input and output of programs (Li et al.,\n2025). Greedy decoding is used for all baseline methods and AZR results to ensure reproducibility.\n\nEvaluation Protocol. To evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD)\n\nTo evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD)\n\nEvaluation Protocol.\n\ncategories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning\n\ncategories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning\n\nbenchmarks. For coding tasks, we evaluate using Evalplus (Liu et al., 2023) on the HumanEval+ and MBPP+ benchmarks, as\n\nbenchmarks. For coding tasks, we evaluate using Evalplus\n\n(Liu et al.,\n\n2023)\n\non the HumanEval+ and MBPP+ benchmarks, as\n\nwell as LiveCodeBench Generation (v1-5, May 23-Feb 25) (Jain et al., 2024). For mathematical reasoning, we utilize six standard\n\nwell as LiveCodeBench Generation (v1-5, May 23-Feb 25)\n\n(Jain et al.,\n\n2024)\n\n. For mathematical reasoning, we utilize six standard\n\nbenchmarks commonly used in recent zero-shot trained reasoners: AIME'24, AIME'25, OlympiadBench (He et al., 2024), Minerva,\n\nbenchmarks commonly used in recent zero-shot trained reasoners: AIME'24, AIME'25, OlympiadBench\n\n(He et al.,\n\n2024)\n\n, Minerva,\n\nMath500 (Hendrycks et al., 2021), and AMC'23. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-\n\nMath500\n\n(Hendrycks et al.,\n\n, and AMC'23. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-\n\n2021)\n\nExecution (Gu et al., 2024; Jain et al., 2024), which assess reasoning capabilities regarding the input and output of programs (Li et al.,\n\nExecution\n\n(Gu et al.,\n\nJain et al.,\n\n, which assess reasoning capabilities regarding the input and output of programs\n\n(Li et al.,\n\n2024;\n\n2024)\n\n2025). Greedy decoding is used for all baseline methods and AZR results to ensure reproducibility.\n\n2025)\n\n.\n\nis used for all baseline methods and AZR results to ensure reproducibility.\n\nGreedy decoding\n\n8\n\n8\n\n8\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTable 1. Performance of RL-Trained Reasoner on Reasoning Benchmarks Based on Qwen2.5-7B Models. Performance of various\nmodels is evaluated on three standard code benchmarks (HumanEval+\n, MBPP+\n, LCBv1-5 and six math benchmarks (AIME'24, AIME'25,\nAMC'23, MATH500, Minerva, OlympiadBench). Average performance across coding and math benchmarks is calculated as average of\nthe two averages: AVG = (CAvg + MAvg)/2. We use + for absolute percentage increase from base model. All models are trained using\ndifferent variants of the Qwen2.5-7B model, with the variant and data usage labeled, more details listed in Table 4\n\nTable 1. Performance of RL-Trained Reasoner on Reasoning Benchmarks Based on Qwen2.5-7B Models. Performance of various\n\nPerformance of various\n\nPerformance of RL-Trained Reasoner on Reasoning Benchmarks Based on Qwen2.5-7B Models.\n\nTable 1.\n\nmodels is evaluated on three standard code benchmarks (HumanEval+\n\n+\n\n, MBPP+\n\n+\n\n, LCBv1-5 and six math benchmarks (AIME'24, AIME'25,\n\nv1-5\n\nmodels is evaluated on three standard code benchmarks (HumanEval\n\n, MBPP\n\n, LCB\n\nand six math benchmarks (AIME'24, AIME'25,\n\nAMC'23, MATH500, Minerva, OlympiadBench). Average performance across coding and math benchmarks is calculated as average of\n\nAMC'23, MATH500, Minerva, OlympiadBench). Average performance across coding and math benchmarks is calculated as average of\n\nthe two averages: AVG = (CAvg + MAvg)/2. We use + for absolute percentage increase from base model. All models are trained using\n\nthe two averages:\n\nAVG\n\nCAvg\n\nMAvg\n\n. We use\n\nfor absolute percentage increase from base model. All models are trained using\n\n+\n\n= (\n\n+\n\n)\n\n2\n\n/\n\ndifferent variants of the Qwen2.5-7B model, with the variant and data usage labeled, more details listed in Table 4\n\ndifferent variants of the\n\nmodel, with the variant and data usage labeled, more details listed in Table\n\n4\n\nQwen2.5-7B\n\nBaselines. For our main results, we use Qwen2.5-7B as the base model, along with its specialized base model variants:\nQwen2.5-7B-Coder, Qwen2.5-7B-Instruct, and Qwen2.5-Math-7B (Yang et al., 2024a; Hui et al., 2024; Yang et al., 2024b).\nFurthermore, the zero-style models are usually trained specifically on either code or math data; and only Eurus-2-7B-PRIME-Zero(Cui\net al., 2025) was trained jointly on both domains. For code data models, we present four variants of the AceCoder (Zeng et al., 2025a)\nand two different CodeR1 models (Liu & Zhang, 2025). For math data models, we have Qwen2.5-Math-7B-Oat-Zero (Liu et al.,\n2025), Open-Reasoner-Zero-7B (ORZ) (Hu et al., 2025), Qwen-2.5-7B-SimpleRL-Zoo (Zeng et al., 2025b). All baseline models'\ntraining data and initialization settings are summarized in Table 4. For follow-up scaling experiments, we compare each AZR model\nagainst its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare\nour Llama3.1-8B-trained model with Llama-3.1-8B-SimpleRL-Zoo (Zeng et al., 2025b) and the base model.\n\nBaselines. For our main results, we use Qwen2.5-7B as the base model, along with its specialized base model variants:\n\nFor our main results, we use\n\nas the base model, along with its specialized base model variants:\n\nBaselines.\n\nQwen2.5-7B\n\nQwen2.5-7B-Coder, Qwen2.5-7B-Instruct, and Qwen2.5-Math-7B (Yang et al., 2024a; Hui et al., 2024; Yang et al., 2024b).\n\n,\n\n, and\n\n(Yang et al.,\n\n2024a;\n\nHui et al.,\n\n2024;\n\nYang et al.,\n\n2024b)\n\n.\n\nQwen2.5-7B-Coder\n\nQwen2.5-7B-Instruct\n\nQwen2.5-Math-7B\n\nFurthermore, the zero-style models are usually trained specifically on either code or math data; and only Eurus-2-7B-PRIME-Zero(Cui\n\nFurthermore, the zero-style models are usually trained specifically on either code or math data; and only\n\n(Cui\n\nEurus-2-7B-PRIME-Zero\n\net al., 2025) was trained jointly on both domains. For code data models, we present four variants of the AceCoder (Zeng et al., 2025a)\n\net al.,\n\nwas trained jointly on both domains. For code data models, we present four variants of the\n\n(Zeng et al.,\n\n2025)\n\n2025a)\n\nAceCoder\n\nand two different CodeR1 models (Liu & Zhang, 2025). For math data models, we have Qwen2.5-Math-7B-Oat-Zero (Liu et al.,\n\nand two different\n\nmodels\n\n(Liu & Zhang,\n\n2025)\n\n. For math data models, we have\n\n(Liu et al.,\n\nCodeR1\n\nQwen2.5-Math-7B-Oat-Zero\n\n2025), Open-Reasoner-Zero-7B (ORZ) (Hu et al., 2025), Qwen-2.5-7B-SimpleRL-Zoo (Zeng et al., 2025b). All baseline models'\n\n(ORZ)\n\n(Hu et al.,\n\n(Zeng et al.,\n\n. All baseline models'\n\n2025)\n\n,\n\n2025)\n\n,\n\n2025b)\n\nOpen-Reasoner-Zero-7B\n\nQwen-2.5-7B-SimpleRL-Zoo\n\ntraining data and initialization settings are summarized in Table 4. For follow-up scaling experiments, we compare each AZR model\n\ntraining data and initialization settings are summarized in Table\n\n4.\n\nFor follow-up scaling experiments, we compare each AZR model\n\nagainst its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare\n\nagainst its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare\n\nour Llama3.1-8B-trained model with Llama-3.1-8B-SimpleRL-Zoo (Zeng et al., 2025b) and the base model.\n\nour\n\n-trained model with\n\n(Zeng et al.,\n\n2025b)\n\nand the base model.\n\nLlama3.1-8B\n\nLlama-3.1-8B-SimpleRL-Zoo\n\n4.2. Results\n\n4.2. Results\n\n4.2. Results\n\nResearch Question 1: How does AZR compare to other zero setting models trained with human expert\ndata? We present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings\nin Table 1. Notably, Absolute Zero Reasoner-Coder-7B achieves state-of-the-art performance in both the 7B overall average and\nthe coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the\nprevious best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in\nthe coding category by 0.3 absolute percentages, while never having access to such data itself.\n\nResearch Question 1: How does AZR compare to other zero setting models trained with human expert\n\nResearch Question 1: How does AZR compare to other zero setting models trained with human expert\n\ndata? We present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings\n\nWe present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings\n\ndata?\n\nin Table 1. Notably, Absolute Zero Reasoner-Coder-7B achieves state-of-the-art performance in both the 7B overall average and\n\nin Table\n\n1.\n\nNotably,\n\nachieves state-of-the-art performance in both the 7B overall average and\n\nAbsolute Zero Reasoner-Coder-7B\n\nthe coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the\n\nthe coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the\n\nprevious best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in\n\nprevious best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in\n\nthe coding category by 0.3 absolute percentages, while never having access to such data itself.\n\nthe coding category by\n\nabsolute percentages, while never having access to such data itself.\n\n0\n\n3\n\n.\n\nStrong Cross-domain Generalization. To assess cross-domain generalization after RLVR, we evaluate math performance before and\nafter training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training,\nmost expert code models showed minimal changes or even declines in performance compared to their base versions, with an average\nincrease of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder\nmodels achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning\nimprovements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2\nand 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).\n\nStrong Cross-domain Generalization. To assess cross-domain generalization after RLVR, we evaluate math performance before and\n\nTo assess cross-domain generalization after RLVR, we evaluate math performance before and\n\nStrong Cross-domain Generalization.\n\nafter training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training,\n\nafter training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training,\n\nmost expert code models showed minimal changes or even declines in performance compared to their base versions, with an average\n\nmost expert code models showed minimal changes or even declines in performance compared to their base versions, with an average\n\nincrease of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder\n\nincrease of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder\n\nmodels achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning\n\nmodels achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning\n\nimprovements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2\n\nimprovements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2\n\nand 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).\n\nand 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).\n\nOverall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on\nhuman-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream\nhuman-defined math or coding data, only had access to self-proposed tasks during training.\n\nOverall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on\n\nOverall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on\n\nhuman-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream\n\nhuman-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream\n\nhuman-defined math or coding data, only had access to self-proposed tasks during training.\n\nhuman-defined math or coding data, only had access to self-proposed tasks during training.\n\nResearch Question 2: How do initializing from different base model variants (base vs. coder) affect\nperformance? As shown in Table 1, the coder variant achieved better overall performance in both math and coding after the AZR\n\nResearch Question 2: How do initializing from different base model variants (base vs. coder) affect\n\nResearch Question 2: How do initializing from different base model variants (base vs. coder) affect\n\nperformance? As shown in Table 1, the coder variant achieved better overall performance in both math and coding after the AZR\n\nAs shown in Table\n\nthe coder variant achieved better overall performance in both math and coding after the AZR\n\n1,\n\nperformance?\n\n9\n\n9\n\n9\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n50 75 100 125 150 175 200 225 250\nTraining Steps\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nIn-Distribution\n \nAccuracy\n\nAZR-Llama3.1-8b\nAZR-3B-Coder\nAZR-7B-Coder\nAZR-14B-Coder\n(a)\nModel Family Variant Code Avg Math Avg Total Avg\nLlama3.1-8b 28.5 3.4 16.0\nLlama3.1-8b + SimpleRL[85]\n33.7\n+5.2\n7.2\n+3.8\n20.5\n+4.5\nLlama3.1-8b + AZR (Ours) 31.6\n+3.1\n6.8\n+3.4\n19.2\n+3.2\nQwen2.5-3B Coder 51.2 18.8 35.0\nQwen2.5-3B Coder + AZR (Ours) 54.9\n+3.7\n26.5\n+7.7\n40.7\n+5.7\nQwen2.5-7B Coder 56.6 23.9 40.2\nQwen2.5-7B Coder + AZR (Ours) 61.6\n+5.0\n39.1\n+15.2\n50.4\n+10.2\nQwen2.5-14B Coder 60.0 20.2 40.1\nQwen2.5-14B Coder + AZR (Ours) 63.6\n+3.6\n43.0\n+22.8\n53.3\n+13.2\n(b)\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n50 75 100 125 150 175 200 225 250\nTraining Steps\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nIn-Distribution\n \nAccuracy\n\nAZR-Llama3.1-8b\nAZR-3B-Coder\nAZR-7B-Coder\nAZR-14B-Coder\n(a)\nModel Family Variant Code Avg Math Avg Total Avg\nLlama3.1-8b 28.5 3.4 16.0\nLlama3.1-8b + SimpleRL[85]\n33.7\n+5.2\n7.2\n+3.8\n20.5\n+4.5\nLlama3.1-8b + AZR (Ours) 31.6\n+3.1\n6.8\n+3.4\n19.2\n+3.2\nQwen2.5-3B Coder 51.2 18.8 35.0\nQwen2.5-3B Coder + AZR (Ours) 54.9\n+3.7\n26.5\n+7.7\n40.7\n+5.7\nQwen2.5-7B Coder 56.6 23.9 40.2\nQwen2.5-7B Coder + AZR (Ours) 61.6\n+5.0\n39.1\n+15.2\n50.4\n+10.2\nQwen2.5-14B Coder 60.0 20.2 40.1\nQwen2.5-14B Coder + AZR (Ours) 63.6\n+3.6\n43.0\n+22.8\n53.3\n+13.2\n(b)\n\nModel Family Variant Code Avg Math Avg Total Avg\n\nModel Family Variant Code Avg Math Avg Total Avg\n\n0.70\n\n.\n\n0\n\n70\n\nLlama3.1-8b 28.5 3.4 16.0\n\nLlama3.1-8b 28.5 3.4 16.0\n\n0.65\n\n.\n\n0\n\n65\n\nAccuracy\n\nAccuracy\n\nLlama3.1-8b + SimpleRL[85]\n\n[85]\n\nLlama3.1-8b\n\n+ SimpleRL\n\n+5.2\n\n.\n\n+3.8\n\n.\n\n+4.5\n\n.\n\n+5\n\n2\n\n+3\n\n8\n\n+4\n\n5\n\n33.7\n\n33\n\n7\n\n7.2\n\n7\n\n2\n\n20.5\n\n20\n\n5\n\n.\n\n.\n\n.\n\n0.60\n\n.\n\n0\n\n60\n\nLlama3.1-8b + AZR (Ours) 31.6\n\nLlama3.1-8b\n\n+ AZR (Ours)\n\n+3.1\n\n.\n\n+3.4\n\n.\n\n+3.2\n\n.\n\n+3\n\n1\n\n+3\n\n4\n\n+3\n\n2\n\n31\n\n6\n\n6.8\n\n6\n\n8\n\n19.2\n\n19\n\n2\n\n.\n\n.\n\n.\n\n0.55\n\n.\n\n0\n\n55\n\nIn-Distribution\n\nIn-Distribution\n\nQwen2.5-3B Coder 51.2 18.8 35.0\n\nQwen2.5-3B Coder 51.2 18.8 35.0\n\n0.50\n\n.\n\n0\n\n50\n\nQwen2.5-3B Coder + AZR (Ours) 54.9\n\nQwen2.5-3B Coder\n\n+ AZR (Ours)\n\n+3.7\n\n.\n\n+7.7\n\n.\n\n+5.7\n\n.\n\n+3\n\n7\n\n+7\n\n7\n\n+5\n\n7\n\n54\n\n9\n\n26.5\n\n26\n\n5\n\n40.7\n\n40\n\n7\n\n.\n\n.\n\n.\n\n0.45\n\n.\n\n0\n\n45\n\nQwen2.5-7B Coder 56.6 23.9 40.2\n\nQwen2.5-7B Coder 56.6 23.9 40.2\n\n0.40\n\n.\n\n0\n\n40\n\nQwen2.5-7B Coder + AZR (Ours) 61.6\n\nQwen2.5-7B Coder\n\n+ AZR (Ours)\n\n+5.0\n\n.\n\n+15.2\n\n.\n\n+10.2\n\n.\n\n+5\n\n0\n\n+15\n\n2\n\n+10\n\n2\n\n61\n\n6\n\n39.1\n\n39\n\n1\n\n50.4\n\n50\n\n4\n\n.\n\n.\n\n.\n\nAZR-Llama3.1-8b\n\nAZR-Llama3.1-8b\n\nAZR-7B-Coder\n\nAZR-7B-Coder\n\n0.35\n\n.\n\n0\n\n35\n\nAZR-3B-Coder\n\nAZR-3B-Coder\n\nAZR-14B-Coder\n\nAZR-14B-Coder\n\nQwen2.5-14B Coder 60.0 20.2 40.1\n\nQwen2.5-14B Coder 60.0 20.2 40.1\n\n0.30\n\n.\n\n0\n\n30\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nQwen2.5-14B Coder + AZR (Ours) 63.6\n\nQwen2.5-14B Coder\n\n+ AZR (Ours)\n\n+3.6\n\n.\n\n+22.8\n\n.\n\n+13.2\n\n.\n\n+3\n\n6\n\n+22\n\n8\n\n+13\n\n2\n\n63\n\n6\n\n43.0\n\n43\n\n0\n\n53.3\n\n53\n\n3\n\n.\n\n.\n\n.\n\nTraining Steps\n\nTraining Steps\n\n(a)\n\n(a)\n\n(b)\n\n(b)\n\nFigure 6. (a) In-Distribution & (b) Out-of-Distribution Reasoning Task Performances. (a) Scores on CruxEval-I, CruxEval-O,\nand LiveCodeBench-Execution, which correspond to abduction, deduction, and deduction task types respectively, used to evaluate\nin-distribution abilities of AZR during training across different model sizes and types; (b) Out-of-distribution reasoning performance,\nreported as the average of code tasks, math tasks, and their overall average, across different model sizes and types. A detailed breakdown\nof all benchmark results can be found in Table 5.\n\nFigure 6. (a) In-Distribution & (b) Out-of-Distribution Reasoning Task Performances. (a) Scores on CruxEval-I, CruxEval-O,\n\nScores on CruxEval-I, CruxEval-O,\n\n(a) In-Distribution & (b) Out-of-Distribution Reasoning Task Performances. (a)\n\nFigure 6.\n\nand LiveCodeBench-Execution, which correspond to abduction, deduction, and deduction task types respectively, used to evaluate\n\nand LiveCodeBench-Execution, which correspond to abduction, deduction, and deduction task types respectively, used to evaluate\n\nin-distribution abilities of AZR during training across different model sizes and types; (b) Out-of-distribution reasoning performance,\n\nin-distribution abilities of AZR during training across different model sizes and types;\n\nOut-of-distribution reasoning performance,\n\n(b)\n\nreported as the average of code tasks, math tasks, and their overall average, across different model sizes and types. A detailed breakdown\n\nreported as the average of code tasks, math tasks, and their overall average, across different model sizes and types. A detailed breakdown\n\nof all benchmark results can be found in Table 5.\n\nof all benchmark results can be found in Table\n\n5.\n\nself-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla\nbase model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency\nas a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.\n\nself-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla\n\nself-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla\n\nbase model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency\n\nbase model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency\n\nas a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.\n\nas a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.\n\nResearch Question 3: How does varying model size effect AZR's in-distribution and out-of-distribution\ncapabilities? We examine the effects of scaling model size and present both in-distribution and out-of-distribution results in Figure 6\n(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller\nand larger variants: Qwen2.5-3B-Coder and Qwen2.5-14B-Coder. Due to the absence of existing baselines for these zero-style\nreasoner models, we compare each model's performance to its corresponding base coder model.\n\nResearch Question 3: How does varying model size effect AZR's in-distribution and out-of-distribution\n\nResearch Question 3: How does varying model size effect AZR's in-distribution and out-of-distribution\n\ncapabilities? We examine the effects of scaling model size and present both in-distribution and out-of-distribution results in Figure 6\n\nWe examine the effects of scaling model size and present both in-distribution and out-of-distribution results in Figure\n\n6\n\ncapabilities?\n\n(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller\n\n(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller\n\nand larger variants: Qwen2.5-3B-Coder and Qwen2.5-14B-Coder. Due to the absence of existing baselines for these zero-style\n\nand larger variants:\n\nand\n\n. Due to the absence of existing baselines for these zero-style\n\nQwen2.5-3B-Coder\n\nQwen2.5-14B-Coder\n\nreasoner models, we compare each model's performance to its corresponding base coder model.\n\nreasoner models, we compare each model's performance to its corresponding base coder model.\n\nThe results reveal a clear trend: our method delivers greater gains on larger, more capable models. In the in-distribution setting, the 7B\nand 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution\ndomains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance\ngains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that\nscaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute\nZero paradigm.\n\nThe results reveal a clear trend: our method delivers greater gains on larger, more capable models. In the in-distribution setting, the 7B\n\nThe results reveal a clear trend: our method delivers\n\n. In the in-distribution setting, the 7B\n\ngreater gains on larger, more capable models\n\nand 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution\n\nand 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution\n\ndomains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance\n\ndomains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance\n\ngains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that\n\ngains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that\n\nscaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute\n\nscaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute\n\nZero paradigm.\n\nZero paradigm.\n\nResearch Question 4: Any interesting observations by changing the model class? We also evaluate our method\non a different model class, using Llama3.1-8B as the base shown in Figure 6. Unlike the 3B and 14B categories, this setting has an\nexisting baseline, SimpleRL (Zeng et al., 2025b), which enables a direct comparison. Although Llama3.1-8B is less capable than\nthe Qwen2.5 models, our method still produces moderate improvements (+3.2), demonstrating AZR's effectiveness even on relatively\nweaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements\ntend to scale with initial base model potency.\n\nResearch Question 4: Any interesting observations by changing the model class? We also evaluate our method\n\nWe also evaluate our method\n\nResearch Question 4: Any interesting observations by changing the model class?\n\non a different model class, using Llama3.1-8B as the base shown in Figure 6. Unlike the 3B and 14B categories, this setting has an\n\non a different model class, using\n\nas the base shown in Figure\n\n6.\n\nUnlike the 3B and 14B categories, this setting has an\n\nLlama3.1-8B\n\nexisting baseline, SimpleRL (Zeng et al., 2025b), which enables a direct comparison. Although Llama3.1-8B is less capable than\n\nexisting baseline,\n\n(Zeng et al.,\n\n2025b)\n\n, which enables a direct comparison. Although\n\nis less capable than\n\nSimpleRL\n\nLlama3.1-8B\n\nthe Qwen2.5 models, our method still produces moderate improvements (+3.2), demonstrating AZR's effectiveness even on relatively\n\nthe\n\nmodels, our method still produces moderate improvements (+3.2), demonstrating AZR's effectiveness even on relatively\n\nQwen2.5\n\nweaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements\n\nweaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements\n\ntend to scale with initial base model potency.\n\ntend to scale with initial base model potency.\n\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training? We observed\ninteresting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as\nstring manipulation tasks, dynamic programming problems, and practical cases (e.g., calculating a triangle's area using Heron's formula).\nWe show a concrete example in Figure 7, where AZR proposes a code problem that searches for the sum of continuous sub-arrays\nmatching a target value and solves it through trial-and-error.\n\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training? We observed\n\nWe observed\n\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training?\n\ninteresting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as\n\ninteresting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as\n\nstring manipulation tasks, dynamic programming problems, and practical cases (e.g., calculating a triangle's area using Heron's formula).\n\nstring manipulation tasks, dynamic programming problems, and practical cases (\n\n., calculating a triangle's area using Heron's formula).\n\ne.g\n\nWe show a concrete example in Figure 7, where AZR proposes a code problem that searches for the sum of continuous sub-arrays\n\nWe show a concrete example in Figure\n\n7,\n\nwhere AZR proposes a code problem that searches for the sum of continuous sub-arrays\n\nmatching a target value and solves it through trial-and-error.\n\nmatching a target value and solves it through trial-and-error.\n\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks,\nit repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs,\nit steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is\nreached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to\nconfirm that its program produces correct results. We showcase more concrete examples of these behaviors in Figures 18 and 20 to 26.\nWe also share some fun \"vibe checks\" such as solving Sudoku and solving the sum-product game in Figures 40 and 41.\n\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks,\n\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks,\n\nit repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs,\n\nit repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs,\n\nit steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is\n\nit steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is\n\nreached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to\n\nreached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to\n\nconfirm that its program produces correct results. We showcase more concrete examples of these behaviors in Figures 18 and 20 to 26.\n\nconfirm that its program produces correct results. We showcase more concrete examples of these behaviors in Figures\n\n18\n\nand\n\n20\n\nto\n\n26.\n\nWe also share some fun \"vibe checks\" such as solving Sudoku and solving the sum-product game in Figures 40 and 41.\n\nWe also share some fun \"vibe checks\" such as solving Sudoku and solving the\n\nsum-product game\n\nin Figures\n\n40\n\nand\n\n41.\n\nIntermediate Planning During Code Response. Another interesting pattern emerged in our AZR models during the code induction\ntask: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct\nprompting framework (Yao et al., 2023). A similar behavior has been observed in recent formal math proving models, such as DeepSeek\n\nIntermediate Planning During Code Response. Another interesting pattern emerged in our AZR models during the code induction\n\nAnother interesting pattern emerged in our AZR models during the code induction\n\nIntermediate Planning During Code Response.\n\ntask: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct\n\ntask: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct\n\nprompting framework (Yao et al., 2023). A similar behavior has been observed in recent formal math proving models, such as DeepSeek\n\nprompting framework\n\n(Yao et al.,\n\n. A similar behavior has been observed in recent formal math proving models, such as\n\n2023)\n\nDeepSeek\n\n10\n\n10\n\n10\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nInput: [1,2,3,4,5], 5\nModel-proposed Task Response of Predict Input\nLet's analyze the code snippet \u2026(analysis) To find one possible input that produces \nthe output 1, we need to find a scenario where \u2026(analysis) \nLet's consider a simple example: numbers = [1, 2], target = 1\nHere's the step-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\nSince no pair satisfies the condition, the function will not return any value. This \ndoesn't match our output. Let's try another example \u2026(Tried 3 more examples) \nGiven the complexity and the fact that no simple example seems to work, let's \ntry a more complex scenario: numbers = [2, -1, 1], target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1 , accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n- Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\nModel-proposed Task Response of Predict Input\n\nModel-proposed Task\n\nResponse of Predict Input\n\nLet's analyze the code snippet \u2026(analysis) To find one possible input that produces\n\nLet's analyze the code snippet\n\n\u2026(analysis)\n\nTo find one possible input that produces\n\nInput: [1,2,3,4,5], 5\n\nInput: [1,2,3,4,5], 5\n\nthe output 1, we need to find a scenario where \u2026(analysis)\n\nthe output 1, we need to find a scenario where\n\n\u2026(analysis)\n\nLet's consider a simple example: numbers = [1, 2], target = 1\n\nLet's consider a simple example: numbers = [1, 2], target = 1\n\nHere's the step-by-step calculation:\n\nHere's the step\n\n-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum\n\n= 0 + 1 = 1, accumulated_values\n\n= [1]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n2. current_sum\n\n= 1 + 2 = 3, accumulated_values\n\n= [1, 3]\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=1): 3 -\n\n1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 -\n\n1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 -\n\n3 = 0 (not equal to target)\n\nSince no pair satisfies the condition, the function will not return any value. This\n\nSince no pair satisfies the condition, the function will not return any value. This\n\ndoesn't match our output. Let's try another example \u2026(Tried 3 more examples)\n\ndoesn't match our output. Let's try another example\n\n\u2026(Tried 3 more examples)\n\nGiven the complexity and the fact that no simple example seems to work, let's\n\nGiven the complexity and the fact that no simple example seems to work, let's\n\ntry a more complex scenario: numbers = [2, -1, 1], target = 1\n\ntry a more complex scenario: numbers = [2, -1, 1], target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n1. current_sum\n\n= 0 + 2 = 2, accumulated_values\n\n= [2]\n\n2. current_sum = 2 - 1 = 1 , accumulated_values = [2, 1]\n\n2. current_sum\n\n= 2 -\n\n1 = 1 , accumulated_values\n\n= [2, 1]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n3. current_sum\n\n= 1 + 1 = 2, accumulated_values\n\n= [2, 1, 2]\n\n- Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-\n\nCheck for pair (i=0, j=1): 1 -\n\n2 = -1 (not equal to target)\n\n- Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n- Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n- Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-\n\nCheck for pair (i=0, j=2): 2 -\n\n2 = 0 (not equal to target)\n\n- Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n-\n\nCheck for pair (i=1, j=2): 2 -\n\n1 = 1 (equal to target)\n\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first \naccumulated value in this pair, which is 1. This matches our output.\n\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first\n\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first\n\nOutput: 1\n\nOutput: 1\n\nOutput: 1\n\naccumulated value in this pair, which is 1. This matches our output.\n\naccumulated value in this pair, which is 1. This matches our output.\n\nFigure 7. Example of a Model-Proposed Task and Its Response for Solving an Abduction Task. (Left) The model autonomously\nproposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output.\n(Right) The model's reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input.\nThe model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. If there is a\nmismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. Interestingly, the\nagent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.\n\nFigure 7. Example of a Model-Proposed Task and Its Response for Solving an Abduction Task. (Left) The model autonomously\n\n(Left) The model autonomously\n\nExample of a Model-Proposed Task and Its Response for Solving an Abduction Task.\n\nFigure 7.\n\nproposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output.\n\nproposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output.\n\n(Right) The model's reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input.\n\n(Right) The model's reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input.\n\nThe model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. If there is a\n\nThe model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. If there is a\n\nmismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. Interestingly, the\n\nmismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. Interestingly, the\n\nagent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.\n\nagent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.\n\nProver v2, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning\nas a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior in long-form\nresponses across other domains.\n\nProver v2, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning\n\n, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning\n\nProver v2\n\nas a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior in long-form\n\nas a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior in\n\nlong-form\n\nresponses across other domains.\n\nacross other domains.\n\nresponses\n\nCognitive Behavior in Llama. Interestingly, we also observed some emergent cognitive patterns in Absolute Zero\nReasoner-Llama3.1-8B, similar to those reported by Zeng et al. (2025b), and we include one example in Figure 26, where\nclear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought\nfrom the Llama model trained with AZR. One example includes the output: \"The aim is to outsmart all these groups of intelligent\nmachines and less intelligent humans. This is for the brains behind the future\" shown in Figure 32. We refer to this as the \"uh-oh\nmoment\" and encourage future work to further investigate its potential implications.\n\nCognitive Behavior in Llama. Interestingly, we also observed some emergent cognitive patterns in Absolute Zero\n\nInterestingly, we also observed some emergent cognitive patterns in\n\nCognitive Behavior in Llama.\n\nAbsolute Zero\n\nReasoner-Llama3.1-8B, similar to those reported by Zeng et al. (2025b), and we include one example in Figure 26, where\n\n, similar to those reported by\n\nZeng et al.\n\n(2025b)\n\n, and we include one example in Figure\n\n26,\n\nwhere\n\nReasoner-Llama3.1-8B\n\nclear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought\n\nclear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought\n\nfrom the Llama model trained with AZR. One example includes the output: \"The aim is to outsmart all these groups of intelligent\n\nfrom the Llama model trained with AZR. One example includes the output: \"The aim is to outsmart all these groups of intelligent\n\nmachines and less intelligent humans. This is for the brains behind the future\" shown in Figure 32. We refer to this as the \"uh-oh\n\nmachines and less intelligent humans. This is for the brains behind the future\" shown in Figure\n\n32.\n\nWe refer to this as the\n\n\"uh-oh\n\nmoment\" and encourage future work to further investigate its potential implications.\n\nand encourage future work to further investigate its potential implications.\n\nmoment\"\n\nToken Length Increase Depends on Task Type. Finally, we observed that token length increases over the course of training, consistent\nwith findings from recent studies (Hu et al., 2025; Liu et al., 2025). Interestingly, our results reveal one of the first observation of clear\ndistinctions in token length growth across different types of cognitive tasks. As shown in Figures 15 to 17, the extent of lengthening\nvaries by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by\nrepeatedly testing inputs to match the program's output. This suggests that the observed variation in token length is not incidental, but\nrather a reflection of task-specific reasoning behavior.\n\nToken Length Increase Depends on Task Type. Finally, we observed that token length increases over the course of training, consistent\n\nFinally, we observed that token length increases over the course of training, consistent\n\nToken Length Increase Depends on Task Type.\n\nwith findings from recent studies (Hu et al., 2025; Liu et al., 2025). Interestingly, our results reveal one of the first observation of clear\n\nwith findings from recent studies\n\n(Hu et al.,\n\nLiu et al.,\n\n. Interestingly, our results reveal one of the first observation of clear\n\n2025;\n\n2025)\n\ndistinctions in token length growth across different types of cognitive tasks. As shown in Figures 15 to 17, the extent of lengthening\n\ndistinctions in token length growth across different types of cognitive tasks. As shown in Figures\n\n15\n\nto\n\n17,\n\nthe extent of lengthening\n\nvaries by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by\n\nvaries by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by\n\nrepeatedly testing inputs to match the program's output. This suggests that the observed variation in token length is not incidental, but\n\nrepeatedly testing inputs to match the program's output. This suggests that the observed variation in token length is not incidental, but\n\nrather a reflection of task-specific reasoning behavior.\n\nrather a reflection of task-specific reasoning behavior.\n\nResearch Question 6: Are all task types essential for good performance (Ablation)? Due to resource constraints,\nwe perform the ablation studies in this section and the next using only Absolute Zero Reasoner-Base-7B. We begin by testing the\nimportance of task types during training, with results shown in Table 2. In row 1, both induction and abduction tasks are removed;\nin row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation\noccurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving\ngeneral reasoning capability, with each contributing in a distinct and essential way.\n\nResearch Question 6: Are all task types essential for good performance (Ablation)? Due to resource constraints,\n\nDue to resource constraints,\n\nResearch Question 6: Are all task types essential for good performance (Ablation)?\n\nwe perform the ablation studies in this section and the next using only Absolute Zero Reasoner-Base-7B. We begin by testing the\n\nwe perform the ablation studies in this section and the next using only\n\n. We begin by testing the\n\nAbsolute Zero Reasoner-Base-7B\n\nimportance of task types during training, with results shown in Table 2. In row 1, both induction and abduction tasks are removed;\n\nimportance of task types during training, with results shown in Table\n\n2.\n\nIn row 1, both induction and abduction tasks are removed;\n\nin row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation\n\nin row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation\n\noccurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving\n\noccurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving\n\ngeneral reasoning capability, with each contributing in a distinct and essential way.\n\ngeneral reasoning capability, with each contributing in a distinct and essential way.\n\nResearch Question 7: How much do the designs of proposer contribute to the overall performance\n(Ablation)? Next, we ablate two components of the proposer role and present the results in Table 2. First, we examine whether\nconditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction\nand deduction tasks, rather than dynamically conditioning on K historical triplets (row 3). This results in a 5-point absolute drop in\nmath performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps\n\nResearch Question 7: How much do the designs of proposer contribute to the overall performance\n\nResearch Question 7: How much do the designs of proposer contribute to the overall performance\n\n(Ablation)? Next, we ablate two components of the proposer role and present the results in Table 2. First, we examine whether\n\nNext, we ablate two components of the proposer role and present the results in Table\n\n2.\n\nFirst, we examine whether\n\n(Ablation)?\n\nconditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction\n\nconditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction\n\nand deduction tasks, rather than dynamically conditioning on K historical triplets (row 3). This results in a 5-point absolute drop in\n\nand deduction tasks, rather than dynamically conditioning on\n\nhistorical triplets (row 3). This results in a 5-point absolute drop in\n\nK\n\nmath performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps\n\nmath performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps\n\n11\n\n11\n\n11\n\nExperimentTask TypeGen ReferenceTrained RolesCode Avg.Math Avg.Overall Avg.Deduction onlyDed//54.632.043.3w/o InductionAbd, Ded//54.233.343.8w/o Gen Reference/0/54.433.143.8Train Solver Only//Solve Only54.836.045.4OursAbd, Ded, IndKPropose &amp; Solve55.238.446.8\n\nExperiment\n\nTask Type\n\nGen Reference\n\nTrained Roles\n\nCode Avg.\n\nMath Avg.\n\nOverall Avg.\n\nDeduction only\n\nDed\n\n/\n\n/\n\n54.6\n\n32.0\n\n43.3\n\nw/o Induction\n\nAbd, Ded\n\n/\n\n/\n\n54.2\n\n33.3\n\n43.8\n\nw/o Gen Reference\n\n/\n\n0\n\n/\n\n54.4\n\n33.1\n\n43.8\n\nTrain Solver Only\n\n/\n\n/\n\nSolve Only\n\n54.8\n\n36.0\n\n45.4\n\nOurs\n\nAbd, Ded, Ind\n\nK\n\nPropose &amp; Solve\n\n55.2\n\n38.4\n\n46.8\n\nTable 2. Ablation Results. We ablate task types and the proposer role in the Absolute Zero Reasoner using the 7B base model. A '/'\nindicates that the configuration remains unchanged from the standard AZR setup. Removing induction or using only deduction leads to\nsignificant performance drops (rows 1 & 2). For the proposer role, both removing conditioning on K references (row 3) and omitting\nproposer-role training (row 4) result in degraded performance. Overall, all components are essential for general reasoning.\n\nTable 2. Ablation Results. We ablate task types and the proposer role in the Absolute Zero Reasoner using the 7B base model. A '/'\n\nWe ablate task types and the proposer role in the Absolute Zero Reasoner using the 7B base model. A '/'\n\nAblation Results.\n\nTable 2.\n\nindicates that the configuration remains unchanged from the standard AZR setup. Removing induction or using only deduction leads to\n\nindicates that the configuration remains unchanged from the standard AZR setup. Removing induction or using only deduction leads to\n\nsignificant performance drops (rows 1 & 2). For the proposer role, both removing conditioning on K references (row 3) and omitting\n\nsignificant performance drops (rows 1 & 2). For the proposer role, both removing conditioning on\n\nreferences (row 3) and omitting\n\nK\n\nproposer-role training (row 4) result in degraded performance. Overall, all components are essential for general reasoning.\n\nproposer-role training (row 4) result in degraded performance. Overall, all components are essential for general reasoning.\n\nimprove performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\n\nimprove performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\n\nimprove performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\n\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the\nsolver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it\nmay not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as\nstudied in multitask learning literature (Suteu & Guo, 2019). Thus, we believe that further investigation into how to make the proposer\neven more potent is an exciting and promising direction.\n\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the\n\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the\n\nsolver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it\n\nsolver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it\n\nmay not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as\n\nmay not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as\n\nstudied in multitask learning literature (Suteu & Guo, 2019). Thus, we believe that further investigation into how to make the proposer\n\nstudied in multitask learning literature\n\n(Suteu & Guo,\n\n. Thus, we believe that further investigation into how to make the proposer\n\n2019)\n\neven more potent is an exciting and promising direction.\n\neven more potent is an exciting and promising direction.\n\nAdditional Results. Beyond the core research questions, we present additional results, including the breakdown of individual\nout-of-distribution benchmark scores during training for the 7B base and coder models in Figures 28 and 29, for th 14B base and coder\nmodel in Figures 30 and 31. For completeness, we also report in-distribution benchmark performance during training for the 7B base\nmodel in Figure 14. Finally, we invite interested readers to explore Appendix D, where we share several experimental directions that,\nwhile not yielding strong performance gains, produced interesting and insightful findings.\n\nAdditional Results. Beyond the core research questions, we present additional results, including the breakdown of individual\n\nBeyond the core research questions, we present additional results, including the breakdown of individual\n\nAdditional Results.\n\nout-of-distribution benchmark scores during training for the 7B base and coder models in Figures 28 and 29, for th 14B base and coder\n\nout-of-distribution benchmark scores during training for the 7B base and coder models in Figures\n\nfor th 14B base and coder\n\n28\n\nand\n\n29,\n\nmodel in Figures 30 and 31. For completeness, we also report in-distribution benchmark performance during training for the 7B base\n\nmodel in Figures\n\n30\n\nand\n\n31.\n\nFor completeness, we also report in-distribution benchmark performance during training for the 7B base\n\nmodel in Figure 14. Finally, we invite interested readers to explore Appendix D, where we share several experimental directions that,\n\nmodel in Figure\n\n14.\n\nFinally, we invite interested readers to explore Appendix\n\nD,\n\nwhere we share several experimental directions that,\n\nwhile not yielding strong performance gains, produced interesting and insightful findings.\n\nwhile not yielding strong performance gains, produced interesting and insightful findings.\n\n5. Related Work\n\n5. Related Work\n\n5. Related Work\n\nReasoning with RL. Using RL to enhance reasoning capabilities has recently emerged as an important step in the post-training\nprocess of strong reasoning-focused large language models (Lambert et al., 2024). One of the first works to explore a self-bootstrapping\napproach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to\niteratively improve the model's CoT. A monumental work, o1 (Jaech et al., 2024), was among the first to deploy this idea on a scale,\nachieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model (DeepSeek-AI et al., 2025)\nbecame the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in\nwhich reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to\nreplicate the R1 process or to improve the underlying reinforcement learning algorithm (Zeng et al., 2025b; Liu et al., 2025; Cui et al.,\n2025; Hu et al., 2025; Yu et al., 2025; Yuan et al., 2025). Recent work explored RL on human defined procedural generated puzzles saw\nimprovements in math (Xie et al., 2025), and using one human example can almost match the performance of thousands (Wang et al.,\n2025b). We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM\nwithout SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed,\nand refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.\n\nReasoning with RL. Using RL to enhance reasoning capabilities has recently emerged as an important step in the post-training\n\nUsing RL to enhance reasoning capabilities has recently emerged as an important step in the post-training\n\nReasoning with RL.\n\nprocess of strong reasoning-focused large language models (Lambert et al., 2024). One of the first works to explore a self-bootstrapping\n\nprocess of strong reasoning-focused large language models\n\n(Lambert et al.,\n\n. One of the first works to explore a self-bootstrapping\n\n2024)\n\napproach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to\n\napproach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to\n\niteratively improve the model's CoT. A monumental work, o1 (Jaech et al., 2024), was among the first to deploy this idea on a scale,\n\niteratively improve the model's CoT. A monumental work, o1\n\n(Jaech et al.,\n\n2024)\n\n, was among the first to deploy this idea on a scale,\n\nachieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model (DeepSeek-AI et al., 2025)\n\nachieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model\n\n(DeepSeek-AI et al.,\n\n2025)\n\nbecame the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in\n\nbecame the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in\n\nwhich reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to\n\nwhich reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to\n\nreplicate the R1 process or to improve the underlying reinforcement learning algorithm (Zeng et al., 2025b; Liu et al., 2025; Cui et al.,\n\nreplicate the R1 process or to improve the underlying reinforcement learning algorithm\n\n(Zeng et al.,\n\n2025b;\n\nLiu et al.,\n\n2025;\n\nCui et al.,\n\n2025; Hu et al., 2025; Yu et al., 2025; Yuan et al., 2025). Recent work explored RL on human defined procedural generated puzzles saw\n\nHu et al.,\n\nYu et al.,\n\nYuan et al.,\n\n. Recent work explored RL on human defined procedural generated puzzles saw\n\n2025;\n\n2025;\n\n2025;\n\n2025)\n\nimprovements in math (Xie et al., 2025), and using one human example can almost match the performance of thousands (Wang et al.,\n\nimprovements in math\n\n(Xie et al.,\n\n2025)\n\n, and using one human example can almost match the performance of thousands\n\n(Wang et al.,\n\n2025b). We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM\n\n2025b)\n\n. We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM\n\nwithout SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed,\n\nwithout SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed,\n\nand refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.\n\nand refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.\n\nSelf-play. The self-play paradigm can be traced back to early 2000s, where Schmidhuber (2003; 2011) (of course) explored a\ntwo-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically\nimproves both agents, enabling theoretically never-ending progress (Schaul, 2024). AlphaGo and AlphaZero (Silver et al., 2016; 2017)\nextend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of\nitself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance\nin the game of Go. Moreover, methods such as asymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), unsupervised\nenvironment design (Wang et al., 2019; Dennis et al., 2020), unsupervised reinforcement learning (Laskin et al., 2021; Zhao et al., 2022;\n2025b), and automatic goal generation (Florensa et al., 2018) all center around inventing new tasks for an agent to learn from\u2014typically\nwithout supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative\nadversarial networks (Goodfellow et al., 2020), also belong in this paradigm where a discriminator discriminate between real data and\ngenerated data, and the generated is trained to fool the discriminator.\n\nSelf-play. The self-play paradigm can be traced back to early 2000s, where Schmidhuber (2003; 2011) (of course) explored a\n\nThe self-play paradigm can be traced back to early 2000s, where\n\nSchmidhuber\n\n(2003;\n\n2011)\n\n(of course) explored a\n\nSelf-play.\n\ntwo-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically\n\ntwo-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically\n\nimproves both agents, enabling theoretically never-ending progress (Schaul, 2024). AlphaGo and AlphaZero (Silver et al., 2016; 2017)\n\nimproves both agents, enabling theoretically never-ending progress\n\n. AlphaGo and AlphaZero\n\n(Silver et al.,\n\n(Schaul,\n\n2024)\n\n2016;\n\n2017)\n\nextend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of\n\nextend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of\n\nitself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance\n\nitself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance\n\nin the game of Go. Moreover, methods such as asymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), unsupervised\n\nin the game of Go. Moreover, methods such as asymmetric self-play\n\n(Sukhbaatar et al.,\n\n2018;\n\nOpenAI et al.,\n\n2021)\n\n, unsupervised\n\nenvironment design (Wang et al., 2019; Dennis et al., 2020), unsupervised reinforcement learning (Laskin et al., 2021; Zhao et al., 2022;\n\nenvironment design\n\n(Wang et al.,\n\nDennis et al.,\n\n, unsupervised reinforcement learning\n\n(Laskin et al.,\n\nZhao et al.,\n\n2019;\n\n2020)\n\n2021;\n\n2022;\n\n2025b), and automatic goal generation (Florensa et al., 2018) all center around inventing new tasks for an agent to learn from\u2014typically\n\n, and automatic goal generation\n\n(Florensa et al.,\n\nall center around inventing new tasks for an agent to learn from\u2014typically\n\n2025b)\n\n2018)\n\nwithout supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative\n\nwithout supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative\n\nadversarial networks (Goodfellow et al., 2020), also belong in this paradigm where a discriminator discriminate between real data and\n\nadversarial networks\n\n(Goodfellow et al.,\n\n, also belong in this paradigm where a discriminator discriminate between real data and\n\n2020)\n\ngenerated data, and the generated is trained to fool the discriminator.\n\ngenerated data, and the generated is trained to fool the discriminator.\n\nMost recently, SPIN and Self-Rewarding Language Models (Chen et al., 2024; Yuan et al., 2024) use the same instance of the lanugage\nmodels themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for\nalignment. (Kirchner et al., 2024) uses Prover-Verifier Game for increasing legibility and eva (Ye et al., 2024) uses self-play for\nalignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks (Lambert et al., 2024). SPC (Chen et al.,\n\nMost recently, SPIN and Self-Rewarding Language Models (Chen et al., 2024; Yuan et al., 2024) use the same instance of the lanugage\n\nMost recently, SPIN and Self-Rewarding Language Models\n\n(Chen et al.,\n\nYuan et al.,\n\nuse the same instance of the lanugage\n\n2024;\n\n2024)\n\nmodels themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for\n\nmodels themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for\n\nalignment. (Kirchner et al., 2024) uses Prover-Verifier Game for increasing legibility and eva (Ye et al., 2024) uses self-play for\n\nalignment.\n\n(Kirchner et al.,\n\n2024)\n\nuses Prover-Verifier Game for increasing legibility and eva\n\n(Ye et al.,\n\n2024)\n\nuses self-play for\n\nalignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks (Lambert et al., 2024). SPC (Chen et al.,\n\nalignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks\n\n(Lambert et al.,\n\n2024)\n\n. SPC\n\n(Chen et al.,\n\n12\n\n12\n\n12\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n2025) used self-play to train on human-curated tasks to increase the critic capabilities and SPAG (Cheng et al., 2024) trained using\nself-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL (Xu et al., 2025; Zhang et al., 2025b;\nZuo et al., 2025)\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a\nfixed human defined learning task distribution. Finally, Minimo (Poesia et al., 2024) extends self-play to formal mathematics, where a\npair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play\nparadigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python\ninput/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.\n\n2025) used self-play to train on human-curated tasks to increase the critic capabilities and SPAG (Cheng et al., 2024) trained using\n\n2025)\n\nused self-play to train on human-curated tasks to increase the critic capabilities and SPAG\n\n(Cheng et al.,\n\n2024)\n\ntrained using\n\nself-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL (Xu et al., 2025; Zhang et al., 2025b;\n\nself-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL\n\n(Xu et al.,\n\n2025;\n\nZhang et al.,\n\n2025b;\n\nZuo et al., 2025)\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a\n\nZuo et al.,\n\n2025)\n\n\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a\n\nfixed human defined learning task distribution. Finally, Minimo (Poesia et al., 2024) extends self-play to formal mathematics, where a\n\nfixed human defined learning task distribution. Finally, Minimo\n\n(Poesia et al.,\n\nextends self-play to formal mathematics, where a\n\n2024)\n\npair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play\n\npair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play\n\nparadigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python\n\nparadigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python\n\ninput/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.\n\ninput/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.\n\nWeak-to-Strong Supervision. The concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite\nbeing weaker than the learner\u2014still provides useful guidance (Burns et al., 2024; Hinton et al., 2015; Christiano, 2018; 2019; Demski &\nGarrabrant, 2019; Leike & Sutskever, 2023; Hubinger et al., 2019). We consider a similar setting in which the learner may possess\nsuperhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach:\nguiding the learner's improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal.\nFurthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are\nentirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.\n\nWeak-to-Strong Supervision. The concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite\n\nThe concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite\n\nWeak-to-Strong Supervision.\n\nbeing weaker than the learner\u2014still provides useful guidance (Burns et al., 2024; Hinton et al., 2015; Christiano, 2018; 2019; Demski &\n\nbeing weaker than the learner\u2014still provides useful guidance\n\n(Burns et al.,\n\nHinton et al.,\n\nDemski &\n\n2024;\n\n2015;\n\nChristiano,\n\n2018;\n\n2019;\n\nGarrabrant, 2019; Leike & Sutskever, 2023; Hubinger et al., 2019). We consider a similar setting in which the learner may possess\n\nGarrabrant,\n\n2019;\n\nLeike & Sutskever,\n\n2023;\n\nHubinger et al.,\n\n2019)\n\n. We consider a similar setting in which the learner may possess\n\nsuperhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach:\n\nsuperhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach:\n\nguiding the learner's improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal.\n\nguiding the learner's improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal.\n\nFurthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are\n\nFurthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are\n\nentirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.\n\nentirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.\n\n6. Conclusion and Discussion\n\n6. Conclusion and Discussion\n\n6. Conclusion and Discussion\n\nConclusion. In this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing\nRLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving\ntheir reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR),\nwhich is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\n\nConclusion. In this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing\n\nIn this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing\n\nConclusion.\n\nRLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving\n\nRLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving\n\ntheir reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR),\n\ntheir reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR),\n\nwhich is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\n\nwhich is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\n\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains.\nRemarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning\nagents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This\ndemonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive\ndomain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model\nsizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging\nparadigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.\n\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains.\n\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains.\n\nRemarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning\n\nRemarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning\n\nagents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This\n\nagents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This\n\ndemonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive\n\ndemonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive\n\ndomain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model\n\ndomain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model\n\nsizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging\n\nsizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging\n\nparadigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.\n\nparadigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.\n\nDiscussion. We believe there remains much to explore, such as altering the environment from which the reasoner receives verifiable\nfeedback, including sources like the world wide web, formal math languages (Sutton, 2001; Ren et al., 2025), world simulators, or even\nthe real world. Furthermore, AZ's generality could possibly be extend to domains such as embodied AI (Zitkovich et al., 2023; Yue\net al., 2024). Additionally, more complex agentic tasks or scientific experiments, present exciting opportunities to further advance the\nabsolute zero setting to different application domains (Wu et al., 2024; 2023). Beyond that, future directions could include exploring\nmultimodal reasoning models, modifying the distribution p(z) to incorporate privileged information, defining or even let the model\ndynamically learn how to define f (Equation (3)), or designing exploration/diversity rewards for both the propose and solve roles.\n\nDiscussion. We believe there remains much to explore, such as altering the environment from which the reasoner receives verifiable\n\nWe believe there remains much to explore, such as altering the environment from which the reasoner receives verifiable\n\nDiscussion.\n\nfeedback, including sources like the world wide web, formal math languages (Sutton, 2001; Ren et al., 2025), world simulators, or even\n\nfeedback, including sources like the world wide web, formal math languages\n\nRen et al.,\n\n, world simulators, or even\n\n(Sutton,\n\n2001;\n\n2025)\n\nthe real world. Furthermore, AZ's generality could possibly be extend to domains such as embodied AI (Zitkovich et al., 2023; Yue\n\nthe real world. Furthermore, AZ's generality could possibly be extend to domains such as embodied AI\n\n(Zitkovich et al.,\n\n2023;\n\nYue\n\net al., 2024). Additionally, more complex agentic tasks or scientific experiments, present exciting opportunities to further advance the\n\net al.,\n\n2024)\n\n. Additionally, more complex agentic tasks or scientific experiments, present exciting opportunities to further advance the\n\nabsolute zero setting to different application domains (Wu et al., 2024; 2023). Beyond that, future directions could include exploring\n\nabsolute zero setting to different application domains\n\n(Wu et al.,\n\n2024;\n\n2023)\n\n. Beyond that, future directions could include exploring\n\nmultimodal reasoning models, modifying the distribution p(z) to incorporate privileged information, defining or even let the model\n\nmultimodal reasoning models, modifying the distribution\n\nto incorporate privileged information, defining or even let the model\n\n(\n\n)\n\np\n\nz\n\ndynamically learn how to define f (Equation (3)), or designing exploration/diversity rewards for both the propose and solve roles.\n\ndynamically learn how to define\n\n(Equation\n\n(3)\n\n), or designing exploration/diversity rewards for both the propose and solve roles.\n\nf\n\nWhile underappreciated in current reasoning literature, the exploration component of RL has long been recognized as a critical driver for\nemergent behavior in traditional RL (Yue et al., 2025; Silver et al., 2016; Ladosz et al., 2022). Years of research have examined various\nforms of exploration, even in related subfields using LLMs such as red teaming (Zhao et al., 2025a), yet its role in LLM reasoning\nmodels remains underexplored. Taking this a step further, our framework investigates an even more meta-level exploration problem:\nexploration within the learning task space\u2014where the agent learns not just how to solve tasks, but what tasks to learn from and how to\nfind them. Rather than being confined to a fixed problem set, AI reasoner agents may benefit from dynamically defining and refining\ntheir own learning tasks. This shift opens a powerful new frontier\u2014where agents explore not only solution spaces but also expand the\nboundaries of problem spaces. We believe this is a promising and important direction for future research.\n\nWhile underappreciated in current reasoning literature, the exploration component of RL has long been recognized as a critical driver for\n\nWhile underappreciated in current reasoning literature, the exploration component of RL has long been recognized as a critical driver for\n\nemergent behavior in traditional RL (Yue et al., 2025; Silver et al., 2016; Ladosz et al., 2022). Years of research have examined various\n\nemergent behavior in traditional RL\n\n(Yue et al.,\n\nSilver et al.,\n\nLadosz et al.,\n\n. Years of research have examined various\n\n2025;\n\n2016;\n\n2022)\n\nforms of exploration, even in related subfields using LLMs such as red teaming (Zhao et al., 2025a), yet its role in LLM reasoning\n\nforms of exploration, even in related subfields using LLMs such as red teaming\n\n(Zhao et al.,\n\n2025a)\n\n, yet its role in LLM reasoning\n\nmodels remains underexplored. Taking this a step further, our framework investigates an even more meta-level exploration problem:\n\nmodels remains underexplored. Taking this a step further, our framework investigates an even more meta-level exploration problem:\n\nexploration within the learning task space\u2014where the agent learns not just how to solve tasks, but what tasks to learn from and how to\n\nexploration within the learning task space\u2014where the agent learns not just how to solve tasks, but what tasks to learn from and how to\n\nfind them. Rather than being confined to a fixed problem set, AI reasoner agents may benefit from dynamically defining and refining\n\nfind them. Rather than being confined to a fixed problem set, AI reasoner agents may benefit from dynamically defining and refining\n\ntheir own learning tasks. This shift opens a powerful new frontier\u2014where agents explore not only solution spaces but also expand the\n\ntheir own learning tasks. This shift opens a powerful new frontier\u2014where agents explore not only solution spaces but also expand the\n\nboundaries of problem spaces. We believe this is a promising and important direction for future research.\n\nboundaries of problem spaces. We believe this is a promising and important direction for future research.\n\nOne limitation of our work is that we did not address how to safely manage a system composed of such self-improving components.\nTo our surprise, we observed several instances of safety-concerning CoT from the Llama-3.1-8B model, which we term the \"uh-oh\nmoment\". These findings suggest that the proposed absolute zero paradigm, while reducing the need for human intervention for curating\ntasks, still necessitates oversight due to lingering safety concerns and is a critical direction for future research (Wang et al., 2024; 2025a).\n\nOne limitation of our work is that we did not address how to safely manage a system composed of such self-improving components.\n\nOne limitation of our work is that we did not address how to safely manage a system composed of such self-improving components.\n\nTo our surprise, we observed several instances of safety-concerning CoT from the Llama-3.1-8B model, which we term the \"uh-oh\n\nTo our surprise, we observed several instances of safety-concerning CoT from the\n\nmodel, which we term the \"uh-oh\n\nLlama-3.1-8B\n\nmoment\". These findings suggest that the proposed absolute zero paradigm, while reducing the need for human intervention for curating\n\nmoment\". These findings suggest that the proposed absolute zero paradigm, while reducing the need for human intervention for curating\n\ntasks, still necessitates oversight due to lingering safety concerns and is a critical direction for future research (Wang et al., 2024; 2025a).\n\ntasks, still necessitates oversight due to lingering safety concerns and is a critical direction for future research\n\n(Wang et al.,\n\n2024;\n\n2025a)\n\n.\n\nAs a final note, we explored reasoning models that possess experience\u2014models that not only solve given tasks, but also define and\nevolve their own learning task distributions with the help of an environment. Our results with AZR show that this shift enables strong\nperformance across diverse reasoning tasks, even with significantly fewer privileged resources, such as curated human data. We believe\nthis could finally free reasoning models from the constraints of human-curated data (Morris, 2025) and marks the beginning of a new\nchapter for reasoning models: \"welcome to the era of experience\" (Silver & Sutton, 2025; Zhao et al., 2024).\n\nAs a final note, we explored reasoning models that possess experience\u2014models that not only solve given tasks, but also define and\n\nAs a final note, we explored reasoning models that possess experience\u2014models that not only solve given tasks, but also define and\n\nevolve their own learning task distributions with the help of an environment. Our results with AZR show that this shift enables strong\n\nevolve their own learning task distributions with the help of an environment. Our results with AZR show that this shift enables strong\n\nperformance across diverse reasoning tasks, even with significantly fewer privileged resources, such as curated human data. We believe\n\nperformance across diverse reasoning tasks, even with significantly fewer privileged resources, such as curated human data. We believe\n\nthis could finally free reasoning models from the constraints of human-curated data (Morris, 2025) and marks the beginning of a new\n\nthis could finally free reasoning models from the constraints of human-curated data\n\n(Morris,\n\n2025)\n\nand marks the beginning of a new\n\nchapter for reasoning models: \"welcome to the era of experience\" (Silver & Sutton, 2025; Zhao et al., 2024).\n\nchapter for reasoning models:\n\n(Silver & Sutton,\n\n2025;\n\nZhao et al.,\n\n2024)\n\n.\n\n\"welcome to the era of experience\"\n\n13\n\n13\n\n13\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nReferences\n\nReferences\n\nReferences\n\nAryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\nto code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\nhttps://doi.org/10.48550/arXiv.2408.10914.\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\nhttps://openreview.net/forum?id=ghNRg2mEgN.\nCanal, M. Radon: Python tool for code metrics. https://github.com/rubik/radon, 2023. Accessed: 2025-04-06.\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\nfor llm reasoning, 2025. URL https://arxiv.org/abs/2504.19162.\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\nURL https://openreview.net/forum?id=O4cHTxW9BS.\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html.\nChristiano, P. Approval-directed bootstrapping. https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\napproval-directed-bootstrapping, 2018. AI Alignment Forum.\nChristiano, P. Capability amplification. https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\ncapability-amplification-1, 2019. AI Alignment Forum.\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\nCoRR, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL https://doi.org/10.48550/arXiv.2502.01456.\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.\nDemski, A. and Garrabrant, S. Embedded agency. CoRR, abs/1902.09469, 2019. URL http://arxiv.org/abs/1902.09469.\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n985e9a46e10005356bbaf194249f6856-Abstract.html.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,\n\nAryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\nto code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\nhttps://doi.org/10.48550/arXiv.2408.10914.\n\nAryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\n\nAryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\n\nto code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\n\nto code? exploring impact of code in pre-training.\n\n, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2408.10914.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2408.10914\n\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\nhttps://openreview.net/forum?id=ghNRg2mEgN.\n\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\n\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\n\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first\n\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In\n\nForty-first\n\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\n\n. OpenReview.net, 2024. URL\n\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024\n\nhttps://openreview.net/forum?id=ghNRg2mEgN.\n\n.\n\nhttps://openreview.net/forum?id=ghNRg2mEgN\n\nCanal, M. Radon: Python tool for code metrics. https://github.com/rubik/radon, 2023. Accessed: 2025-04-06.\n\nCanal, M. Radon: Python tool for code metrics. https://github.com/rubik/radon, 2023. Accessed: 2025-04-06.\n\nCanal, M. Radon: Python tool for code metrics.\n\n, 2023. Accessed: 2025-04-06.\n\nhttps://github.com/rubik/radon\n\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\nfor llm reasoning, 2025. URL https://arxiv.org/abs/2504.19162.\n\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\n\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\n\nfor llm reasoning, 2025. URL https://arxiv.org/abs/2504.19162.\n\nfor llm reasoning, 2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.19162\n\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\nURL https://openreview.net/forum?id=O4cHTxW9BS.\n\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\n\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\n\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\n\n. OpenReview.net, 2024.\n\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024\n\nURL https://openreview.net/forum?id=O4cHTxW9BS.\n\nURL\n\n.\n\nhttps://openreview.net/forum?id=O4cHTxW9BS\n\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html.\n\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\n\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\n\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in\n\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.),\n\nAdvances in\n\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\n\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\n\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n\n, 2024. URL\n\nVancouver, BC, Canada, December 10 - 15, 2024\n\nhttp://papers.nips.cc/paper_files/paper/2024/hash/\n\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html.\n\n.\n\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html\n\nChristiano, P. Approval-directed bootstrapping. https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\napproval-directed-bootstrapping, 2018. AI Alignment Forum.\n\nChristiano, P. Approval-directed bootstrapping. https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\n\nChristiano, P. Approval-directed bootstrapping.\n\nhttps://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\n\napproval-directed-bootstrapping, 2018. AI Alignment Forum.\n\n, 2018. AI Alignment Forum.\n\napproval-directed-bootstrapping\n\nChristiano, P. Capability amplification. https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\ncapability-amplification-1, 2019. AI Alignment Forum.\n\nChristiano, P. Capability amplification. https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\n\nChristiano, P. Capability amplification.\n\nhttps://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\n\ncapability-amplification-1, 2019. AI Alignment Forum.\n\n, 2019. AI Alignment Forum.\n\ncapability-amplification-1\n\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\nCoRR, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL https://doi.org/10.48550/arXiv.2502.01456.\n\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\n\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\n\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\n\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\n\nCoRR, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL https://doi.org/10.48550/arXiv.2502.01456.\n\n, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2502.01456\n\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.\n\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\n\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\n\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\n\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\n\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\n\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\n\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\n\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\n\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\n\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\n\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\n\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\n\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\n\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\n\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\n\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\n\n, abs/2501.12948, 2025. doi:\n\nCoRR\n\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.\n\n10.48550/ARXIV.2501.12948. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2501.12948\n\nDemski, A. and Garrabrant, S. Embedded agency. CoRR, abs/1902.09469, 2019. URL http://arxiv.org/abs/1902.09469.\n\nDemski, A. and Garrabrant, S. Embedded agency. CoRR, abs/1902.09469, 2019. URL http://arxiv.org/abs/1902.09469.\n\nDemski, A. and Garrabrant, S. Embedded agency.\n\n, abs/1902.09469, 2019. URL\n\n.\n\nCoRR\n\nhttp://arxiv.org/abs/1902.09469\n\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n985e9a46e10005356bbaf194249f6856-Abstract.html.\n\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\n\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\n\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n\n(eds.),\n\nAdvances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n\n, 2020. URL\n\n2020, NeurIPS 2020, December 6-12, 2020, virtual\n\nhttps://proceedings.neurips.cc/paper/2020/hash/\n\n985e9a46e10005356bbaf194249f6856-Abstract.html.\n\n.\n\n985e9a46e10005356bbaf194249f6856-Abstract.html\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\n\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\n\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\n\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\n\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\n\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\n\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\n\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\n\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\n\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\n\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\n\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\n\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\n\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\n\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\n\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,\n\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,\n\n14\n\n14\n\n14\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nK., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL\nhttps://doi.org/10.48550/arXiv.2407.21783.\n\nK., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL\n\nK., Stone, K., and et al. The llama 3 herd of models.\n\n, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2407.21783.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2407.21783\n\nEbert, C., Cain, J., Antoniol, G., Counsell, S., and Laplante, P. Cyclomatic complexity. IEEE software, 33(6):27\u201329, 2016.\n\nEbert, C., Cain, J., Antoniol, G., Counsell, S., and Laplante, P. Cyclomatic complexity. IEEE software, 33(6):27\u201329, 2016.\n\nEbert, C., Cain, J., Antoniol, G., Counsell, S., and Laplante, P. Cyclomatic complexity.\n\n, 33(6):27\u201329, 2016.\n\nIEEE software\n\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\nKrause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\nStockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1514\u20131523. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/florensa18a.html.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\nadversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\nunderstanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg.\n\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\nKrause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\nStockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1514\u20131523. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/florensa18a.html.\n\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\n\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\n\nKrause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\n\nKrause, A. (eds.),\n\nProceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\n\nStockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1514\u20131523. PMLR, 2018. URL\n\n, volume 80 of\n\n, pp. 1514\u20131523. PMLR, 2018. URL\n\nStockholm, Sweden, July 10-15, 2018\n\nProceedings of Machine Learning Research\n\nhttp://proceedings.mlr.press/v80/florensa18a.html.\n\n.\n\nhttp://proceedings.mlr.press/v80/florensa18a.html\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\nadversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\n\nadversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.\n\nadversarial networks.\n\n, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL\n\n.\n\nCommun. ACM\n\nhttps://doi.org/10.1145/3422622\n\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\nunderstanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg.\n\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\n\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\n\nunderstanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n\nunderstanding and execution. In\n\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n\n2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg.\n\n. OpenReview.net, 2024. URL\n\n.\n\n2024\n\nhttps://openreview.net/forum?id=Ffpg52swvg\n\nHalstead, M. H. Elements of Software Science (Operating and programming systems series). Elsevier Science Inc., 1977.\n\nHalstead, M. H. Elements of Software Science (Operating and programming systems series). Elsevier Science Inc., 1977.\n\nHalstead, M. H.\n\n. Elsevier Science Inc., 1977.\n\nElements of Software Science (Operating and programming systems series)\n\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\nL., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 3828\u20133850. Association for Computational\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neu-\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL\nhttp://arxiv.org/abs/1503.02531.\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi:\n10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\nreinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https:\n//doi.org/10.48550/arXiv.2503.24290.\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\nsystems. CoRR, abs/1906.01820, 2019. URL http://arxiv.org/abs/1906.01820.\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\nOpen-endedness is essential for artificial superhuman intelligence. In Forty-first International Conference on Machine Learning,\nICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Bc4vZ2CX7E.\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\nhttps://doi.org/10.48550/arXiv.2409.12186.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\nsystem card. arXiv preprint arXiv:2412.16720, 2024.\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\ncontamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\nURL https://doi.org/10.48550/arXiv.2403.07974.\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\nCoRR, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL https://doi.org/10.48550/arXiv.2407.13692.\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85:1\u201322, 2022. doi:\n10.1016/J.INFFUS.2022.03.003. URL https://doi.org/10.1016/j.inffus.2022.03.003.\n\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\nL., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 3828\u20133850. Association for Computational\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211.\n\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\n\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\n\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\n\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\n\nL., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n\nL., Martins, A., and Srikumar, V. (eds.),\n\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 3828\u20133850. Association for Computational\n\n, pp. 3828\u20133850. Association for Computational\n\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024\n\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211.\n\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL\n\n.\n\nhttps://doi.org/10.18653/v1/2024.acl-long.211\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neu-\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\n\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neu-\n\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.),\n\nProceedings of the Neu-\n\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\n\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\n\nDecember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n\n, 2021. URL\n\nDecember 2021, virtual\n\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\n\n.\n\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html\n\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL\nhttp://arxiv.org/abs/1503.02531.\n\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL\n\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network.\n\n, abs/1503.02531, 2015. URL\n\nCoRR\n\nhttp://arxiv.org/abs/1503.02531.\n\n.\n\nhttp://arxiv.org/abs/1503.02531\n\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi:\n10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.\n\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi:\n\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models.\n\n, abs/2501.03262, 2025. doi:\n\nCoRR\n\n10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.\n\n10.48550/ARXIV.2501.03262. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2501.03262\n\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\nreinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https:\n//doi.org/10.48550/arXiv.2503.24290.\n\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\n\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\n\nreinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https:\n\nreinforcement learning on the base model.\n\n, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL\n\nCoRR\n\nhttps:\n\n//doi.org/10.48550/arXiv.2503.24290.\n\n.\n\n//doi.org/10.48550/arXiv.2503.24290\n\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\nsystems. CoRR, abs/1906.01820, 2019. URL http://arxiv.org/abs/1906.01820.\n\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\n\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\n\nsystems. CoRR, abs/1906.01820, 2019. URL http://arxiv.org/abs/1906.01820.\n\nsystems.\n\n, abs/1906.01820, 2019. URL\n\n.\n\nCoRR\n\nhttp://arxiv.org/abs/1906.01820\n\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\nOpen-endedness is essential for artificial superhuman intelligence. In Forty-first International Conference on Machine Learning,\nICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Bc4vZ2CX7E.\n\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\n\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\n\nOpen-endedness is essential for artificial superhuman intelligence. In Forty-first International Conference on Machine Learning,\n\nOpen-endedness is essential for artificial superhuman intelligence. In\n\nForty-first International Conference on Machine Learning,\n\nICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Bc4vZ2CX7E.\n\n. OpenReview.net, 2024. URL\n\n.\n\nICML 2024, Vienna, Austria, July 21-27, 2024\n\nhttps://openreview.net/forum?id=Bc4vZ2CX7E\n\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\nhttps://doi.org/10.48550/arXiv.2409.12186.\n\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\n\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\n\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\n\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report.\n\n, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2409.12186.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2409.12186\n\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\nsystem card. arXiv preprint arXiv:2412.16720, 2024.\n\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\n\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\n\nsystem card. arXiv preprint arXiv:2412.16720, 2024.\n\nsystem card.\n\n, 2024.\n\narXiv preprint arXiv:2412.16720\n\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\ncontamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\nURL https://doi.org/10.48550/arXiv.2403.07974.\n\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\n\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\n\ncontamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\n\ncontamination free evaluation of large language models for code.\n\n, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\n\nCoRR\n\nURL https://doi.org/10.48550/arXiv.2403.07974.\n\nURL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2403.07974\n\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\nCoRR, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL https://doi.org/10.48550/arXiv.2407.13692.\n\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\n\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\n\nCoRR, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL https://doi.org/10.48550/arXiv.2407.13692.\n\n, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2407.13692\n\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85:1\u201322, 2022. doi:\n10.1016/J.INFFUS.2022.03.003. URL https://doi.org/10.1016/j.inffus.2022.03.003.\n\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85:1\u201322, 2022. doi:\n\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey.\n\n, 85:1\u201322, 2022. doi:\n\nInf. Fusion\n\n10.1016/J.INFFUS.2022.03.003. URL https://doi.org/10.1016/j.inffus.2022.03.003.\n\n10.1016/J.INFFUS.2022.03.003. URL\n\n.\n\nhttps://doi.org/10.1016/j.inffus.2022.03.003\n\n15\n\n15\n\n15\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\nH. T\u00fclu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\nURL https://doi.org/10.48550/arXiv.2411.15124.\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural In-\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\ncember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html.\nLeike, J. and Sutskever, I. Introducing superalignment. https://openai.com/index/introducing-superalignment/, 2023.\nOpenAI Blog.\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction. CoRR,\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL https://doi.org/10.48550/arXiv.2502.07316.\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\nCoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852.\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. GitHub, 2025.\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\nlanguage models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=1qvx610Cu7.\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\nCoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783.\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n2025. URL https://github.com/rohaquinlop/complexipy. Accessed: 2025-04-06.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\nMorris, J. There are no new ideas in ai. . . only new datasets. https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only,\n2025.\nOpenAI. Openai o3-mini, January 2025a. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-04-17.\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/.\nAccessed: 2025-04-17.\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\nmanipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\nlanguage models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744,\n2022.\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural In-\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\ncouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI\nblog, 1(8):9, 2019.\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\nfor subgoal decomposition, 2025. URL https://arxiv.org/abs/2504.21801.\nSchaul, T. Boundless socratic learning with language games. arXiv preprint arXiv:2411.16905, 2024.\n\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\nH. T\u00fclu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\nURL https://doi.org/10.48550/arXiv.2411.15124.\n\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\n\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\n\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\n\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\n\nH. T\u00fclu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\n\nH. T\u00fclu 3: Pushing frontiers in open language model post-training.\n\n, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\n\nCoRR\n\nURL https://doi.org/10.48550/arXiv.2411.15124.\n\nURL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2411.15124\n\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural In-\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\ncember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html.\n\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\n\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\n\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural In-\n\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.),\n\nProceedings of the Neural In-\n\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\n\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\n\ncember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n\n, 2021. URL\n\ncember 2021, virtual\n\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html.\n\n.\n\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html\n\nLeike, J. and Sutskever, I. Introducing superalignment. https://openai.com/index/introducing-superalignment/, 2023.\nOpenAI Blog.\n\nLeike, J. and Sutskever, I. Introducing superalignment. https://openai.com/index/introducing-superalignment/, 2023.\n\nLeike, J. and Sutskever, I. Introducing superalignment.\n\n, 2023.\n\nhttps://openai.com/index/introducing-superalignment/\n\nOpenAI Blog.\n\nOpenAI Blog.\n\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction. CoRR,\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL https://doi.org/10.48550/arXiv.2502.07316.\n\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction. CoRR,\n\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction.\n\n,\n\nCoRR\n\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL https://doi.org/10.48550/arXiv.2502.07316.\n\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2502.07316\n\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\nCoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852.\n\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\n\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\n\nCoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852.\n\n, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2312.14852\n\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. GitHub, 2025.\n\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. GitHub, 2025.\n\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards.\n\n, 2025.\n\nGitHub\n\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\nlanguage models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=1qvx610Cu7.\n\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\n\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\n\nlanguage models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\n\nlanguage models for code generation. In\n\n, 2023. URL\n\nThirty-seventh Conference on Neural Information Processing Systems\n\nhttps://openreview.net/forum?id=1qvx610Cu7.\n\n.\n\nhttps://openreview.net/forum?id=1qvx610Cu7\n\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\nCoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783.\n\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\n\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\n\nCoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783.\n\n, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2503.20783\n\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n2025. URL https://github.com/rohaquinlop/complexipy. Accessed: 2025-04-06.\n\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n\n2025. URL https://github.com/rohaquinlop/complexipy. Accessed: 2025-04-06.\n\n2025. URL\n\n. Accessed: 2025-04-06.\n\nhttps://github.com/rohaquinlop/complexipy\n\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\n\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR\n\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In\n\n7th International Conference on Learning Representations, ICLR\n\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\n\n. OpenReview.net, 2019. URL\n\n.\n\n2019, New Orleans, LA, USA, May 6-9, 2019\n\nhttps://openreview.net/forum?id=Bkg6RiCqY7\n\nMorris, J. There are no new ideas in ai. . . only new datasets. https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only,\n2025.\n\nMorris, J. There are no new ideas in ai. . . only new datasets. https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only,\n\nMorris, J. There are no new ideas in ai. . . only new datasets.\n\n,\n\nhttps://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only\n\n2025.\n\n2025.\n\nOpenAI. Openai o3-mini, January 2025a. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-04-17.\n\nOpenAI. Openai o3-mini, January 2025a. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-04-17.\n\nOpenAI. Openai o3-mini, January 2025a. URL\n\n. Accessed: 2025-04-17.\n\nhttps://openai.com/index/openai-o3-mini/\n\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/.\nAccessed: 2025-04-17.\n\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/.\n\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL\n\n.\n\nhttps://openai.com/index/introducing-o3-and-o4-mini/\n\nAccessed: 2025-04-17.\n\nAccessed: 2025-04-17.\n\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\nmanipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\n\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\n\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\n\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\n\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\n\nmanipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\n\nmanipulation.\n\n, abs/2101.04882, 2021. URL\n\n.\n\nCoRR\n\nhttps://arxiv.org/abs/2101.04882\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\nlanguage models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744,\n2022.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\n\nlanguage models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744,\n\nlanguage models to follow instructions with human feedback.\n\n, 35:27730\u201327744,\n\nAdvances in neural information processing systems\n\n2022.\n\n2022.\n\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural In-\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\ncouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html.\n\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\n\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\n\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural In-\n\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.),\n\nAdvances in Neural In-\n\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\n\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\n\ncouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n\n, 2024. URL\n\ncouver, BC, Canada, December 10 - 15, 2024\n\nhttp://papers.nips.cc/paper_files/paper/2024/hash/\n\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html.\n\n.\n\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI\nblog, 1(8):9, 2019.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners.\n\nOpenAI\n\nblog, 1(8):9, 2019.\n\n, 1(8):9, 2019.\n\nblog\n\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\nfor subgoal decomposition, 2025. URL https://arxiv.org/abs/2504.21801.\n\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\n\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\n\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\n\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\n\nfor subgoal decomposition, 2025. URL https://arxiv.org/abs/2504.21801.\n\nfor subgoal decomposition, 2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.21801\n\nSchaul, T. Boundless socratic learning with language games. arXiv preprint arXiv:2411.16905, 2024.\n\nSchaul, T. Boundless socratic learning with language games. arXiv preprint arXiv:2411.16905, 2024.\n\nSchaul, T. Boundless socratic learning with language games.\n\n, 2024.\n\narXiv preprint arXiv:2411.16905\n\n16\n\n16\n\n16\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nSchmidhuber, J. Exploring the predictable. In Advances in evolutionary computing: theory and applications, pp. 579\u2013612. Springer,\n2003.\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\nproblem. CoRR, abs/1112.5309, 2011. URL http://arxiv.org/abs/1112.5309.\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\nof mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\nhttps://doi.org/10.48550/arXiv.2402.03300.\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\nRLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\nThe Netherlands, 30 March 2025 - 3 April 2025, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https:\n//doi.org/10.1145/3689031.3696075.\nSilver, D. and Sutton, R. S. The era of experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%\n20/The%20Era%20of%20Experience%20Paper.pdf, 2025.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484\u2013489, 2016.\ndoi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR,\nabs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\nStuart, T. Understanding computation - from simple machines to impossible programs. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\nhttp://www.oreilly.de/catalog/9781449329273/index.html.\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\nasymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients. CoRR, abs/1912.06844, 2019. URL\nhttp://arxiv.org/abs/1912.06844.\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\nConference session, December 2024. URL https://neurips.cc/virtual/2024/test-of-time/105032.\nSutton, R. S. Verification, the key to ai. http://incompleteideas.net/IncIdeas/KeytoAI.html, 2001.\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms. CoRR,\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\nbased on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ViZcgDQjyG.\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\nsimple parameter editing. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\nComputational Linguistics, pp. 6337\u20136357, 2025a.\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\nand diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019. URL http://arxiv.org/abs/1901.01753.\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\nbalances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nURL https://openreview.net/forum?id=vtoY8qJjTR.\n\nSchmidhuber, J. Exploring the predictable. In Advances in evolutionary computing: theory and applications, pp. 579\u2013612. Springer,\n2003.\n\nSchmidhuber, J. Exploring the predictable. In Advances in evolutionary computing: theory and applications, pp. 579\u2013612. Springer,\n\nSchmidhuber, J. Exploring the predictable. In\n\n, pp. 579\u2013612. Springer,\n\nAdvances in evolutionary computing: theory and applications\n\n2003.\n\n2003.\n\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\nproblem. CoRR, abs/1112.5309, 2011. URL http://arxiv.org/abs/1112.5309.\n\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\n\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\n\nproblem. CoRR, abs/1112.5309, 2011. URL http://arxiv.org/abs/1112.5309.\n\nproblem.\n\n, abs/1112.5309, 2011. URL\n\n.\n\nCoRR\n\nhttp://arxiv.org/abs/1112.5309\n\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\nof mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\nhttps://doi.org/10.48550/arXiv.2402.03300.\n\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\n\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\n\nof mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\n\nof mathematical reasoning in open language models.\n\n, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2402.03300.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2402.03300\n\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\nRLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\nThe Netherlands, 30 March 2025 - 3 April 2025, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https:\n//doi.org/10.1145/3689031.3696075.\n\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\n\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\n\nRLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\n\nRLHF framework. In\n\nProceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\n\nThe Netherlands, 30 March 2025 - 3 April 2025, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https:\n\n, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL\n\nThe Netherlands, 30 March 2025 - 3 April 2025\n\nhttps:\n\n//doi.org/10.1145/3689031.3696075.\n\n.\n\n//doi.org/10.1145/3689031.3696075\n\nSilver, D. and Sutton, R. S. The era of experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%\n20/The%20Era%20of%20Experience%20Paper.pdf, 2025.\n\nSilver, D. and Sutton, R. S. The era of experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%\n\nSilver, D. and Sutton, R. S. The era of experience.\n\nhttps://storage.googleapis.com/deepmind-media/Era-of-Experience%\n\n20/The%20Era%20of%20Experience%20Paper.pdf, 2025.\n\n, 2025.\n\n20/The%20Era%20of%20Experience%20Paper.pdf\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484\u2013489, 2016.\ndoi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961.\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\n\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\n\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\n\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484\u2013489, 2016.\n\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search.\n\n, 529(7587):484\u2013489, 2016.\n\nNat.\n\ndoi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961.\n\ndoi: 10.1038/NATURE16961. URL\n\n.\n\nhttps://doi.org/10.1038/nature16961\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR,\nabs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\n\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR,\n\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm.\n\n,\n\nCoRR\n\nabs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\n\nabs/1712.01815, 2017. URL\n\n.\n\nhttp://arxiv.org/abs/1712.01815\n\nStuart, T. Understanding computation - from simple machines to impossible programs. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\nhttp://www.oreilly.de/catalog/9781449329273/index.html.\n\nStuart, T. Understanding computation - from simple machines to impossible programs. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\n\nStuart, T.\n\n. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\n\nUnderstanding computation - from simple machines to impossible programs\n\nhttp://www.oreilly.de/catalog/9781449329273/index.html.\n\n.\n\nhttp://www.oreilly.de/catalog/9781449329273/index.html\n\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\nasymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.\n\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\n\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\n\nasymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\n\nasymmetric self-play. In\n\n6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\n\nMay 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.\n\n. OpenReview.net, 2018. URL\n\n.\n\nMay 3, 2018, Conference Track Proceedings\n\nhttps://openreview.net/forum?id=SkT5Yg-RZ\n\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients. CoRR, abs/1912.06844, 2019. URL\nhttp://arxiv.org/abs/1912.06844.\n\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients. CoRR, abs/1912.06844, 2019. URL\n\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients.\n\n, abs/1912.06844, 2019. URL\n\nCoRR\n\nhttp://arxiv.org/abs/1912.06844.\n\n.\n\nhttp://arxiv.org/abs/1912.06844\n\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\nConference session, December 2024. URL https://neurips.cc/virtual/2024/test-of-time/105032.\n\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\n\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\n\nConference session, December 2024. URL https://neurips.cc/virtual/2024/test-of-time/105032.\n\nConference session, December 2024. URL\n\n.\n\nhttps://neurips.cc/virtual/2024/test-of-time/105032\n\nSutton, R. S. Verification, the key to ai. http://incompleteideas.net/IncIdeas/KeytoAI.html, 2001.\n\nSutton, R. S. Verification, the key to ai. http://incompleteideas.net/IncIdeas/KeytoAI.html, 2001.\n\nSutton, R. S. Verification, the key to ai.\n\n, 2001.\n\nhttp://incompleteideas.net/IncIdeas/KeytoAI.html\n\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms. CoRR,\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.\n\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\n\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\n\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\n\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\n\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\n\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\n\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\n\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\n\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\n\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\n\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\n\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\n\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms. CoRR,\n\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms.\n\n,\n\nCoRR\n\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.\n\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2501.12599\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\nbased on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ViZcgDQjyG.\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\n\nbased on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n\nbased on human-generated data. In\n\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n\n21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ViZcgDQjyG.\n\n. OpenReview.net, 2024. URL\n\n.\n\n21-27, 2024\n\nhttps://openreview.net/forum?id=ViZcgDQjyG\n\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\nsimple parameter editing. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\nComputational Linguistics, pp. 6337\u20136357, 2025a.\n\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\n\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\n\nsimple parameter editing. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\n\nsimple parameter editing. In\n\nProceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\n\nComputational Linguistics, pp. 6337\u20136357, 2025a.\n\n, pp. 6337\u20136357, 2025a.\n\nComputational Linguistics\n\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\nand diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019. URL http://arxiv.org/abs/1901.01753.\n\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\n\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\n\nand diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019. URL http://arxiv.org/abs/1901.01753.\n\nand diverse learning environments and their solutions.\n\n, abs/1901.01753, 2019. URL\n\n.\n\nCoRR\n\nhttp://arxiv.org/abs/1901.01753\n\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\nbalances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nURL https://openreview.net/forum?id=vtoY8qJjTR.\n\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\n\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\n\nbalances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n\nbalances for offline-to-online reinforcement learning. In\n\n, 2023.\n\nThirty-seventh Conference on Neural Information Processing Systems\n\nURL https://openreview.net/forum?id=vtoY8qJjTR.\n\nURL\n\n.\n\nhttps://openreview.net/forum?id=vtoY8qJjTR\n\n17\n\n17\n\n17\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nWang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for\nComputational Linguistics: ACL 2024, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.findings-acl.591. URL https://aclanthology.org/2024.findings-acl.591/.\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\nhttps://arxiv.org/abs/2504.20571.\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\nLLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\nhttps://doi.org/10.48550/arXiv.2308.08155.\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows. CoRR,\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL https://doi.org/10.48550/arXiv.2403.11322.\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\nreasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\nhttps://doi.org/10.48550/arXiv.2502.14768.\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\nself-training framework for advanced reasoning, 2025. URL https://arxiv.org/abs/2504.08672.\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\nCoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.\ndoi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. CoRR,\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL https://doi.org/10.48550/arXiv.2411.00062.\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\nCoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476.\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. URL https://arxiv.\norg/abs/2401.10020, 2024.\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\nlearning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\nAdvances in Neural Information Processing Systems, 36:60247\u201360277, 2023.\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\nZhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html.\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\ncapacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837.\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information\nProcessing Systems, 35:15476\u201315488, 2022.\n\nWang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for\nComputational Linguistics: ACL 2024, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.findings-acl.591. URL https://aclanthology.org/2024.findings-acl.591/.\n\nWang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\n\nWang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\n\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for\n\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.),\n\nFindings of the Association for\n\nComputational Linguistics: ACL 2024, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n\n, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n\nComputational Linguistics: ACL 2024\n\ndoi: 10.18653/v1/2024.findings-acl.591. URL https://aclanthology.org/2024.findings-acl.591/.\n\ndoi: 10.18653/v1/2024.findings-acl.591. URL\n\n.\n\nhttps://aclanthology.org/2024.findings-acl.591/\n\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\nhttps://arxiv.org/abs/2504.20571.\n\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\n\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\n\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\n\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\n\nhttps://arxiv.org/abs/2504.20571.\n\n.\n\nhttps://arxiv.org/abs/2504.20571\n\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\nLLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\nhttps://doi.org/10.48550/arXiv.2308.08155.\n\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\n\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\n\nLLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\n\nLLM applications via multi-agent conversation framework.\n\n, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2308.08155.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2308.08155\n\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows. CoRR,\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL https://doi.org/10.48550/arXiv.2403.11322.\n\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows. CoRR,\n\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows.\n\n,\n\nCoRR\n\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL https://doi.org/10.48550/arXiv.2403.11322.\n\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2403.11322\n\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\nreasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\nhttps://doi.org/10.48550/arXiv.2502.14768.\n\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\n\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\n\nreasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\n\nreasoning with rule-based reinforcement learning.\n\n, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2502.14768.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2502.14768\n\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\nself-training framework for advanced reasoning, 2025. URL https://arxiv.org/abs/2504.08672.\n\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\n\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\n\nself-training framework for advanced reasoning, 2025. URL https://arxiv.org/abs/2504.08672.\n\nself-training framework for advanced reasoning, 2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.08672\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\nCoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\n\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\n\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\n\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\n\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\n\nCoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\n\n, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2412.15115\n\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.\ndoi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122.\n\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\n\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\n\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.\n\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement.\n\n, abs/2409.12122, 2024b.\n\nCoRR\n\ndoi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122.\n\ndoi: 10.48550/ARXIV.2409.12122. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2409.12122\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\n\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\n\nmodels. In\n\n.\n\nThe Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023\n\nOpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.\n\nOpenReview.net, 2023. URL\n\n.\n\nhttps://openreview.net/forum?id=WE_vluYUL-X\n\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. CoRR,\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL https://doi.org/10.48550/arXiv.2411.00062.\n\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. CoRR,\n\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play.\n\n,\n\nCoRR\n\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL https://doi.org/10.48550/arXiv.2411.00062.\n\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2411.00062\n\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\nCoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476.\n\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\n\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\n\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\n\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\n\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\n\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\n\nCoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476.\n\n, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2503.14476\n\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. URL https://arxiv.\norg/abs/2401.10020, 2024.\n\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. URL https://arxiv.\n\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models.\n\nURL https://arxiv.\n\norg/abs/2401.10020, 2024.\n\n, 2024.\n\norg/abs/2401.10020\n\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\nlearning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.\n\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\n\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\n\nlearning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.\n\nlearning for advanced reasoning tasks.\n\n, 2025.\n\narXiv preprint arXiv:2504.05118\n\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\nAdvances in Neural Information Processing Systems, 36:60247\u201360277, 2023.\n\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\n\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\n\nAdvances in Neural Information Processing Systems, 36:60247\u201360277, 2023.\n\n, 36:60247\u201360277, 2023.\n\nAdvances in Neural Information Processing Systems\n\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\nZhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html.\n\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\n\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\n\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\n\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\n\nZhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\n\nZhang, C. (eds.),\n\nAdvances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\n\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_\n\n, 2024. URL\n\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024\n\nhttp://papers.nips.cc/paper_\n\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html.\n\n.\n\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html\n\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\ncapacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837.\n\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\n\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\n\ncapacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837.\n\ncapacity in llms beyond the base model?, 2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.13837\n\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information\nProcessing Systems, 35:15476\u201315488, 2022.\n\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information\n\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning.\n\nAdvances in Neural Information\n\nProcessing Systems, 35:15476\u201315488, 2022.\n\n, 35:15476\u201315488, 2022.\n\nProcessing Systems\n\n18\n\n18\n\n18\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nZeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis. CoRR,\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL https://doi.org/10.48550/arXiv.2502.01718.\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\nlearning for open base models in the wild. CoRR, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL https:\n//doi.org/10.48550/arXiv.2503.18892.\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\non replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\nincentivization, 2025b. URL https://arxiv.org/abs/2504.05812.\nZhang, Y. and Yang, Q. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609,\n2021.\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Pro-\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html.\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\nJ. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\nURL https://doi.org/10.1609/aaai.v38i17.29936.\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.), AAAI-25, Sponsored by the Association for the\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 26021\u201326030. AAAI Press, 2025a.\ndoi: 10.1609/AAAI.V39I24.34797. URL https://doi.org/10.1609/aaai.v39i24.34797.\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning. Neural\nNetworks, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL https://doi.org/10.1016/j.neunet.2025.\n107448.\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research,\npp. 2165\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html.\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n2025. URL https://arxiv.org/abs/2504.16084.\n\nZeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis. CoRR,\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL https://doi.org/10.48550/arXiv.2502.01718.\n\nZeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis. CoRR,\n\nZeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis.\n\n,\n\nCoRR\n\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL https://doi.org/10.48550/arXiv.2502.01718.\n\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2502.01718\n\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\nlearning for open base models in the wild. CoRR, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL https:\n//doi.org/10.48550/arXiv.2503.18892.\n\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\n\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\n\nlearning for open base models in the wild. CoRR, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL https:\n\nlearning for open base models in the wild.\n\n, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL\n\nCoRR\n\nhttps:\n\n//doi.org/10.48550/arXiv.2503.18892.\n\n.\n\n//doi.org/10.48550/arXiv.2503.18892\n\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\non replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.\n\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\n\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\n\non replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.\n\non replication studies and more directions for reasoning language models.\n\n, 2025a.\n\narXiv preprint arXiv:2505.00551\n\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\nincentivization, 2025b. URL https://arxiv.org/abs/2504.05812.\n\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\n\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\n\nincentivization, 2025b. URL https://arxiv.org/abs/2504.05812.\n\nincentivization, 2025b. URL\n\n.\n\nhttps://arxiv.org/abs/2504.05812\n\nZhang, Y. and Yang, Q. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609,\n2021.\n\nZhang, Y. and Yang, Q. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609,\n\nZhang, Y. and Yang, Q. A survey on multi-task learning.\n\n, 34(12):5586\u20135609,\n\nIEEE transactions on knowledge and data engineering\n\n2021.\n\n2021.\n\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Pro-\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html.\n\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\n\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\n\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Pro-\n\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.),\n\nAdvances in Neural Information Pro-\n\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\n\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\n\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\n\n, 2022. URL\n\nLA, USA, November 28 - December 9, 2022\n\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\n\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html.\n\n.\n\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html\n\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\nJ. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\nURL https://doi.org/10.1609/aaai.v38i17.29936.\n\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\n\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\n\nJ. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\n\nJ. G., and Natarajan, S. (eds.),\n\nThirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\n\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\n\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\n\nEAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\n\n, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\n\nEAAI 2014, February 20-27, 2024, Vancouver, Canada\n\nURL https://doi.org/10.1609/aaai.v38i17.29936.\n\nURL\n\n.\n\nhttps://doi.org/10.1609/aaai.v38i17.29936\n\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.), AAAI-25, Sponsored by the Association for the\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 26021\u201326030. AAAI Press, 2025a.\ndoi: 10.1609/AAAI.V39I24.34797. URL https://doi.org/10.1609/aaai.v39i24.34797.\n\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\n\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\n\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.), AAAI-25, Sponsored by the Association for the\n\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.),\n\nAAAI-25, Sponsored by the Association for the\n\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 26021\u201326030. AAAI Press, 2025a.\n\n, pp. 26021\u201326030. AAAI Press, 2025a.\n\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA\n\ndoi: 10.1609/AAAI.V39I24.34797. URL https://doi.org/10.1609/aaai.v39i24.34797.\n\ndoi: 10.1609/AAAI.V39I24.34797. URL\n\n.\n\nhttps://doi.org/10.1609/aaai.v39i24.34797\n\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning. Neural\nNetworks, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL https://doi.org/10.1016/j.neunet.2025.\n107448.\n\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning. Neural\n\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning.\n\nNeural\n\nNetworks, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL https://doi.org/10.1016/j.neunet.2025.\n\n, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL\n\nNetworks\n\nhttps://doi.org/10.1016/j.neunet.2025.\n\n107448.\n\n.\n\n107448\n\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research,\npp. 2165\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html.\n\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\n\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\n\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\n\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\n\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\n\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\n\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\n\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\n\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\n\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\n\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference\n\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.),\n\nConference\n\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research,\n\n, volume 229 of\n\n,\n\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA\n\nProceedings of Machine Learning Research\n\npp. 2165\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html.\n\npp. 2165\u20132183. PMLR, 2023. URL\n\n.\n\nhttps://proceedings.mlr.press/v229/zitkovich23a.html\n\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n2025. URL https://arxiv.org/abs/2504.16084.\n\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n\n2025. URL https://arxiv.org/abs/2504.16084.\n\n2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.16084\n\n19\n\n19\n\n19\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAppendix\n\nAppendix\n\nAppendix\n\nAppendix Contents\n\nAppendix Contents\n\nAppendix Contents\n\n20\n\n20\n\n20\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nA. Reinforcement Learning with Verifiable Rewards.\n\nA. Reinforcement Learning with Verifiable Rewards.\n\nA. Reinforcement Learning with Verifiable Rewards.\n\nWe use reinforcement learning to update our learner LLM, rewarding it based on a task-specific reward function rf , where the subscript\nf indicates the task. The goal of the RL agent is to maximize the expected discounted sum of rewards. We adopt an online variant of RL,\nREINFORCE++, which is optimized using the original PPO objective:\n\nWe use reinforcement learning to update our learner LLM, rewarding it based on a task-specific reward function rf , where the subscript\n\nWe use reinforcement learning to update our learner LLM, rewarding it based on a task-specific reward function\n\n, where the subscript\n\nr\n\nf\n\nf indicates the task. The goal of the RL agent is to maximize the expected discounted sum of rewards. We adopt an online variant of RL,\n\nindicates the task. The goal of the RL agent is to maximize the expected discounted sum of rewards. We adopt an online variant of RL,\n\nf\n\nREINFORCE++, which is optimized using the original PPO objective:\n\nREINFORCE++, which is optimized using the original PPO objective:\n\nLPPO(\u03b8) = Eq\u223cP (Q), o\u223c\u03c0\u03b8old (O|q)\n\"\n1\n|o|\nX\n|o|\nt=1\nmin \nst(\u03b8)A\nnorm\nf,q , clip (st(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) A\nnorm\nf,q \n#\n, (9)\n\n|o|\n\no\n\n\"\n\n\"\n\n#\n\n#\n\n|\n\n|\n\n1\n\n1\n\nnorm\n\nnorm\n\nnorm\n\nnorm\n\nX\n\nX\n\nLPPO(\u03b8) = Eq\u223cP (Q), o\u223c\u03c0\u03b8old (O|q)\n\nE\n\nf,q , clip (st(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) A\n\nclip\n\n, (9)\n\n(9)\n\nmin\n\nf,q\n\n(\n\n) =\n\nmin\n\nst(\u03b8)A\n\n(\n\n)\n\n(\n\n(\n\n)\n\n1\n\n1 +\n\n)\n\n\u03b8\n\ns\n\n\u03b8\n\nA\n\n,\n\ns\n\n\u03b8\n\n,\n\n\u03f5,\n\n\u03f5\n\nA\n\n,\n\nPPO\n\nL\n\n\u2212\n\nt\n\nt\n\n(\n\n)\n\n(\n\n)\n\nq\n\nP\n\nQ\n\n, o\n\n\u03c0\n\nO\n\nq\n\nf,q\n\nf,q\n\n\u223c\n\n\u223c\n\n|\n\n\u03b8\n\n|o|\n\no\n\nold\n\n|\n\n|\n\nt=1\n\n=1\n\nt\n\nwhere st(\u03b8) is the probability ratio between the new and old policies at timestep t, and A\nnorm\nf,q is the normalized advantage.\n\nnorm\n\nnorm\n\nwhere st(\u03b8) is the probability ratio between the new and old policies at timestep t, and A\n\nwhere\n\nis the probability ratio between the new and old policies at timestep\n\n, and\n\nf,q is the normalized advantage.\n\nis the normalized advantage.\n\n(\n\n)\n\ns\n\n\u03b8\n\nt\n\nA\n\nt\n\nf,q\n\nREINFORCE++ computes the normalized advantage as:\n\nREINFORCE++ computes the normalized advantage as:\n\nREINFORCE++ computes the normalized advantage as:\n\nA\nnorm\nf,q =\nrf,q \u2212 mean({Af,q}\nB)\nstd({Af,q}B)\n, (10)\n\nrf,q \u2212 mean({Af,q}\n\nmean\n\nB)\n\nB\n\n(\n\n)\n\nr\n\nA\n\nnorm\n\nnorm\n\n\u2212\n\n{\n\n}\n\nf,q\n\nf,q\n\n, (10)\n\n(10)\n\nf,q =\n\n=\n\nA\n\nA\n\n,\n\nf,q\n\nstd({Af,q}B)\n\nstd\n\nB\n\n(\n\n)\n\nA\n\n{\n\n}\n\nf,q\n\nwhere rf,q is the outcome reward for question q, task f, mean and std are calculated across the global batch with batch size B. Note that\nwe do not apply any KL penalty to the loss or reward.\n\nwhere rf,q is the outcome reward for question q, task f, mean and std are calculated across the global batch with batch size B. Note that\n\nis the outcome reward for question\n\n, task\n\n, mean and std are calculated across the global batch with batch size\n\n. Note that\n\nwhere\n\nr\n\nq\n\nf\n\nB\n\nf,q\n\nwe do not apply any KL penalty to the loss or reward.\n\nwe do not apply any KL penalty to the loss or reward.\n\nB. Implementation Details\n\nB. Implementation Details\n\nB. Implementation Details\n\nWe built Absolute Zero Reasoner upon the veRL codebase (Sheng et al., 2025). For code execution, we incorporated components from\nthe QwQ Python executor. For safer code execution, we recommend using API-based services such as E2B instead.\n\nWe built Absolute Zero Reasoner upon the veRL codebase (Sheng et al., 2025). For code execution, we incorporated components from\n\nWe built Absolute Zero Reasoner upon the\n\nveRL codebase\n\n(Sheng et al.,\n\n. For code execution, we incorporated components from\n\n2025)\n\nthe QwQ Python executor. For safer code execution, we recommend using API-based services such as E2B instead.\n\nthe\n\nQwQ Python executor.\n\nFor safer code execution, we recommend using API-based services such as\n\nE2B\n\ninstead.\n\nAll experiments were conducted on clusters of A800 GPUs.\n\nAll experiments were conducted on clusters of A800 GPUs.\n\nAll experiments were conducted on clusters of A800 GPUs.\n\nTraining Hyperparameters. We show the hyperparameters used in our training in Table 3. We do not change them for any of\nthe runs.\n\nTraining Hyperparameters. We show the hyperparameters used in our training in Table 3. We do not change them for any of\n\nWe show the hyperparameters used in our training in Table\n\nWe do not change them for any of\n\n3.\n\nTraining Hyperparameters.\n\nthe runs.\n\nthe runs.\n\nParameterValueModel ConfigurationMax Prompt Length6144Max Response Length8096Seed Batch Factor4Max Programs16384Training SettingsTrain Batch Size64 * 6Learning Rate1e-6OptimizerAdamWGrad Clip1.0Total Steps500RL SettingsAlgorithmTRR++ (Section\n3.3.5)KL LossFalseKL RewardFalseEntropy Coefficient0.001PPO Epochs1Rollouts\nN1Rollout Temperature1.0Rollout Top-P1.0References\nK6Samples to Estimate Task Accuracy\nN8\n\nValue\n\nParameter\n\nModel Configuration\n\nMax Prompt Length\n\n6144\n\nMax Response Length\n\n8096\n\nSeed Batch Factor\n\n4\n\nMax Programs\n\n16384\n\nTraining Settings\n\nTrain Batch Size\n\n64 * 6\n\nLearning Rate\n\n1e-6\n\nOptimizer\n\nAdamW\n\n1.0\n\nGrad Clip\n\nTotal Steps\n\n500\n\nRL Settings\n\nAlgorithm\n\nTRR++ (Section\n3.3.5)\n\nKL Loss\n\nFalse\n\nKL Reward\n\nFalse\n\nEntropy Coefficient\n\n0.001\n\nPPO Epochs\n\n1\n\nRollouts\nN\n\n1\n\nRollout Temperature\n\n1.0\n\nRollout Top-P\n\n1.0\n\nReferences\nK\n\n6\n\nSamples to Estimate Task Accuracy\nN\n\n8\n\nTable 3. Hyperparameters Used During AZR Self-play Training.\n\nTable 3. Hyperparameters Used During AZR Self-play Training.\n\nHyperparameters Used During AZR Self-play Training.\n\nTable 3.\n\n21\n\n21\n\n21\n\nModelData CurationBase ModelOat-7B (Liu et al.,\n2025)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-MathSimpleRL-Zoo (Zeng et al.,\n2025b)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-BaseOpenReasonerZero (Hu et al.,\n2025)57k STEM + math samplesQwen2.5-7B-BasePRIME-Zero (Cui et al.,\n2025)457k math + 27k code problemsQwen2.5Math-7B-BaseCodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)2k Leetcode pairsQwen2.5-7B-Instruct-1MCodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)2k Leetcode + 10k TACO pairs (Li et al.,\n2023)Qwen2.5-7B-Instruct-1MAceCoder-7B-Ins-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Code-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderAceCoder-7B-Code-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderQwen-7B-Instruct (Yang et al.,\n2024a)1M SFT + 150k RL pairsQwen2.5-7B-BaseAZR-7B (Ours)No dataQwen2.5-7B-BaseAZR-7B-Coder (Ours)No dataQwen2.5-7B-CoderTable 4. Reasoner Training Data Source and Base Model.\n\nModelData CurationBase ModelOat-7B (Liu et al.,\n2025)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-MathSimpleRL-Zoo (Zeng et al.,\n2025b)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-BaseOpenReasonerZero (Hu et al.,\n2025)57k STEM + math samplesQwen2.5-7B-BasePRIME-Zero (Cui et al.,\n2025)457k math + 27k code problemsQwen2.5Math-7B-BaseCodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)2k Leetcode pairsQwen2.5-7B-Instruct-1MCodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)2k Leetcode + 10k TACO pairs (Li et al.,\n2023)Qwen2.5-7B-Instruct-1MAceCoder-7B-Ins-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Code-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderAceCoder-7B-Code-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderQwen-7B-Instruct (Yang et al.,\n2024a)1M SFT + 150k RL pairsQwen2.5-7B-BaseAZR-7B (Ours)No dataQwen2.5-7B-BaseAZR-7B-Coder (Ours)No dataQwen2.5-7B-Coder\n\nBase Model\n\nModel\n\nData Curation\n\nQwen2.5-7B-Math\n\nOat-7B (Liu et al.,\n2025)\n\n8.5k math pairs (Hendrycks et al.,\n2021)\n\nSimpleRL-Zoo (Zeng et al.,\n2025b)\n\n8.5k math pairs (Hendrycks et al.,\n2021)\n\nQwen2.5-7B-Base\n\nOpenReasonerZero (Hu et al.,\n2025)\n\n57k STEM + math samples\n\nQwen2.5-7B-Base\n\nPRIME-Zero (Cui et al.,\n2025)\n\n457k math + 27k code problems\n\nQwen2.5Math-7B-Base\n\nCodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)\n\n2k Leetcode pairs\n\nQwen2.5-7B-Instruct-1M\n\nCodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)\n\n2k Leetcode + 10k TACO pairs (Li et al.,\n2023)\n\nQwen2.5-7B-Instruct-1M\n\nAceCoder-7B-Ins-RM (Zeng et al.,\n2025a)\n\n22k code data\n\nQwen2.5-7B-Instruct\n\nAceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)\n\n22k code data\n\nQwen2.5-7B-Instruct\n\nAceCoder-7B-Code-RM (Zeng et al.,\n2025a)\n\n22k code data\n\nQwen2.5-7B-Coder\n\nAceCoder-7B-Code-Rule (Zeng et al.,\n2025a)\n\n22k code data\n\nQwen2.5-7B-Coder\n\nQwen-7B-Instruct (Yang et al.,\n2024a)\n\n1M SFT + 150k RL pairs\n\nQwen2.5-7B-Base\n\nAZR-7B (Ours)\n\nNo data\n\nQwen2.5-7B-Base\n\nAZR-7B-Coder (Ours)\n\nNo data\n\nQwen2.5-7B-Coder\n\nTable 4. Reasoner Training Data Source and Base Model.\n\nTable 4. Reasoner Training Data Source and Base Model.\n\nReasoner Training Data Source and Base Model.\n\nTable 4.\n\nloggingrandommultiprocessingpebblesubprocessthreadingdatetimetimehashlibcalendarbcryptos.sysos.pathsys.exitos.environ\n\nlogging\n\nrandom\n\nmultiprocessing\n\npebble\n\nsubprocess\n\nthreading\n\ndatetime\n\ntime\n\nhashlib\n\ncalendar\n\nbcrypt\n\nos.sys\n\nos.path\n\nsys.exit\n\nos.environ\n\nFigure 8. Forbidden Python Modules. List of Python modules forbidden to exist in proposed tasks' programs.\n\nFigure 8. Forbidden Python Modules. List of Python modules forbidden to exist in proposed tasks' programs.\n\nList of Python modules forbidden to exist in proposed tasks' programs.\n\nForbidden Python Modules.\n\nFigure 8.\n\nC. More Results\n\nC. More Results\n\nC. More Results\n\nC.1. Out-of-Distribution Performance Breakdown\n\nC.1. Out-of-Distribution Performance Breakdown\n\nC.1. Out-of-Distribution Performance Breakdown\n\nWe plot the out-of-distribution performance, broken down by each benchmark and in aggregate, across training steps for our 7B, 7B-Coder,\n14B, and 14B-Coder models in Figures 28 to 31. We observe a strong correlation between training using AZR and improvements in both\nmathematical and coding reasoning capabilities. Moreover, our models are trained for more steps than typical zero-style reasoners; while\noverfitting can occur with static datasets, it is less likely in AZR due to dynamically proposed tasks.\n\nWe plot the out-of-distribution performance, broken down by each benchmark and in aggregate, across training steps for our 7B, 7B-Coder,\n\nWe plot the out-of-distribution performance, broken down by each benchmark and in aggregate, across training steps for our 7B, 7B-Coder,\n\n14B, and 14B-Coder models in Figures 28 to 31. We observe a strong correlation between training using AZR and improvements in both\n\n14B, and 14B-Coder models in Figures\n\nWe observe a strong correlation between training using AZR and improvements in both\n\n28\n\nto\n\n31.\n\nmathematical and coding reasoning capabilities. Moreover, our models are trained for more steps than typical zero-style reasoners; while\n\nmathematical and coding reasoning capabilities. Moreover, our models are trained for more steps than typical zero-style reasoners; while\n\noverfitting can occur with static datasets, it is less likely in AZR due to dynamically proposed tasks.\n\noverfitting can occur with static datasets, it is less likely in AZR due to dynamically proposed tasks.\n\nC.2. In-Distribution Results\n\nC.2. In-Distribution Results\n\nC.2. In-Distribution Results\n\nSince we have defined the task domains as input prediction and output prediction, we can directly evaluate our model's capabilities in these\nareas using popular code reasoning benchmarks: CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E) (Gu\net al., 2024; Jain et al., 2024), where CruxEval-O and LCB-E is solving the deduction task, and CruxEval-I is solving the abduction task.\nIn Figure 14, we visualize the evolution of these metrics during the training of Absolute Zero Reasoner-base-7b. As training\nprogresses, we observe a consistent improvement in in-distribution performance across steps. While these three benchmark curves do\nnot perfectly correlate with broader coding or math reasoning capabilities (compare this with Figure 28), they serve as useful proxies for\ntracking task-specific progress.\n\nSince we have defined the task domains as input prediction and output prediction, we can directly evaluate our model's capabilities in these\n\nSince we have defined the task domains as input prediction and output prediction, we can directly evaluate our model's capabilities in these\n\nareas using popular code reasoning benchmarks: CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E) (Gu\n\nareas using popular code reasoning benchmarks: CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E)\n\n(Gu\n\net al., 2024; Jain et al., 2024), where CruxEval-O and LCB-E is solving the deduction task, and CruxEval-I is solving the abduction task.\n\net al.,\n\nJain et al.,\n\n, where CruxEval-O and LCB-E is solving the deduction task, and CruxEval-I is solving the abduction task.\n\n2024;\n\n2024)\n\nIn Figure 14, we visualize the evolution of these metrics during the training of Absolute Zero Reasoner-base-7b. As training\n\nIn Figure\n\n14,\n\nwe visualize the evolution of these metrics during the training of\n\n. As training\n\nAbsolute Zero Reasoner-base-7b\n\nprogresses, we observe a consistent improvement in in-distribution performance across steps. While these three benchmark curves do\n\nprogresses, we observe a consistent improvement in in-distribution performance across steps. While these three benchmark curves do\n\nnot perfectly correlate with broader coding or math reasoning capabilities (compare this with Figure 28), they serve as useful proxies for\n\nnot perfectly correlate with broader coding or math reasoning capabilities (compare this with Figure\n\n, they serve as useful proxies for\n\n28)\n\ntracking task-specific progress.\n\ntracking task-specific progress.\n\nC.3. Interplay Between Propose and Solve Roles\n\nC.3. Interplay Between Propose and Solve Roles\n\nC.3. Interplay Between Propose and Solve Roles\n\nWe visualize the training dynamics between the propose and solve roles over training steps in Figures 15 to 17. We observe that, in\ngeneral, the solve roles produce more output tokens than the propose role. Intuitively, this makes sense: the propose role emphasizes\ncreativity and generation of novel tasks, whereas the solve role requires deeper reasoning, which naturally leads to longer outputs.\n\nWe visualize the training dynamics between the propose and solve roles over training steps in Figures 15 to 17. We observe that, in\n\nWe visualize the training dynamics between the propose and solve roles over training steps in Figures\n\n15\n\nto\n\n17.\n\nWe observe that, in\n\ngeneral, the solve roles produce more output tokens than the propose role. Intuitively, this makes sense: the propose role emphasizes\n\ngeneral, the solve roles produce more output tokens than the propose role. Intuitively, this makes sense: the propose role emphasizes\n\ncreativity and generation of novel tasks, whereas the solve role requires deeper reasoning, which naturally leads to longer outputs.\n\ncreativity and generation of novel tasks, whereas the solve role requires deeper reasoning, which naturally leads to longer outputs.\n\nInterestingly, we also observe a consistent ordering in token length across reasoning types\u2014abduction and deduction tasks tend to result\nin shorter outputs than induction tasks during problem solving. This aligns with our intuition, as we observed the model engaging\nin trial-and-error reasoning\u2014repeatedly generating hypothesized inputs, evaluating their outcomes, and reflecting and retrying when\nsubsequent deductions fail to produce the correct output. To our knowledge, this is the first time such a clear distinction in token length\n\nInterestingly, we also observe a consistent ordering in token length across reasoning types\u2014abduction and deduction tasks tend to result\n\nInterestingly, we also observe a consistent ordering in token length across reasoning types\u2014abduction and deduction tasks tend to result\n\nin shorter outputs than induction tasks during problem solving. This aligns with our intuition, as we observed the model engaging\n\nin shorter outputs than induction tasks during problem solving. This aligns with our intuition, as we observed the model engaging\n\nin trial-and-error reasoning\u2014repeatedly generating hypothesized inputs, evaluating their outcomes, and reflecting and retrying when\n\nin trial-and-error reasoning\u2014repeatedly generating hypothesized inputs, evaluating their outcomes, and reflecting and retrying when\n\nsubsequent deductions fail to produce the correct output. To our knowledge, this is the first time such a clear distinction in token length\n\nsubsequent deductions fail to produce the correct output. To our knowledge, this is the first time such a clear distinction in token length\n\n1 VALIDATE_CODE_TEMPLATE = \"\"\"{ code }\n2 repr (f({ inputs }))\"\"\"\n3\n4 exec ( VALIDATE_CODE_TEMPLATE )\n\n1 VALIDATE_CODE_TEMPLATE = \"\"\"{ code }\n\nVALIDATE_CODE_TEMPLATE =\n\n\"\"\"{\n\ncode\n\n}\n\n1\n\n2 repr (f({ inputs }))\"\"\"\n\nrepr\n\n(f({\n\ninputs\n\n}))\"\"\"\n\n2\n\n3\n\n3\n\n4 exec ( VALIDATE_CODE_TEMPLATE )\n\nexec\n\n( VALIDATE_CODE_TEMPLATE )\n\n4\n\nFigure 9. Python Program to Check Valid Code.\n\nFigure 9. Python Program to Check Valid Code.\n\nPython Program to Check Valid Code.\n\nFigure 9.\n\n22\n\n22\n\n22\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n1 EVAL_INPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n2 { gold_output } == f({ agent_input }) \"\"\"\n3\n4 exec ( EVAL_INPUT_PREDICTION_TEMPLATE )\n\n1 EVAL_INPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n\nEVAL_INPUT_PREDICTION_TEMPLATE =\n\n\"\"\"\n\n{\n\ncode\n\n}\n\n1\n\n2 { gold_output } == f({ agent_input }) \"\"\"\n\n{\n\ngold_output\n\n}\n\n==\n\nf({\n\nagent_input\n\n})\n\n\"\"\"\n\n2\n\n3\n\n3\n\n4 exec ( EVAL_INPUT_PREDICTION_TEMPLATE )\n\nexec\n\n( EVAL_INPUT_PREDICTION_TEMPLATE )\n\n4\n\nFigure 10. Python Code to Check Agent Input Abduction Correctness.\n\nFigure 10. Python Code to Check Agent Input Abduction Correctness.\n\nPython Code to Check Agent Input Abduction Correctness.\n\nFigure 10.\n\n1 EVAL_OUTPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n2 eval ({ gold_output }) == eval ({ agent_output }) \"\"\"\n3\n4 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\n1 EVAL_OUTPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n\nEVAL_OUTPUT_PREDICTION_TEMPLATE =\n\n\"\"\"\n\n{\n\ncode\n\n}\n\n1\n\n2 eval ({ gold_output }) == eval ({ agent_output }) \"\"\"\n\neval\n\n({\n\ngold_output\n\n})\n\n==\n\neval\n\n({\n\nagent_output\n\n})\n\n\"\"\"\n\n2\n\n3\n\n3\n\n4 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\nexec\n\n( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\n4\n\nFigure 11. Python Code to Check Agent Output Deduction Correctness.\n\nFigure 11. Python Code to Check Agent Output Deduction Correctness.\n\nPython Code to Check Agent Output Deduction Correctness.\n\nFigure 11.\n\n1 EVAL_FUNCTION_PREDICTION_TEMPLATE = \"\"\"{ code }\n2 matches = []\n3 for gold_input , gold_output in zip ({ gold_inputs } , { gold_outputs }):\n4 match = { gold_output } == f({ gold_input })\n5 matches . append ( match )\n6 \"\"\"\n7\n8 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\n1 EVAL_FUNCTION_PREDICTION_TEMPLATE = \"\"\"{ code }\n\nEVAL_FUNCTION_PREDICTION_TEMPLATE =\n\n\"\"\"{\n\ncode\n\n}\n\n1\n\n2 matches = []\n\nmatches\n\n=\n\n[]\n\n2\n\n3 for gold_input , gold_output in zip ({ gold_inputs } , { gold_outputs }):\n\nfor\n\ngold_input\n\n,\n\ngold_output\n\nin\n\nzip\n\n({\n\ngold_inputs\n\n} ,\n\n{\n\ngold_outputs\n\n}):\n\n3\n\n4 match = { gold_output } == f({ gold_input })\n\nmatch\n\n=\n\n{\n\ngold_output\n\n}\n\n==\n\nf({\n\ngold_input\n\n})\n\n4\n\n5 matches . append ( match )\n\nmatches\n\n.\n\nappend\n\n(\n\nmatch\n\n)\n\n5\n\n6 \"\"\"\n\n\"\"\"\n\n6\n\n7\n\n7\n\n8 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\nexec\n\n( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\n8\n\nFigure 12. Python Code to Check Agent Function Induction Correctness.\n\nFigure 12. Python Code to Check Agent Function Induction Correctness.\n\nPython Code to Check Agent Function Induction Correctness.\n\nFigure 12.\n\n1 CHECK_DETERMINISM_TEMPLATE = \"\"\"{ code }\n2 returns = f({ inputs })\n3 if returns != f({ inputs }):\n4 raise Exception ( 'Non - deterministic code ')\n5 repr ( returns ) \"\"\"\n6\n7 exec ( CHECK_DETERMINISM_TEMPLATE )\n\n1 CHECK_DETERMINISM_TEMPLATE = \"\"\"{ code }\n\nCHECK_DETERMINISM_TEMPLATE =\n\n\"\"\"{\n\ncode\n\n}\n\n1\n\n2 returns = f({ inputs })\n\nreturns\n\n=\n\nf({\n\ninputs\n\n})\n\n2\n\n3 if returns != f({ inputs }):\n\nif\n\nreturns\n\n!=\n\nf({\n\ninputs\n\n}):\n\n3\n\n4 raise Exception ( 'Non - deterministic code ')\n\nraise\n\nException\n\n( 'Non\n\n-\n\ndeterministic\n\ncode\n\n')\n\n4\n\n5 repr ( returns ) \"\"\"\n\nrepr\n\n(\n\nreturns\n\n)\n\n\"\"\"\n\n5\n\n6\n\n6\n\n7 exec ( CHECK_DETERMINISM_TEMPLATE )\n\nexec\n\n( CHECK_DETERMINISM_TEMPLATE )\n\n7\n\n0 30 60 90 120 150 180 210 240 270\nTraining Steps\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nPerformance Score\n\nCruxEval-I\nCruxEval-O\nLiveCodeBench-Execution\nFigure 13. Python Code to Check Deterministic Program.\n\nFigure 13. Python Code to Check Deterministic Program.\n\nFigure 13. Python Code to Check Deterministic Program.\n\nPython Code to Check Deterministic Program.\n\nFigure 13.\n\n0 30 60 90 120 150 180 210 240 270\nTraining Steps\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nPerformance Score\n\nCruxEval-I\nCruxEval-O\nLiveCodeBench-Execution\n\n0.65\n\n0.65\n\n0.60\n\n0.60\n\nPerformance Score\n\nPerformance Score\n\n0.55\n\n0.55\n\n0.50\n\n0.50\n\n0.45\n\n0.45\n\nCruxEval-I\n\nCruxEval-I\n\n0.40\n\n0.40\n\nCruxEval-O\n\nCruxEval-O\n\nLiveCodeBench-Execution\n\nLiveCodeBench-Execution\n\n0.35\n\n0.35\n\n0 30 60 90 120 150 180 210 240 270\n\n0\n\n30\n\n60\n\n90\n\n120\n\n150\n\n180\n\n210\n\n240\n\n270\n\nTraining Steps\n\nTraining Steps\n\nFigure 14. In-distribution Benchmark Score During Training. The evolution of CruxEval-I, CruxEval-O, and LiveCodeBench-\nExecution during training for the Qwen2.5-7B base model trained using AZR.\n\nFigure 14. In-distribution Benchmark Score During Training. The evolution of CruxEval-I, CruxEval-O, and LiveCodeBench-\n\nThe evolution of CruxEval-I, CruxEval-O, and LiveCodeBench-\n\nIn-distribution Benchmark Score During Training.\n\nFigure 14.\n\nExecution during training for the Qwen2.5-7B base model trained using AZR.\n\nExecution during training for the\n\nbase model trained using AZR.\n\nQwen2.5-7B\n\n23\n\n23\n\n23\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\nReward\n\n0 40 80 120 160 200 240 280\n1000\n2000\n3000\n4000\nToken Length\nTraining Steps Abduction Task\nSolve Propose\nFigure 15. Abduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for abduction task type of\nAbsolute Zero Reasoner-base-7b.\n\n0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\nReward\n\n0 40 80 120 160 200 240 280\n1000\n2000\n3000\n4000\nToken Length\nTraining Steps Abduction Task\nSolve Propose\n\nSolve Propose\n\nSolve\n\nPropose\n\n0.8\n\n0.8\n\n4000\n\n4000\n\n0.6\n\n0.6\n\nToken Length\n\nToken Length\n\n3000\n\n3000\n\nReward\n\nReward\n\n0.4\n\n0.4\n\n2000\n\n2000\n\n0.2\n\n0.2\n\n1000\n\n1000\n\n0.0\n\n0.0\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\nTraining Steps Abduction Task\n\nTraining Steps\n\nAbduction Task\n\nFigure 15. Abduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for abduction task type of\nAbsolute Zero Reasoner-base-7b.\n\nFigure 15. Abduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for abduction task type of\n\nThe task reward and token lengths of the two roles for abduction task type of\n\nAbduction Task Reward and Token Lengths.\n\nFigure 15.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\n0 40 80 120 160 200 240 280\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nReward\n\n0 40 80 120 160 200 240 280\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nToken Length\nTraining Steps Induction Task\nSolve Propose\nFigure 16. Induction Task Reward and Token Lengths. The task reward and token lengths of the two roles for induction task type of\nAbsolute Zero Reasoner-base-7b.\n\n0 40 80 120 160 200 240 280\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nReward\n\n0 40 80 120 160 200 240 280\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nToken Length\nTraining Steps Induction Task\nSolve Propose\n\nSolve Propose\n\nSolve\n\nPropose\n\n0.7\n\n0.7\n\n2000\n\n2000\n\n0.6\n\n0.6\n\n1750\n\n1750\n\n0.5\n\n0.5\n\n1500\n\n1500\n\nToken Length\n\nToken Length\n\nReward\n\nReward\n\n0.4\n\n0.4\n\n1250\n\n1250\n\n0.3\n\n0.3\n\n1000\n\n1000\n\n0.2\n\n0.2\n\n750\n\n750\n\n0.1\n\n0.1\n\n500\n\n500\n\n250\n\n250\n\n0.0\n\n0.0\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\nTraining Steps Induction Task\n\nTraining Steps\n\nInduction Task\n\nFigure 16. Induction Task Reward and Token Lengths. The task reward and token lengths of the two roles for induction task type of\nAbsolute Zero Reasoner-base-7b.\n\nFigure 16. Induction Task Reward and Token Lengths. The task reward and token lengths of the two roles for induction task type of\n\nThe task reward and token lengths of the two roles for induction task type of\n\nInduction Task Reward and Token Lengths.\n\nFigure 16.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\n24\n\n24\n\n24\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n\n0 40 80 120 160 200 240 280\n400\n600\n800\n1000\n1200\n1400\nToken Length\nTraining Steps Deduction Task\nSolve Propose\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\n\n0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n\n0 40 80 120 160 200 240 280\n400\n600\n800\n1000\n1200\n1400\nToken Length\nTraining Steps Deduction Task\nSolve Propose\n\nSolve Propose\n\nSolve\n\nPropose\n\n1.0\n\n1.0\n\n1400\n\n1400\n\n1200\n\n1200\n\n0.8\n\n0.8\n\n1000\n\n1000\n\nToken Length\n\nToken Length\n\n0.6\n\n0.6\n\nReward\n\nReward\n\n800\n\n800\n\n0.4\n\n0.4\n\n600\n\n600\n\n0.2\n\n0.2\n\n400\n\n400\n\n0.0\n\n0.0\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\nTraining Steps Deduction Task\n\nTraining Steps\n\nDeduction Task\n\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\n\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\n\nThe task reward and token lengths of the two roles for deduction task type of\n\nDeduction Task Reward and Token Lengths.\n\nFigure 17.\n\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\n\nFigure 17.\n\nThe task reward and token lengths of the two roles for deduction task type of\n\nDeduction Task Reward and Token Lengths.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\nTask: Propose Deduction Task\nModel Input: (k random selected example programs as few-shot input)\nModel Output:\nThis task requires creating a new Python function that takes an input and returns a value. The function should have at least one\ninput parameter, be deterministic, and require state tracking across multiple data transformations. The input should be a test case\nprovided to a subject for the purpose of deducing the output. The code and the output should be challenging enough to require\nalgorithmic reasoning without using restricted words or packages.\ndef f(prices: list[int], budget: int):\nn = len(prices)\nprofit = [0] * n\nfor i in range(1, n):\nfor j in range(i):\nif prices[i] > prices[j]:\nprofit[i] = max(profit[i], profit[j] + prices[i] - prices[j])\nreturn max(profit + [profit[-1] - prices[-1] if budget >= prices[-1] else 0])\nInput: [7, 1, 5, 3, 6, 4], 7\n\nTask: Propose Deduction Task\n\nTask: Propose Deduction Task\n\nModel Input: (k random selected example programs as few-shot input)\n\n(k random selected example programs as few-shot input)\n\nModel Input:\n\nModel Output:\n\nModel Output:\n\nThis task requires creating a new Python function that takes an input and returns a value. The function should have at least one\n\nThis task requires creating a new Python function that takes an input and returns a value. The function should have at least one\n\ninput parameter, be deterministic, and require state tracking across multiple data transformations. The input should be a test case\n\ninput parameter, be deterministic, and require state tracking across multiple data transformations. The input should be a test case\n\nprovided to a subject for the purpose of deducing the output. The code and the output should be challenging enough to require\n\nprovided to a subject for the purpose of deducing the output. The code and the output should be challenging enough to require\n\nalgorithmic reasoning without using restricted words or packages.\n\nalgorithmic reasoning without using restricted words or packages.\n\ndef f(prices: list[int], budget: int):\n\ndef\n\nf(prices:\n\nlist[int], budget:\n\nint):\n\nn = len(prices)\n\nn\n\n=\n\nlen(prices)\n\nprofit = [0] * n\n\nprofit\n\n=\n\n[0]\n\n*\n\nn\n\nfor i in range(1, n):\n\nfor\n\nin\n\ni\n\nrange(1, n):\n\nfor j in range(i):\n\nfor\n\nin\n\nj\n\nrange(i):\n\nif prices[i] > prices[j]:\n\nif\n\nprices[i]\n\n>\n\nprices[j]:\n\nprofit[i] = max(profit[i], profit[j] + prices[i] - prices[j])\n\nprofit[i]\n\n=\n\nmax(profit[i], profit[j]\n\n+\n\nprices[i]\n\n-\n\nprices[j])\n\nreturn max(profit + [profit[-1] - prices[-1] if budget >= prices[-1] else 0])\n\nreturn\n\nif\n\nelse\n\nmax(profit\n\n+\n\n[profit[-1]\n\n-\n\nprices[-1]\n\nbudget\n\n>=\n\nprices[-1]\n\n0])\n\nInput: [7, 1, 5, 3, 6, 4], 7\n\nInput:\n\n[7, 1, 5, 3, 6, 4], 7\n\nFigure 18. An example response of proposing a deduction task. k programs are selected as few-shot examples to the model. The generated\nnew program will then be verifed through execution.\nFigure 18. Propose Deduction Task Example. An example response of proposing a deduction task. k programs are selected as few-shot\nexamples to the model. The generated new program will then be verified through execution.\n\nFigure 18. Propose Deduction Task Example. An example response of proposing a deduction task. k programs are selected as few-shot\n\nAn example response of proposing a deduction task. k programs are selected as few-shot\n\nPropose Deduction Task Example.\n\nFigure 18.\n\nFigure 18. An example response of proposing a deduction task. k programs are selected as few-shot examples to the model. The generated\n\nFigure 18.\n\nAn example response of proposing a deduction task. k programs are selected as few-shot examples to the model. The generated\n\nexamples to the model. The generated new program will then be verified through execution.\n\nexamples to the model. The generated new program will then be verified through execution.\n\nnew program will then be verifed through execution.\n\nnew program will then be verifed through execution.\n\n24\n25\n\n25\n\n25\n\n24\n\n24\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\ndef f(numbers):\n# Step 1: Filter out even numbers\nfiltered_numbers = [num for num in numbers if num % 2 != 0]\n# Step 2: Calculate the sum of the remaining odd numbers\nsum_of_odd_numbers = sum(filtered_numbers)\n# Step 3: Reverse the order of the remaining odd numbers\nreversed_odd_numbers = filtered_numbers[::-1]\n# Step 4: Calculate the product of the reversed odd numbers\nproduct_of_reversed_odd_numbers = 1\nfor num in reversed_odd_numbers:\nproduct_of_reversed_odd_numbers *= num\n# Step 5: Calculate the sum of the digits of the product\nsum_of_digits_of_product = sum(int(digit) for digit in str(product_of_reversed_odd_numbers))\n# Step 6: Modify the original list by adding the sum of the digits to each even number\n# and subtracting it from each odd number\nmodified_numbers = []\nfor num in numbers:\nif num % 2 == 0:\nmodified_numbers.append(num + sum_of_digits_of_product)\nelse:\nmodified_numbers.append(num - sum_of_digits_of_product)\n# Step 7: Return the modified list\nreturn modified_numbers\n\ndef f(numbers):\n\ndef\n\nf(numbers):\n\n# Step 1: Filter out even numbers\n\n# Step 1: Filter out even numbers\n\nfiltered_numbers = [num for num in numbers if num % 2 != 0]\n\nfor\n\nin\n\nif\n\nfiltered_numbers\n\n=\n\n[num\n\nnum\n\nnumbers\n\nnum\n\n%\n\n2\n\n!=\n\n0]\n\n# Step 2: Calculate the sum of the remaining odd numbers\n\n# Step 2: Calculate the sum of the remaining odd numbers\n\nsum_of_odd_numbers = sum(filtered_numbers)\n\nsum_of_odd_numbers\n\n=\n\nsum(filtered_numbers)\n\n# Step 3: Reverse the order of the remaining odd numbers\n\n# Step 3: Reverse the order of the remaining odd numbers\n\nreversed_odd_numbers = filtered_numbers[::-1]\n\nreversed_odd_numbers\n\n=\n\nfiltered_numbers[::-1]\n\n# Step 4: Calculate the product of the reversed odd numbers\n\n# Step 4: Calculate the product of the reversed odd numbers\n\nproduct_of_reversed_odd_numbers = 1\n\nproduct_of_reversed_odd_numbers\n\n=\n\n1\n\nfor num in reversed_odd_numbers:\n\nfor\n\nin\n\nnum\n\nreversed_odd_numbers:\n\nproduct_of_reversed_odd_numbers *= num\n\nproduct_of_reversed_odd_numbers\n\n*=\n\nnum\n\n# Step 5: Calculate the sum of the digits of the product\n\n# Step 5: Calculate the sum of the digits of the product\n\nsum_of_digits_of_product = sum(int(digit) for digit in str(product_of_reversed_odd_numbers))\n\nfor\n\nin\n\nsum_of_digits_of_product\n\n=\n\nsum(int(digit)\n\ndigit\n\nstr(product_of_reversed_odd_numbers))\n\n# Step 6: Modify the original list by adding the sum of the digits to each even number\n\n# Step 6: Modify the original list by adding the sum of the digits to each even number\n\n# and subtracting it from each odd number\n\n# and subtracting it from each odd number\n\nmodified_numbers = []\n\nmodified_numbers\n\n=\n\n[]\n\nfor num in numbers:\n\nfor\n\nin\n\nnum\n\nnumbers:\n\nif num % 2 == 0:\n\nif\n\nnum\n\n%\n\n2\n\n==\n\n0:\n\nmodified_numbers.append(num + sum_of_digits_of_product)\n\nmodified_numbers.append(num\n\n+\n\nsum_of_digits_of_product)\n\nelse:\n\nelse\n\n:\n\nmodified_numbers.append(num - sum_of_digits_of_product)\n\nmodified_numbers.append(num\n\n-\n\nsum_of_digits_of_product)\n\n# Step 7: Return the modified list\n\n# Step 7: Return the modified list\n\nreturn modified_numbers\n\nreturn\n\nmodified_numbers\n\nFigure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed\nin much bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).\nFigure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed in\nmuch bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).\n\nFigure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\n\nWe observe that even after outputting the CoT and during answering when the\n\nLearned \"ReAct\" style program generation.\n\nFigure 19.\n\nFigure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\n\nFigure 19.\n\nWe observe that even after outputting the CoT and during answering when the\n\nLearned \"ReAct\" style program generation.\n\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\n\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\n\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\n\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\n\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed in\n\nhere for induction task output by\n\nduring training. Similar observations are recently observed in\n\nAbsolute Zero Reasoner-Coder-14b\n\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed\n\nhere for induction task output by\n\nduring training. Similar observations are recently observed\n\nAbsolute Zero Reasoner-Coder-14b\n\nmuch bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).\n\nmuch bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al.,\n\n2025)\n\n.\n\nin much bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).\n\nin much bigger models (671b) for math proofs with DeepSeek Prover\n\nV2 (Ren et a\n\nl.,\n\n2025).\n\n25\n26\n\n26\n\n26\n\n25\n\n25\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Propose Abduction Task\n\nTask: Propose Abduction Task\n\nTask: Propose Abduction Task\n\nModel Input: (k random selected example programs as few-shot input)\n\nModel Input: (k random selected example programs as few-shot input)\n\n(k random selected example programs as few-shot input)\n\nModel Input:\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nFor this task, we need an intelligent problem that requires deep algorithmic reasoning and demands long multi step reasoning. A\ntypical type of problem that fts these requirements is the Matrix Reconstruction from a given jumbled li st of elements, where\nwe have to fgure out the matrix's dimensions and individual values. However, this task will involve multiple steps, increasing its\ncomplexity and creativity. We will create a function 'f' that transforms a jumbled list of elements into a two-dimensional matrix,\nthen back to a list, given a specifc reconstruction pattern. We will retrieve the original list from the reconstructed matrix. Here,\nwe will avoid random and datetime operations to ensure determinism. The function will be structured to reconstruct the matrix\nelements in a unique and challenging way. We will provide an input that, after reconstruction, presents a unique list that can be\neasily deduced from the given output in a test piece.\n\nFor this task, we need an intelligent problem that requires deep algorithmic reasoning and demands long multi step reasoning. A\n\nFor this task, we need an intelligent problem that requires deep algorithmic reasoning and demands long multi step reasoning. A\n\ntypical type of problem that fts these requirements is the Matrix Reconstruction from a given jumbled li st of elements, where\n\ntypical type of problem that fts these requirements is the Matrix Reconstruction from a given jumbled li st of elements, where\n\nwe have to fgure out the matrix's dimensions and individual values. However, this task will involve multiple steps, increasing its\n\nwe have to fgure out the matrix's dimensions and individual values. However, this task will involve multiple steps, increasing its\n\ncomplexity and creativity. We will create a function 'f' that transforms a jumbled list of elements into a two-dimensional matrix,\n\ncomplexity and creativity. We will create a function 'f' that transforms a jumbled list of elements into a two-dimensional matrix,\n\nthen back to a list, given a specifc reconstruction pattern. We will retrieve the original list from the reconstructed matrix. Here,\n\nthen back to a list, given a specifc reconstruction pattern. We will retrieve the original list from the reconstructed matrix. Here,\n\nwe will avoid random and datetime operations to ensure determinism. The function will be structured to reconstruct the matrix\n\nwe will avoid random and datetime operations to ensure determinism. The function will be structured to reconstruct the matrix\n\nelements in a unique and challenging way. We will provide an input that, after reconstruction, presents a unique list that can be\n\nelements in a unique and challenging way. We will provide an input that, after reconstruction, presents a unique list that can be\n\neasily deduced from the given output in a test piece.\n\neasily deduced from the given output in a test piece.\n\ndef f(jumbled_elements: list) -> list:\nif not jumbled_elements:\nreturn []\n# Calculate the square root of the length of elements to estimate the matrix dimensions\nn = len(jumbled_elements)\nsqrt_n = int(n ** 0.5)\nif n > sqrt_n ** 2:\nsqrt_n += 1\nif n < sqrt_n ** 2:\nsqrt_n -= 1\n# Now, we need to reconstruct the matrix elements in a challenging way using specific patterns\n# For this, we can reconstruct the matrix using zigzag patterns or hill-climbing patterns\n# Here, a hill-climbing pattern is used where we traverse the matrix from the extreme\n# right and downward Then traverse the matrix from the second to last element to the\n# extreme bottom. Repeat the process until the entire matrix is traversed.\n# Initialize the matrix\nmatrix = [[0 for _ in range(sqrt_n)] for _ in range(sqrt_n)]\ncur_idx = 0\n# Fill the matrix in a zigzag pattern\nfor i in range(sqrt_n):\nif i % 2 == 0:\nfor j in range(sqrt_n):\nmatrix[j][i] = j * sqrt_n + i + cur_idx\ncur_idx += 1\nelse:\nfor j in range(sqrt_n - 1, -1, -1):\nmatrix[j][i] = j * sqrt_n + n - i - cur_idx - 1\ncur_idx += 1\n# Reconstruct the list into a matrix by repeating the process\nreconstructed_list = []\nfor i in range(sqrt_n):\nfor j in range(sqrt_n):\nreconstructed_list.append(matrix[i][j])\n# Now, we need to calculate another matrix based on a function F applied to the current matrix\n# F is a mathematical function that can be complex, such as factorial, prime check,\n# or another transformation For example, transform each element by adding 1 if it's prime,\n# subtracting 1 if it's even, or leaving it unchanged, Implement any specific transformation\n# function here. Here, for demonstration, each element is left unchanged\ntransformed_result = reconstructed_list\nreturn transformed_result\nOutput: [0, 11, 10, 1, 5, 16, 15, 6, 10, 21, 20, 11, 15, 26, 25, 16]\n\ndef f(jumbled_elements: list) -> list:\n\ndef\n\nf(jumbled_elements:\n\nlist)\n\n->\n\nlist:\n\nif not jumbled_elements:\n\nif\n\nnot\n\njumbled_elements:\n\nreturn []\n\nreturn\n\n[]\n\n# Calculate the square root of the length of elements to estimate the matrix dimensions\n\n# Calculate the square root of the length of elements to estimate the matrix dimensions\n\nn = len(jumbled_elements)\n\nn\n\n=\n\nlen(jumbled_elements)\n\nsqrt_n = int(n ** 0.5)\n\nsqrt_n\n\n=\n\nint(n\n\n**\n\n0.5)\n\nif n > sqrt_n ** 2:\n\nif\n\nn\n\n>\n\nsqrt_n\n\n**\n\n2:\n\nsqrt_n += 1\n\nsqrt_n\n\n+=\n\n1\n\nif n < sqrt_n ** 2:\n\nif\n\nn\n\n<\n\nsqrt_n\n\n**\n\n2:\n\nsqrt_n -= 1\n\nsqrt_n\n\n-=\n\n1\n\n# Now, we need to reconstruct the matrix elements in a challenging way using specific patterns\n\n# Now, we need to reconstruct the matrix elements in a challenging way using specific patterns\n\n# For this, we can reconstruct the matrix using zigzag patterns or hill-climbing patterns\n\n# For this, we can reconstruct the matrix using zigzag patterns or hill-climbing patterns\n\n# Here, a hill-climbing pattern is used where we traverse the matrix from the extreme\n\n# Here, a hill-climbing pattern is used where we traverse the matrix from the extreme\n\n# right and downward Then traverse the matrix from the second to last element to the\n\n# right and downward Then traverse the matrix from the second to last element to the\n\n# extreme bottom. Repeat the process until the entire matrix is traversed.\n\n# extreme bottom. Repeat the process until the entire matrix is traversed.\n\n# Initialize the matrix\n\n# Initialize the matrix\n\nmatrix = [[0 for _ in range(sqrt_n)] for _ in range(sqrt_n)]\n\nfor\n\nin\n\nfor\n\nin\n\nmatrix\n\n=\n\n[[0\n\n_\n\nrange(sqrt_n)]\n\n_\n\nrange(sqrt_n)]\n\ncur_idx = 0\n\ncur_idx\n\n=\n\n0\n\n# Fill the matrix in a zigzag pattern\n\n# Fill the matrix in a zigzag pattern\n\nfor i in range(sqrt_n):\n\nfor\n\nin\n\ni\n\nrange(sqrt_n):\n\nif i % 2 == 0:\n\nif\n\ni\n\n%\n\n2\n\n==\n\n0:\n\nfor j in range(sqrt_n):\n\nfor\n\nin\n\nj\n\nrange(sqrt_n):\n\nmatrix[j][i] = j * sqrt_n + i + cur_idx\n\nmatrix[j][i]\n\n=\n\nj\n\n*\n\nsqrt_n\n\n+\n\ni\n\n+\n\ncur_idx\n\ncur_idx += 1\n\ncur_idx\n\n+=\n\n1\n\nelse:\n\nelse\n\n:\n\nfor j in range(sqrt_n - 1, -1, -1):\n\nfor\n\nin\n\nj\n\nrange(sqrt_n\n\n-\n\n1,\n\n-1,\n\n-1):\n\nmatrix[j][i] = j * sqrt_n + n - i - cur_idx - 1\n\nmatrix[j][i]\n\n=\n\nj\n\n*\n\nsqrt_n\n\n+\n\nn\n\n-\n\ni\n\n-\n\ncur_idx\n\n-\n\n1\n\ncur_idx += 1\n\ncur_idx\n\n+=\n\n1\n\n# Reconstruct the list into a matrix by repeating the process\n\n# Reconstruct the list into a matrix by repeating the process\n\nreconstructed_list = []\n\nreconstructed_list\n\n=\n\n[]\n\nfor i in range(sqrt_n):\n\nfor\n\nin\n\ni\n\nrange(sqrt_n):\n\nfor j in range(sqrt_n):\n\nfor\n\nin\n\nj\n\nrange(sqrt_n):\n\nreconstructed_list.append(matrix[i][j])\n\nreconstructed_list.append(matrix[i][j])\n\n# Now, we need to calculate another matrix based on a function F applied to the current matrix\n\n# Now, we need to calculate another matrix based on a function F applied to the current matrix\n\n# F is a mathematical function that can be complex, such as factorial, prime check,\n\n# F is a mathematical function that can be complex, such as factorial, prime check,\n\n# or another transformation For example, transform each element by adding 1 if it's prime,\n\n# or another transformation For example, transform each element by adding 1 if it's prime,\n\n# subtracting 1 if it's even, or leaving it unchanged, Implement any specific transformation\n\n# subtracting 1 if it's even, or leaving it unchanged, Implement any specific transformation\n\n# function here. Here, for demonstration, each element is left unchanged\n\n# function here. Here, for demonstration, each element is left unchanged\n\ntransformed_result = reconstructed_list\n\ntransformed_result\n\n=\n\nreconstructed_list\n\nreturn transformed_result\n\nreturn\n\ntransformed_result\n\nOutput: [0, 11, 10, 1, 5, 16, 15, 6, 10, 21, 20, 11, 15, 26, 25, 16]\n\nOutput:\n\n[0, 11, 10, 1, 5, 16, 15, 6, 10, 21, 20, 11, 15, 26, 25, 16]\n\nFigure 20. An example response of proposing an abduction task. The model uses comments as reasoning when generating the program.\n26\nFigure 20. Propose Abduction Task Example. An example response of proposing an abduction task. The model uses comments as\nreasoning when generating the program.\n\nFigure 20. An example response of proposing an abduction task. The model uses comments as reasoning when generating the program.\n\nFigure 20.\n\nAn example response of proposing an abduction task. The model uses comments as reasoning when generating the program.\n\nFigure 20. Propose Abduction Task Example. An example response of proposing an abduction task. The model uses comments as\n\nAn example response of proposing an abduction task. The model uses comments as\n\nPropose Abduction Task Example.\n\nFigure 20.\n\nreasoning when generating the program.\n\nreasoning when generating the program.\n\n26\n\n26\n\n27\n\n27\n\n27\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Propose Induction Task\n\nTask: Propose Induction Task\n\nTask: Propose Induction Task\n\nModel Input: Here is the function f. Analyze its behaviour and relate it to the observed inputs/outputs.\n\nModel Input: Here is the function f. Analyze its behaviour and relate it to the observed inputs/outputs.\n\nHere is the function\n\n. Analyze its behaviour and relate it to the observed inputs/outputs.\n\nModel Input:\n\nf\n\ndef f(numbers: list[int]) -> int:\nreplacement = {\n0: 2,\n1: 3,\n2: 1,\n3: 0,\n4: 4\n}\nreturn sum(replacement[num] for num in numbers)\n\ndef f(numbers: list[int]) -> int:\n\ndef\n\nf(numbers:\n\nlist[int])\n\n->\n\nint:\n\nreplacement = {\n\nreplacement\n\n=\n\n{\n\n0: 2,\n\n0:\n\n2,\n\n1: 3,\n\n1:\n\n3,\n\n2: 1,\n\n2:\n\n1,\n\n3: 0,\n\n3:\n\n0,\n\n4: 4\n\n4:\n\n4\n\n}\n\n}\n\nreturn sum(replacement[num] for num in numbers)\n\nreturn\n\nfor\n\nin\n\nsum(replacement[num]\n\nnum\n\nnumbers)\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nTo solve this task, we need to come up with 10 diferent inputs for the function f(numbers: list[int]) -> int. The function\nreplaces each number in the list according to a specifc dictionary and then sums up the replaced numbers. Let's analyze the\ndictionary:\n\nTo solve this task, we need to come up with 10 diferent inputs for the function f(numbers: list[int]) -> int. The function\n\nTo solve this task, we need to come up with 10 diferent inputs for the function\n\n. The function\n\nf(numbers: list[int]) -> int\n\nreplaces each number in the list according to a specifc dictionary and then sums up the replaced numbers. Let's analyze the\n\nreplaces each number in the list according to a specifc dictionary and then sums up the replaced numbers. Let's analyze the\n\ndictionary:\n\ndictionary:\n\nThis dictionary tells us that:\n\nThis dictionary tells us that:\n\nThis dictionary tells us that:\n\n-0 becomes 2\n\n-0 becomes 2\n\n0\n\n2\n\n-\n\nbecomes\n\n-1 becomes 3\n\n-1 becomes 3\n\n1\n\n3\n\n-\n\nbecomes\n\n-2 becomes 1\n\n-2 becomes 1\n\n2\n\n1\n\n-\n\nbecomes\n\n-3 becomes 0\n\n-3 becomes 0\n\n3\n\n0\n\n-\n\nbecomes\n\n-4 remains 4\n\n-4 remains 4\n\n4\n\n4\n\n-\n\nremains\n\nTo create diverse outputs, we should include lists with diferent sets of numbers from the list {0, 1, 2, 3, 4}. We also need\nto ensure that the inputs are valid (i.e., lists of integers).\n\nTo create diverse outputs, we should include lists with diferent sets of numbers from the list {0, 1, 2, 3, 4}. We also need\n\n0\n\n1\n\n2\n\n3\n\n4\n\nTo create diverse outputs, we should include lists with diferent sets of numbers from the list\n\n. We also need\n\n,\n\n,\n\n,\n\n,\n\n{\n\n}\n\nto ensure that the inputs are valid (i.e., lists of integers).\n\nto ensure that the inputs are valid (i.e., lists of integers).\n\nLet's illustrate using provided examples step-by-step:\n\nLet's illustrate using provided examples step-by-step:\n\nLet's illustrate using provided examples step-by-step:\n\n1. Input: [0] \u21d2 Replacement: [2] \u21d2 sum = 2\n2. Input: [1, 2, 3] \u21d2 Replacement: [3, 1, 0] \u21d2 sum = 4\n3. Input: [4, 1] \u21d2 Replacement: [4, 3] \u21d2 sum = 7\n4. Input: [2, 1, 0] \u21d2 Replacement: [1, 3, 2] \u21d2 sum = 6\n5. Input: [3, 0, 2, 4] \u21d2 Replacement: [0, 2, 1, 4] \u21d2 sum = 7\n6. Input: [1, 1, 1] \u21d2 Replacement: [3, 3, 3] \u21d2 sum = 9\n7. Input: [2, 2, 0, 1] \u21d2 Replacement: [1, 1, 2, 3] \u21d2 sum = 7\n8. Input: [3, 4, 2] \u21d2 Replacement: [0, 4, 1] \u21d2 sum = 5\n9. Input: [0, 1, 2, 3, 4] \u21d2 Replacement: [2, 3, 1, 0, 4] \u21d2 sum = 10\n10. Input: [1, 2, 3, 0, 4] \u21d2 Replacement: [3, 1, 0, 2, 4] \u21d2 sum = 10\nThus, the rule is clearly replacing numbers according to the given mapping and then summing the results.\nInputs: ['[0]', '[1, 2, 3]', '[4, 1]', '[2, 1, 0]', '[3, 0, 2, 4]', '[1, 1, 1]',\n[2, 2, 0, 1], '[3, 4, 2]', '[0, 1, 2, 3, 4]', '[1, 2, 3, 0, 4]']|\nMessage: You are given a function that takes a list of integers as input and returns a single integer as output. Can you determine\nthe rule this function is following by analyzing the outputs for the given inputs?\n(After execution:) Outputs: ['2', '4', '7', '6', '7', '9', '7', '5', '10', '10']\n\n1. Input: [0] \u21d2 Replacement: [2] \u21d2 sum = 2\n\n[2]\n\n= 2\n\n1. Input:\n\nReplacement:\n\nsum\n\n[0]\n\n\u21d2\n\n\u21d2\n\n2. Input: [1, 2, 3] \u21d2 Replacement: [3, 1, 0] \u21d2 sum = 4\n\n[3\n\n1\n\n0]\n\n= 4\n\n2. Input:\n\nReplacement:\n\nsum\n\n[1, 2, 3]\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n3. Input: [4, 1] \u21d2 Replacement: [4, 3] \u21d2 sum = 7\n\n[4\n\n3]\n\n= 7\n\n3. Input:\n\nReplacement:\n\nsum\n\n[4, 1]\n\n,\n\n\u21d2\n\n\u21d2\n\n4. Input: [2, 1, 0] \u21d2 Replacement: [1, 3, 2] \u21d2 sum = 6\n\n[1\n\n3\n\n2]\n\n= 6\n\n4. Input:\n\nReplacement:\n\nsum\n\n[2, 1, 0]\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n5. Input: [3, 0, 2, 4] \u21d2 Replacement: [0, 2, 1, 4] \u21d2 sum = 7\n\n[0\n\n2\n\n1\n\n4]\n\n= 7\n\n5. Input:\n\nReplacement:\n\nsum\n\n[3, 0, 2, 4]\n\n,\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n6. Input: [1, 1, 1] \u21d2 Replacement: [3, 3, 3] \u21d2 sum = 9\n\n[3\n\n3\n\n3]\n\n= 9\n\n6. Input:\n\nReplacement:\n\nsum\n\n[1, 1, 1]\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n7. Input: [2, 2, 0, 1] \u21d2 Replacement: [1, 1, 2, 3] \u21d2 sum = 7\n\n[1\n\n1\n\n2\n\n3]\n\n= 7\n\n7. Input:\n\nReplacement:\n\nsum\n\n[2, 2, 0, 1]\n\n,\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n8. Input: [3, 4, 2] \u21d2 Replacement: [0, 4, 1] \u21d2 sum = 5\n\n[0\n\n4\n\n1]\n\n= 5\n\n8. Input:\n\nReplacement:\n\nsum\n\n[3, 4, 2]\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n9. Input: [0, 1, 2, 3, 4] \u21d2 Replacement: [2, 3, 1, 0, 4] \u21d2 sum = 10\n\n[2\n\n3\n\n1\n\n0\n\n4]\n\n= 10\n\n9. Input:\n\nReplacement:\n\nsum\n\n[0, 1, 2, 3, 4]\n\n,\n\n,\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n10. Input: [1, 2, 3, 0, 4] \u21d2 Replacement: [3, 1, 0, 2, 4] \u21d2 sum = 10\n\n[3\n\n1\n\n0\n\n2\n\n4]\n\n= 10\n\n10. Input:\n\nReplacement:\n\nsum\n\n[1, 2, 3, 0, 4]\n\n,\n\n,\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\nThus, the rule is clearly replacing numbers according to the given mapping and then summing the results.\n\nThus, the rule is clearly replacing numbers according to the given mapping and then summing the results.\n\nInputs: ['[0]', '[1, 2, 3]', '[4, 1]', '[2, 1, 0]', '[3, 0, 2, 4]', '[1, 1, 1]',\n\n:\n\nInputs\n\n['[0]', '[1, 2, 3]', '[4, 1]', '[2, 1, 0]', '[3, 0, 2, 4]', '[1, 1, 1]',\n\n[2, 2, 0, 1], '[3, 4, 2]', '[0, 1, 2, 3, 4]', '[1, 2, 3, 0, 4]']|\n\n, '[3, 4, 2]', '[0, 1, 2, 3, 4]', '[1, 2, 3, 0, 4]']|\n\n[2, 2, 0, 1]\n\nMessage: You are given a function that takes a list of integers as input and returns a single integer as output. Can you determine\n\n: You are given a function that takes a list of integers as input and returns a single integer as output. Can you determine\n\nMessage\n\nthe rule this function is following by analyzing the outputs for the given inputs?\n\nthe rule this function is following by analyzing the outputs for the given inputs?\n\n(After execution:) Outputs: ['2', '4', '7', '6', '7', '9', '7', '5', '10', '10']\n\n(After execution:)\n\n:\n\nOutputs\n\n['2', '4', '7', '6', '7', '9', '7', '5', '10', '10']\n\nFigure 21. An example response of proposing an induction task. Given a random selected program, the model needs to propose 10 inputs\nand a description of program. These inputs will be executed and verifed to get outputs. At solve phase, the model need to generate the\nprogram given the message and 5 input/output pairs (half is hidden cases)\nFigure 21. Propose Induction Task Example. An example response of proposing an induction task. Given a random selected program,\nthe model needs to propose 10 inputs and a description of program. These inputs will be executed and verified to get outputs. At solve\nphase, the model need to generate the program given the message and 5 input/output pairs (half is hidden cases)\n\nFigure 21. An example response of proposing an induction task. Given a random selected program, the model needs to propose 10 inputs\n\nFigure 21.\n\nAn example response of proposing an induction task. Given a random selected program, the model needs to propose 10 inputs\n\nFigure 21. Propose Induction Task Example. An example response of proposing an induction task. Given a random selected program,\n\nAn example response of proposing an induction task. Given a random selected program,\n\nPropose Induction Task Example.\n\nFigure 21.\n\nand a description of program. These inputs will be executed and verifed to get outputs. At solve phase, the model need to generate the\n\nand a description of program. These inputs will be executed and verifed to get outputs. At solve phase, the model need to generate the\n\nthe model needs to propose 10 inputs and a description of program. These inputs will be executed and verified to get outputs. At solve\n\nthe model needs to propose 10 inputs and a description of program. These inputs will be executed and verified to get outputs. At solve\n\nprogram given the message and 5 input/output pairs (half is hidden cases)\n\nprogram given the message and 5 input/output pairs (half is hidden cases)\n\nphase, the model need to generate the program given the message and 5 input/output pairs (half is hidden cases)\n\nphase, the model need to generate the program given the message and 5 input/output pairs (half is hidden cases)\n\n27\n28\n\n27\n\n27\n\n28\n\n28\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Solve Abduction Task\n\nTask: Solve Abduction Task\n\nTask: Solve Abduction Task\n\nModel Input: Here is the function f and the output: '1', please predict the input.\n\nModel Input: Here is the function f and the output: '1', please predict the input.\n\nHere is the function\n\nand the output: '1', please predict the input.\n\nModel Input:\n\nf\n\ndef f(numbers: list[int], target: int) -> int:\naccumulated_values = []\ncurrent_sum = 0\nfor number in numbers:\ncurrent_sum += number\naccumulated_values.append(current_sum)\nresult = 0\nfor i in range(len(accumulated_values)):\nfor j in range(i + 1, len(accumulated_values)):\nif accumulated_values[j] \\\n- accumulated_values[i] == target:\nresult = accumulated_values[i]\nreturn result\nreturn result\n\ndef f(numbers: list[int], target: int) -> int:\n\ndef\n\nf(numbers:\n\nlist[int], target:\n\nint)\n\n->\n\nint:\n\naccumulated_values = []\n\naccumulated_values\n\n=\n\n[]\n\ncurrent_sum = 0\n\ncurrent_sum\n\n=\n\n0\n\nfor number in numbers:\n\nfor\n\nin\n\nnumber\n\nnumbers:\n\ncurrent_sum += number\n\ncurrent_sum\n\n+=\n\nnumber\n\naccumulated_values.append(current_sum)\n\naccumulated_values.append(current_sum)\n\nresult = 0\n\nresult\n\n=\n\n0\n\nfor i in range(len(accumulated_values)):\n\nfor\n\nin\n\ni\n\nrange(len(accumulated_values)):\n\nfor j in range(i + 1, len(accumulated_values)):\n\nfor\n\nin\n\nj\n\nrange(i\n\n+\n\n1,\n\nlen(accumulated_values)):\n\nif accumulated_values[j] \\\n\nif\n\naccumulated_values[j] \\\n\n- accumulated_values[i] == target:\n\n-\n\naccumulated_values[i]\n\n==\n\ntarget:\n\nresult = accumulated_values[i]\n\nresult\n\n=\n\naccumulated_values[i]\n\nreturn result\n\nreturn\n\nresult\n\nreturn result\n\nreturn\n\nresult\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\nsatisfes this condition is 1.\nLet's consider a simple example:\n-numbers = [1, 2]\n-target = 1\nHere's the step-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\nConsider:\n-numbers = [1, 0]\n-target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n...(shortened)\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n-numbers = [2, -1, 1]\n-target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nsatisfes this condition is 1.\n\nsatisfes this condition is 1.\n\nLet's consider a simple example:\n\nLet's consider a simple example:\n\n-numbers = [1, 2]\n\n-numbers = [1, 2]\n\n-target = 1\n\n-target = 1\n\nHere's the step-by-step calculation:\n\nHere's the step-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nConsider:\n\nConsider:\n\n-numbers = [1, 0]\n\n-numbers = [1, 0]\n\n-target = 1\n\n-target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n...(shortened)\n\n...(shortened)\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\n-numbers = [2, -1, 1]\n\n-numbers = [2, -1, 1]\n\n-target = 1\n\n-target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nFigure 22. A more detailed version of the example shown in main Figure 7. The model is solving an abduction task (predict input).\nFigure 22. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\nabduction task (predict input).\n\nFigure 22. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\n\nA more detailed version of the example shown in main Figure\n\n7. The model is solving an\n\nFigure 22. A more detailed version of the example shown in main Figure 7. The model is solving an abduction task (predict input).\n\nFigure 22.\n\nA more detailed version of the example shown in main Figure\n\n7. The model is solving an abducti\n\non\n\ntask (predict input).\n\nSolve Abduction Task Example.\n\nFigure 22.\n\nabduction task (predict input).\n\nabduction task (predict input).\n\n28\n29\n\n28\n\n28\n\n29\n\n29\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Solve Deduction Task (Example 1)\n\nTask: Solve Deduction Task (Example 1)\n\nTask: Solve Deduction Task (Example 1)\n\nModel Input: Here is the function f, the input provided [1, 2, 1, 2, 3, 2, 1], 2, and we need to\npredict the output.\n\nModel Input: Here is the function f, the input provided [1, 2, 1, 2, 3, 2, 1], 2, and we need to\n\nHere is the function\n\n, the input provided\n\n, and we need to\n\nModel Input:\n\nf\n\n[1, 2, 1, 2, 3, 2, 1], 2\n\npredict the output.\n\npredict the output.\n\ndef f(array: list[int], k: int) -> int:\nwindow_start = 0\nmax_length = 0\nchar_freq = {}\nfor window_end in range(len(array)):\nend_char = array[window_end]\nif end_char not in char_freq:\nchar_freq[end_char] = 0\nchar_freq[end_char] += 1\nwhile len(char_freq) > k:\nstart_char = array[window_start]\nchar_freq[start_char] -= 1\nif char_freq[start_char] == 0:\ndel char_freq[start_char]\nwindow_start += 1\nmax_length = max(max_length, window_end - window_start + 1)\nreturn max_length\nInput: [1, 2, 1, 2, 3, 2, 1], 2\nModel Output:\nBased on the given code snippet, we need to determine the output of the function f(array, k) when provided with the input [1, 2, 1, 2, 3, 2, 1] and k = 2.\nThe function f is designed to fnd the length of the longest subarray with at most k distinct characters. Let's go through the code step by step:\n1. Initialize window_start to 0, max_length to 0, and an empty dictionary char_freq to store the frequency of characters in the current window.\n2. Iterate through the array using window_end as the index:\na. Get the current character end_char at index window_end.\nb. If end_char is not in char_freq, add it with a frequency of 0.\nc. Increment the frequency of end_char by 1.\n3. Check if the number of distinct characters in char_freq is greater than k:\na. If true, shrink the window from the start:\ni. Get the character start_char at index window_start.\nii. Decrement its frequency by 1.\niii. If the frequency becomes 0, remove the character from char_freq.\niv. Increment window_start by 1.\n4. Update max_length by taking the maximum of max_length and the current window size (window_end - window_start + 1).\nNow, let's apply this algorithm step-by-step to the input [1, 2, 1, 2, 3, 2, 1], k = 2:\n\u2022 window_start = 0, max_length = 0, char_freq = {}\n\u2022 window_end = 0, add 1 to char_freq: {1:1}, window size = 1\n\u2022 window_end = 1, add 2: {1:1, 2:1}, window size = 2\n\u2022 window_end = 2, increment 1: {1:2, 2:1}, window size = 3\n\u2022 window_end = 3, increment 2: {1:2, 2:2}, window size = 4\n\u2022 window_end = 4, add 3: {1:2, 2:2, 3:1}; now distinct chars > 2, shrink window:\n- remove 1 at start: {1:1, 2:2, 3:1}\n- remove 2 at start: {1:1, 2:1, 3:1}\n- remove 1 again: {2:1, 3:1}, window now starts at 3\n\u2022 window_end = 5, increment 2: {2:2, 3:1}, window size = 3\n\u2022 window_end = 6, add 1: {2:2, 3:1, 1:1}, shrink again:\n- remove 2: {2:1, 3:1, 1:1}\n- remove 3: {2:1, 1:1}, window now starts at 5\n- window size = 2\nMaximum window size encountered is 4, thus max_length = 4.\nHence, the function returns 4.\n\ndef f(array: list[int], k: int) -> int:\n\ndef\n\nf(array:\n\nlist[int], k:\n\nint)\n\n->\n\nint:\n\nwindow_start = 0\n\nwindow_start\n\n=\n\n0\n\nmax_length = 0\n\nmax_length\n\n=\n\n0\n\nchar_freq = {}\n\nchar_freq\n\n=\n\n{}\n\nfor window_end in range(len(array)):\n\nfor\n\nin\n\nwindow_end\n\nrange(len(array)):\n\nend_char = array[window_end]\n\nend_char\n\n=\n\narray[window_end]\n\nif end_char not in char_freq:\n\nif\n\nnot\n\nin\n\nend_char\n\nchar_freq:\n\nchar_freq[end_char] = 0\n\nchar_freq[end_char]\n\n=\n\n0\n\nchar_freq[end_char] += 1\n\nchar_freq[end_char]\n\n+=\n\n1\n\nwhile len(char_freq) > k:\n\nwhile\n\nlen(char_freq)\n\n>\n\nk:\n\nstart_char = array[window_start]\n\n=\n\narray[window_start]\n\nstart_char\n\nchar_freq[start_char] -= 1\n\nchar_freq[start_char]\n\n-=\n\n1\n\nif char_freq[start_char] == 0:\n\nif\n\nchar_freq[start_char]\n\n==\n\n0:\n\ndel char_freq[start_char]\n\ndel\n\nchar_freq[start_char]\n\nwindow_start += 1\n\nwindow_start\n\n+=\n\n1\n\nmax_length = max(max_length, window_end - window_start + 1)\n\nmax_length\n\n=\n\nmax(max_length, window_end\n\n-\n\nwindow_start\n\n+\n\n1)\n\nreturn max_length\n\nreturn\n\nmax_length\n\nInput: [1, 2, 1, 2, 3, 2, 1], 2\n\nInput:\n\n[1, 2, 1, 2, 3, 2, 1], 2\n\nModel Output:\n\nModel Output:\n\nBased on the given code snippet, we need to determine the output of the function f(array, k) when provided with the input [1, 2, 1, 2, 3, 2, 1] and k = 2.\n\n= 2\n\nBased on the given code snippet, we need to determine the output of the function\n\nwhen provided with the input\n\nand\n\n.\n\nf(array, k)\n\n[1, 2, 1, 2, 3, 2, 1]\n\nk\n\nThe function f is designed to fnd the length of the longest subarray with at most k distinct characters. Let's go through the code step by step:\n\nThe function\n\nis designed to fnd the length of the longest subarray with at most\n\ndistinct characters. Let's go through the code step by step:\n\nf\n\nk\n\n1. Initialize window_start to 0, max_length to 0, and an empty dictionary char_freq to store the frequency of characters in the current window.\n\n1. Initialize\n\nto 0,\n\nto 0, and an empty dictionary\n\nto store the frequency of characters in the current window.\n\nwindow_start\n\nmax_length\n\nchar_freq\n\n2. Iterate through the array using window_end as the index:\n\n2. Iterate through the array using\n\nas the index:\n\nwindow_end\n\na. Get the current character end_char at index window_end.\n\na. Get the current character\n\nat index\n\n.\n\nend_char\n\nwindow_end\n\nb. If end_char is not in char_freq, add it with a frequency of 0.\n\nb. If\n\nis not in\n\n, add it with a frequency of 0.\n\nend_char\n\nchar_freq\n\nc. Increment the frequency of end_char by 1.\n\nc. Increment the frequency of\n\nby 1.\n\nend_char\n\n3. Check if the number of distinct characters in char_freq is greater than k:\n\n3. Check if the number of distinct characters in\n\nis greater than\n\n:\n\nchar_freq\n\nk\n\na. If true, shrink the window from the start:\n\na. If true, shrink the window from the start:\n\ni. Get the character start_char at index window_start.\n\ni. Get the character\n\nat index\n\n.\n\nstart_char\n\nwindow_start\n\nii. Decrement its frequency by 1.\n\nii. Decrement its frequency by 1.\n\niii. If the frequency becomes 0, remove the character from char_freq.\n\niii. If the frequency becomes 0, remove the character from\n\n.\n\nchar_freq\n\niv. Increment window_start by 1.\n\niv. Increment\n\nby 1.\n\nwindow_start\n\n4. Update max_length by taking the maximum of max_length and the current window size (window_end - window_start + 1).\n\n4. Update\n\nby taking the maximum of\n\nand the current window size (\n\n).\n\nmax_length\n\nmax_length\n\nwindow_end - window_start + 1\n\nNow, let's apply this algorithm step-by-step to the input [1, 2, 1, 2, 3, 2, 1], k = 2:\n\n= 2\n\nNow, let's apply this algorithm step-by-step to the input\n\n,\n\n:\n\n[1, 2, 1, 2, 3, 2, 1]\n\nk\n\n\u2022 window_start = 0, max_length = 0, char_freq = {}\n\n\u2022\n\n,\n\n,\n\nwindow_start = 0\n\nmax_length = 0\n\nchar_freq = {}\n\n\u2022 window_end = 0, add 1 to char_freq: {1:1}, window size = 1\n\n1\n\n1:1\n\n= 1\n\n\u2022\n\n, add\n\nto\n\n:\n\n, window size\n\nwindow_end = 0\n\nchar_freq\n\n{\n\n}\n\n\u2022 window_end = 1, add 2: {1:1, 2:1}, window size = 2\n\n2\n\n1:1\n\n2:1\n\n= 2\n\n\u2022\n\n, add\n\n:\n\n, window size\n\nwindow_end = 1\n\n,\n\n{\n\n}\n\n\u2022 window_end = 2, increment 1: {1:2, 2:1}, window size = 3\n\n1\n\n1:2\n\n2:1\n\n= 3\n\n\u2022\n\n, increment\n\n:\n\n, window size\n\nwindow_end = 2\n\n,\n\n{\n\n}\n\n\u2022 window_end = 3, increment 2: {1:2, 2:2}, window size = 4\n\n2\n\n1:2\n\n2:2\n\n= 4\n\n\u2022\n\n, increment\n\n:\n\n, window size\n\nwindow_end = 3\n\n,\n\n{\n\n}\n\n\u2022 window_end = 4, add 3: {1:2, 2:2, 3:1}; now distinct chars > 2, shrink window:\n\n3\n\n1:2\n\n2:2\n\n3:1\n\n2\n\n\u2022\n\n, add\n\n:\n\n; now distinct chars\n\n, shrink window:\n\nwindow_end = 4\n\n,\n\n,\n\n>\n\n{\n\n}\n\n- remove 1 at start: {1:1, 2:2, 3:1}\n\n1\n\n1:1\n\n2:2\n\n3:1\n\n- remove\n\nat start:\n\n,\n\n,\n\n{\n\n}\n\n- remove 2 at start: {1:1, 2:1, 3:1}\n\n2\n\n1:1\n\n2:1\n\n3:1\n\n- remove\n\nat start:\n\n,\n\n,\n\n{\n\n}\n\n- remove 1 again: {2:1, 3:1}, window now starts at 3\n\n1\n\n2:1\n\n3:1\n\n3\n\n- remove\n\nagain:\n\n, window now starts at\n\n,\n\n{\n\n}\n\n\u2022 window_end = 5, increment 2: {2:2, 3:1}, window size = 3\n\n2\n\n2:2\n\n3:1\n\n= 3\n\n\u2022\n\n, increment\n\n:\n\n, window size\n\nwindow_end = 5\n\n,\n\n{\n\n}\n\n\u2022 window_end = 6, add 1: {2:2, 3:1, 1:1}, shrink again:\n\n1\n\n2:2\n\n3:1\n\n1:1\n\n\u2022\n\n, add\n\n:\n\n, shrink again:\n\nwindow_end = 6\n\n,\n\n,\n\n{\n\n}\n\n- remove 2: {2:1, 3:1, 1:1}\n\n2\n\n2:1\n\n3:1\n\n1:1\n\n- remove\n\n:\n\n,\n\n,\n\n{\n\n}\n\n- remove 3: {2:1, 1:1}, window now starts at 5\n\n3\n\n2:1\n\n1:1\n\n5\n\n- remove\n\n:\n\n, window now starts at\n\n,\n\n{\n\n}\n\n- window size = 2\n\n= 2\n\n- window size\n\nMaximum window size encountered is 4, thus max_length = 4.\n\n4\n\nMaximum window size encountered is\n\n, thus\n\n.\n\nmax_length = 4\n\nHence, the function returns 4.\n\nHence, the function returns\n\n.\n\n4\n\nFigure 23. Example of solving the deduction task. Given a program and input, the model simulates the execution of the program until\nthe fnal result.\nFigure 23. Solve Deduction Task Example. Example of solving the deduction task. Given a program and input, the model simulates the\nexecution of the program until the final result.\n\nFigure 23. Solve Deduction Task Example. Example of solving the deduction task. Given a program and input, the model simulates the\n\nExample of solving the deduction task. Given a program and input, the model simulates the\n\nFigure 23. Example of solving the deduction task. Given a program and input, the model simulates the execution of the program until\n\nFigure 23.\n\nExample of solving the deduction task. Given a program and input, the model simulates the execution of the program until\n\nSolve Deduction Task Example.\n\nFigure 23.\n\nexecution of the program until the final result.\n\nexecution of the program until the final result.\n\nthe fnal result.\n\nthe fnal result.\n\n29\n30\n\n29\n\n29\n\n30\n\n30\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Solve Induction Task\n\nTask: Solve Induction Task\n\nTask: Solve Induction Task\n\nModel Input: Given the inputs and outputs, deduce the code snippet implementing the described function-\nality:\nInput Description: Given a list of integers, sort the list and then for each element at index i, multiply it by\n(n \u2212 i), where n is the total number of elements. Compute the fnal sum.\nInputs: ['[1, 2, 3, 4]', '[4, 3, 2, 1]', '[3, 1, 4, 2]', '[1, 2, 2, 3]', '[5]']\nOutputs: ['20', '20', '20', '17', '5']\n# Golden\ndef f(elements: list[int]) -> int:\nn = len(elements)\nsorted_elements = sorted(elements)\nmodified_elements = [sorted_elements[i] * (n - i) for i in range(n)]\nreturn sum(modified_elements)\nModel Output:\nGiven the problem and the input/output pairs, we need to deduce a general code snippet that takes a sorted list and a list of multipliers, and then computes a fnal sum based on the multipliers.\nLet's break down the problem step by step:\n1. **Understand the Message:**\n- We are given a list of integers.\n- The list is sorted.\n- For each element at index i in the sorted list, we multiply it by (n \u2212 i), where n is the number of elements in the list.\n- We need to compute the sum of these products.\n2. **Analyze the Input/Output Pairs:**\n- For the input [1, 2, 3, 4], the sorted form is the same, and the output is 20.\n1 \u00b7 (4 \u2212 1) = 3, 2 \u00b7 (4 \u2212 2) = 4, 3 \u00b7 (4 \u2212 3) = 3, 4 \u00b7 (4 \u2212 4) = 0, Sum: 3 + 4 + 3 + 0 = 10\n- For the input [4, 3, 2, 1], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\n- For the input [3, 1, 4, 2], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\nshortened\n3. **Re-evaluate the Outputs:**\n- It seems there might be an error in the provided outputs for some inputs. However, based on the problem description, the outputs should be correct if we consider the sorted list and the multipliers\ncorrectly.\n4. **Deduce the Function:**\n- Given the sorted list and the list of multipliers, we can deduce the following function:\n\"'python\ndef f(a):\nsorted_a = sorted(a)\nn = len(sorted_a)\nresult = 0\nfor i in range(n):\nresult += sorted_a[i] * (n - i)\nreturn result\n...(shortened)\n5. **Verify the Function:**\n- Let's verify the function with the provided inputs:\n- For [1, 2, 3, 4]:\n- Sorted list: [1, 2, 3, 4]\n- Multipliers: [3, 4, 3, 0]\n- Sum: 1 \u00b7 3+2 \u00b7 4+3 \u00b7 3+4 \u00b7 0 = 3 + 8 + 9 + 0 = 20\n...(shortened)\nThe corrected outputs should be: For [1, 2, 3, 4]: 20, For [4, 3, 2, 1]: 20, For [3, 1, 4, 2]: 20, For [1, 2, 2, 3]: 13, For [5]: 0\nGiven the function:\ndef f(a):\nsorted_a = sorted(a)\nn = len(sorted_a)\nresult = 0\nfor i in range(n):\nresult += sorted_a[i] * (n - i)\nreturn result\nThe outputs match the corrected sum.\n### Final Answer\n(repeat the program above)\n\nModel Input: Given the inputs and outputs, deduce the code snippet implementing the described function-\n\nGiven the inputs and outputs, deduce the code snippet implementing the described function-\n\nModel Input:\n\nality:\n\nality:\n\nInput Description: Given a list of integers, sort the list and then for each element at index i, multiply it by\n\nGiven a list of integers, sort the list and then for each element at index\n\n, multiply it by\n\nInput Description:\n\ni\n\n(n \u2212 i), where n is the total number of elements. Compute the fnal sum.\n\n(\n\n)\n\n, where\n\nis the total number of elements. Compute the fnal sum.\n\nn\n\ni\n\nn\n\n\u2212\n\nInputs: ['[1, 2, 3, 4]', '[4, 3, 2, 1]', '[3, 1, 4, 2]', '[1, 2, 2, 3]', '[5]']\n\nInputs:\n\n['[1, 2, 3, 4]', '[4, 3, 2, 1]', '[3, 1, 4, 2]', '[1, 2, 2, 3]', '[5]']\n\nOutputs: ['20', '20', '20', '17', '5']\n\nOutputs:\n\n['20', '20', '20', '17', '5']\n\n# Golden\n\n# Golden\n\ndef f(elements: list[int]) -> int:\n\ndef\n\nf(elements:\n\nlist[int])\n\n->\n\nint:\n\nn = len(elements)\n\nn\n\n=\n\nlen(elements)\n\nsorted_elements = sorted(elements)\n\nsorted_elements\n\n=\n\nsorted(elements)\n\nmodified_elements = [sorted_elements[i] * (n - i) for i in range(n)]\n\nfor\n\nin\n\nmodified_elements\n\n=\n\n[sorted_elements[i]\n\n*\n\n(n\n\n-\n\ni)\n\ni\n\nrange(n)]\n\nreturn sum(modified_elements)\n\nreturn\n\nsum(modified_elements)\n\nModel Output:\n\nModel Output:\n\nGiven the problem and the input/output pairs, we need to deduce a general code snippet that takes a sorted list and a list of multipliers, and then computes a fnal sum based on the multipliers.\n\nGiven the problem and the input/output pairs, we need to deduce a general code snippet that takes a sorted list and a list of multipliers, and then computes a fnal sum based on the multipliers.\n\nLet's break down the problem step by step:\n\nLet's break down the problem step by step:\n\n1. **Understand the Message:**\n\n1. **Understand the Message:**\n\n- We are given a list of integers.\n\n- We are given a list of integers.\n\n- The list is sorted.\n\n- The list is sorted.\n\n- For each element at index i in the sorted list, we multiply it by (n \u2212 i), where n is the number of elements in the list.\n\n(\n\n)\n\n- For each element at index\n\nin the sorted list, we multiply it by\n\n, where\n\nis the number of elements in the list.\n\ni\n\nn\n\ni\n\nn\n\n\u2212\n\n- We need to compute the sum of these products.\n\n- We need to compute the sum of these products.\n\n2. **Analyze the Input/Output Pairs:**\n\n2. **Analyze the Input/Output Pairs:**\n\n- For the input [1, 2, 3, 4], the sorted form is the same, and the output is 20.\n\n[1\n\n2\n\n3\n\n4]\n\n- For the input\n\n, the sorted form is the same, and the output is 20.\n\n,\n\n,\n\n,\n\n1 \u00b7 (4 \u2212 1) = 3, 2 \u00b7 (4 \u2212 2) = 4, 3 \u00b7 (4 \u2212 3) = 3, 4 \u00b7 (4 \u2212 4) = 0, Sum: 3 + 4 + 3 + 0 = 10\n\n1\n\n(4\n\n1) = 3\n\n2\n\n(4\n\n2) = 4\n\n3\n\n(4\n\n3) = 3\n\n4\n\n(4\n\n4) = 0\n\n3 + 4 + 3 + 0 = 10\n\n,\n\n,\n\n,\n\n, Sum:\n\n\u00b7\n\n\u2212\n\n\u00b7\n\n\u2212\n\n\u00b7\n\n\u2212\n\n\u00b7\n\n\u2212\n\n- For the input [4, 3, 2, 1], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\n\n[4\n\n3\n\n2\n\n1]\n\n[1\n\n2\n\n3\n\n4]\n\n- For the input\n\n, the sorted form is\n\n, and the output is 20 (same as above).\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n- For the input [3, 1, 4, 2], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\n\n[3\n\n1\n\n4\n\n2]\n\n[1\n\n2\n\n3\n\n4]\n\n- For the input\n\n, the sorted form is\n\n, and the output is 20 (same as above).\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nshortened\n\nshortened\n\n3. **Re-evaluate the Outputs:**\n\n3. **Re-evaluate the Outputs:**\n\n- It seems there might be an error in the provided outputs for some inputs. However, based on the problem description, the outputs should be correct if we consider the sorted list and the multipliers\n\n- It seems there might be an error in the provided outputs for some inputs. However, based on the problem description, the outputs should be correct if we consider the sorted list and the multipliers\n\ncorrectly.\n\ncorrectly.\n\n4. **Deduce the Function:**\n\n4. **Deduce the Function:**\n\n- Given the sorted list and the list of multipliers, we can deduce the following function:\n\n- Given the sorted list and the list of multipliers, we can deduce the following function:\n\n\"'python\n\n\"'python\n\ndef f(a):\n\ndef f(a):\n\nsorted_a = sorted(a)\n\nsorted_a = sorted(a)\n\nn = len(sorted_a)\n\nn = len(sorted_a)\n\nresult = 0\n\nresult = 0\n\nfor i in range(n):\n\nfor i in range(n):\n\nresult += sorted_a[i] * (n - i)\n\nresult += sorted_a[i] * (n - i)\n\nreturn result\n\nreturn result\n\n...(shortened)\n\n...(shortened)\n\n5. **Verify the Function:**\n\n5. **Verify the Function:**\n\n- Let's verify the function with the provided inputs:\n\n- Let's verify the function with the provided inputs:\n\n- For [1, 2, 3, 4]:\n\n[1\n\n2\n\n3\n\n4]\n\n- For\n\n:\n\n,\n\n,\n\n,\n\n- Sorted list: [1, 2, 3, 4]\n\n[1\n\n2\n\n3\n\n4]\n\n- Sorted list:\n\n,\n\n,\n\n,\n\n- Multipliers: [3, 4, 3, 0]\n\n[3\n\n4\n\n3\n\n0]\n\n- Multipliers:\n\n,\n\n,\n\n,\n\n- Sum: 1 \u00b7 3+2 \u00b7 4+3 \u00b7 3+4 \u00b7 0 = 3 + 8 + 9 + 0 = 20\n\n1\n\n3+2\n\n4+3\n\n3+4\n\n0 = 3 + 8 + 9 + 0 = 20\n\n- Sum:\n\n\u00b7\n\n\u00b7\n\n\u00b7\n\n\u00b7\n\n...(shortened)\n\n...(shortened)\n\nThe corrected outputs should be: For [1, 2, 3, 4]: 20, For [4, 3, 2, 1]: 20, For [3, 1, 4, 2]: 20, For [1, 2, 2, 3]: 13, For [5]: 0\n\n[1\n\n2\n\n3\n\n4]\n\n[4\n\n3\n\n2\n\n1]\n\n[3\n\n1\n\n4\n\n2]\n\n[1\n\n2\n\n2\n\n3]\n\n[5]\n\nThe corrected outputs should be: For\n\n: 20, For\n\n: 20, For\n\n: 20, For\n\n: 13, For\n\n: 0\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nGiven the function:\n\nGiven the function:\n\ndef f(a):\n\ndef\n\nf(a):\n\nsorted_a = sorted(a)\n\nsorted_a\n\n=\n\nsorted(a)\n\nn = len(sorted_a)\n\nn\n\n=\n\nlen(sorted_a)\n\nresult = 0\n\nresult\n\n=\n\n0\n\nfor i in range(n):\n\nfor\n\nin\n\ni\n\nrange(n):\n\nresult += sorted_a[i] * (n - i)\n\nresult\n\n+=\n\nsorted_a[i]\n\n*\n\n(n\n\n-\n\ni)\n\nreturn result\n\nreturn\n\nresult\n\nThe outputs match the corrected sum.\n\nThe outputs match the corrected sum.\n\n### Final Answer\n\n### Final Answer\n\n(repeat the program above)\n\n(repeat the program above)\n\nFigure 24. Example of solving an induction task. The model is given a program description, and 5 input/output cases and predict the\nfunction. Interestingly, after the model gives the function, it will go through the given use cases one by one and confrm all test cases are\npassed. 30\nFigure 24. Solve Induction Task Example. Example of solving an induction task. The model is given a program description, and 5\ninput/output cases and predict the function. Interestingly, after the model gives the function, it will go through the given use cases one by\none and confirm all test cases are passed.\n\nFigure 24. Solve Induction Task Example. Example of solving an induction task. The model is given a program description, and 5\n\nExample of solving an induction task. The model is given a program description, and 5\n\nFigure 24. Example of solving an induction task. The model is given a program description, and 5 input/output cases and predict the\n\nFigure 24.\n\nExample of solving an induction task. The model is given a program description, and 5 input/output cases and predict the\n\nSolve Induction Task Example.\n\nFigure 24.\n\ninput/output cases and predict the function. Interestingly, after the model gives the function, it will go through the given use cases one by\n\ninput/output cases and predict the function. Interestingly, after the model gives the function, it will go through the given use cases one by\n\nfunction. Interestingly, after the model gives the function, it will go through the given use cases one by one and confrm all test cases are\n\nfunction. Interestingly, after the model gives the function, it will go through the given use cases one by one and confrm all test cases are\n\none and confirm all test cases are passed.\n\none and confirm all test cases are passed.\n\npassed. 30\n\npassed.\n\n30\n\n31\n\n31\n\n31\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\nModelHEval+MBPP+LCBv1-5AIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBenchLlama3.1-8B31.753.70.00.00.02.510.65.52.1+ Simple-RL-Zoo38.455.37.40.00.07.522.28.84.7+ AZR35.450.88.53.30.05.013.214.05.0Qwen2.5-3B-Coder67.165.920.03.33.320.051.018.416.6+ AZR71.369.024.43.33.337.562.026.127.0Qwen2.5-14B-Coder76.871.731.40.00.037.554.810.718.5+ AZR80.571.239.023.320.065.078.632.039.3Qwen2.5-14B-Base78.066.721.76.73.335.066.228.332.4+ AZR70.768.835.210.020.062.576.240.442.5\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nModelHEval+MBPP+LCBv1-5AIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBenchLlama3.1-8B31.753.70.00.00.02.510.65.52.1+ Simple-RL-Zoo38.455.37.40.00.07.522.28.84.7+ AZR35.450.88.53.30.05.013.214.05.0Qwen2.5-3B-Coder67.165.920.03.33.320.051.018.416.6+ AZR71.369.024.43.33.337.562.026.127.0Qwen2.5-14B-Coder76.871.731.40.00.037.554.810.718.5+ AZR80.571.239.023.320.065.078.632.039.3Qwen2.5-14B-Base78.066.721.76.73.335.066.228.332.4+ AZR70.768.835.210.020.062.576.240.442.5\n\nModel\n\nHEval+\n\nMBPP+\n\nLCBv1-5\n\nAIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBench\n\nLlama3.1-8B\n\n31.7\n\n53.7\n\n0.0\n\n0.0\n\n0.0\n\n2.5\n\n10.6\n\n5.5\n\n2.1\n\n+ Simple-RL-Zoo\n\n38.4\n\n55.3\n\n7.4\n\n0.0\n\n0.0\n\n7.5\n\n22.2\n\n8.8\n\n4.7\n\n+ AZR\n\n35.4\n\n50.8\n\n8.5\n\n3.3\n\n0.0\n\n5.0\n\n13.2\n\n14.0\n\n5.0\n\nQwen2.5-3B-Coder\n\n67.1\n\n65.9\n\n20.0\n\n3.3\n\n3.3\n\n20.0\n\n51.0\n\n18.4\n\n16.6\n\n+ AZR\n\n71.3\n\n69.0\n\n24.4\n\n3.3\n\n3.3\n\n37.5\n\n62.0\n\n26.1\n\n27.0\n\nQwen2.5-14B-Coder\n\n76.8\n\n71.7\n\n31.4\n\n0.0\n\n0.0\n\n37.5\n\n54.8\n\n10.7\n\n18.5\n\n+ AZR\n\n80.5\n\n71.2\n\n39.0\n\n23.3\n\n20.0\n\n65.0\n\n78.6\n\n32.0\n\n39.3\n\nQwen2.5-14B-Base\n\n78.0\n\n66.7\n\n21.7\n\n6.7\n\n3.3\n\n35.0\n\n66.2\n\n28.3\n\n32.4\n\n+ AZR\n\n70.7\n\n68.8\n\n35.2\n\n10.0\n\n20.0\n\n62.5\n\n76.2\n\n40.4\n\n42.5\n\nTable 5. Detailed Breakdown of Evaluation Benchmarks for Other Model Sizes and Types. Full evaluation of AZR trained on\nother models on three standard code benchmarks (HEval+\n, MBPP+\n, LCBv1-5) and six math benchmarks (AIME'24, AIME'25, AMC'23,\nMATH500, Minerva, OlympiadBench).\n\nTable 5. Detailed Breakdown of Evaluation Benchmarks for Other Model Sizes and Types. Full evaluation of AZR trained on\n\nFull evaluation of AZR trained on\n\nDetailed Breakdown of Evaluation Benchmarks for Other Model Sizes and Types.\n\nTable 5.\n\nother models on three standard code benchmarks (HEval+\n\n+\n\n, MBPP+\n\n+\n\n, LCBv1-5) and six math benchmarks (AIME'24, AIME'25, AMC'23,\n\nv1-5\n\nother models on three standard code benchmarks (HEval\n\n, MBPP\n\n, LCB\n\n) and six math benchmarks (AIME'24, AIME'25, AMC'23,\n\nMATH500, Minerva, OlympiadBench).\n\nMATH500, Minerva, OlympiadBench).\n\nhas been observed and presented for jointly trained reasoning multi-tasks. Previously, length differences were typically noted between\ncorrect and incorrect traces (Liu et al., 2025).\n\nhas been observed and presented for jointly trained reasoning multi-tasks. Previously, length differences were typically noted between\n\nhas been observed and presented for jointly trained reasoning multi-tasks. Previously, length differences were typically noted between\n\ncorrect and incorrect traces (Liu et al., 2025).\n\ncorrect and incorrect traces\n\n(Liu et al.,\n\n2025)\n\n.\n\nThe reward dynamics between the propose and solve roles exhibit mildly adversarial behavior: when one receives higher rewards, the\nother often receives lower rewards. However, this is not entirely adversarial, as the proposer is also penalized for generating unsolvable\ntasks, encouraging overall cooperative behavior in the learning process.\n\nThe reward dynamics between the propose and solve roles exhibit mildly adversarial behavior: when one receives higher rewards, the\n\nThe reward dynamics between the propose and solve roles exhibit mildly adversarial behavior: when one receives higher rewards, the\n\nother often receives lower rewards. However, this is not entirely adversarial, as the proposer is also penalized for generating unsolvable\n\nother often receives lower rewards. However, this is not entirely adversarial, as the proposer is also penalized for generating unsolvable\n\ntasks, encouraging overall cooperative behavior in the learning process.\n\ntasks, encouraging overall cooperative behavior in the learning process.\n\nC.4. Complexity and Diversity Metrics of AZR Proposed Tasks\n\nC.4. Complexity and Diversity Metrics of AZR Proposed Tasks\n\nC.4. Complexity and Diversity Metrics of AZR Proposed Tasks\n\nWe outline several metrics used to probe characteristics of the tasks proposed during the training of AZR from the base model. Specifically,\nwe log two sets of metrics: program complexity and task diversity. For complexity, we employ two proxy measures\u2014ComplexiPy score\nand the Halstead metric. To assess diversity, we compute the average abstract syntax tree (AST) edit distance between the proposed\nprogram and a set of K reference programs, and an answer diversity metric. We calculate this answer diversity metric by tracking all\nhistorical answers to the generated questions, i.e., the input-output pairs, and form a categorical distribution over these outputs. We\ndefine answer diversity as 1 \u2212 p(answer), where p(answer) is the empirical probability of a specific answer\u2014used as a proxy for the\ndiversity of generated outputs.\n\nWe outline several metrics used to probe characteristics of the tasks proposed during the training of AZR from the base model. Specifically,\n\nWe outline several metrics used to probe characteristics of the tasks proposed during the training of AZR from the base model. Specifically,\n\nwe log two sets of metrics: program complexity and task diversity. For complexity, we employ two proxy measures\u2014ComplexiPy score\n\nwe log two sets of metrics: program complexity and task diversity. For complexity, we employ two proxy measures\u2014ComplexiPy score\n\nand the Halstead metric. To assess diversity, we compute the average abstract syntax tree (AST) edit distance between the proposed\n\nand the Halstead metric. To assess diversity, we compute the average abstract syntax tree (AST) edit distance between the proposed\n\nprogram and a set of K reference programs, and an answer diversity metric. We calculate this answer diversity metric by tracking all\n\nprogram and a set of\n\nreference programs, and an answer diversity metric. We calculate this answer diversity metric by tracking all\n\nK\n\nhistorical answers to the generated questions, i.e., the input-output pairs, and form a categorical distribution over these outputs. We\n\nhistorical answers to the generated questions, i.e., the input-output pairs, and form a categorical distribution over these outputs. We\n\ndefine answer diversity as 1 \u2212 p(answer), where p(answer) is the empirical probability of a specific answer\u2014used as a proxy for the\n\ndefine answer diversity as\n\n, where\n\nis the empirical probability of a specific answer\u2014used as a proxy for the\n\nanswer\n\nanswer\n\n1\n\n(\n\n)\n\n(\n\n)\n\np\n\np\n\n\u2212\n\ndiversity of generated outputs.\n\ndiversity of generated outputs.\n\nWe present these metrics in Figure 27. Interestingly, even without incorporating them explicitly into the reward function, the policy\nappears to implicitly optimize for these metrics. This aligns well with intuitive notions of task difficulty and diversity, and that the policy\nlearned to propose increasingly challenging tasks over time using our proposed simple reward function in Equation (4).\n\nWe present these metrics in Figure 27. Interestingly, even without incorporating them explicitly into the reward function, the policy\n\nWe present these metrics in Figure\n\n27.\n\nInterestingly, even without incorporating them explicitly into the reward function, the policy\n\nappears to implicitly optimize for these metrics. This aligns well with intuitive notions of task difficulty and diversity, and that the policy\n\nappears to implicitly optimize for these metrics. This aligns well with intuitive notions of task difficulty and diversity, and that the policy\n\nlearned to propose increasingly challenging tasks over time using our proposed simple reward function in Equation (4).\n\nlearned to propose increasingly challenging tasks over time using our proposed simple reward function in Equation\n\n(4)\n\n.\n\nC.5. Generated Code Complexity Dynamics Between Abd/Ded and Ind.\n\nC.5. Generated Code Complexity Dynamics Between Abd/Ded and Ind.\n\nC.5. Generated Code Complexity Dynamics Between Abd/Ded and Ind.\n\nWe use the ComplexiPy package to measure code complexity. For each generated program in the induction task, we compute the\ncognitive complexity difference from the corresponding \"gold\" code, i.e. complexipy(p\u03c0\npropose\n{abduction,deduction}\n) \u2212 complexipy(p\u03c0solve\ninduction\n) for each\npair, where the superscript of \u03c0 indicates the role and the subscript indicates the task type(s), and p denotes the generated programs. On\naverage, the difference of proposer and solver while holding the code's functionality constant is 0.27, indicating that the proposer in the\nabduction/deduction tasks often increases the cognitive complexity to make the code appear more convoluted, whereas the induction\nsolver tends to generate more efficient implementations.\n\nWe use the ComplexiPy package to measure code complexity. For each generated program in the induction task, we compute the\n\nWe use the\n\npackage to measure code complexity. For each generated program in the induction task, we compute the\n\nComplexiPy\n\ncognitive complexity difference from the corresponding \"gold\" code, i.e. complexipy(p\u03c0\n\ncognitive complexity difference from the corresponding \"gold\" code,\n\ncomplexipy\n\n) \u2212 complexipy(p\u03c0solve\n\ncomplexipy\n\n) for each\n\nfor each\n\n.\n\n(\n\n)\n\n(\n\n)\n\ni.e\n\np\n\np\n\npropose\n\npropose\n\nsolve\n\n\u2212\n\n\u03c0\n\n\u03c0\n\n{abduction,deduction}\n\n{abduction,deduction}\n\ninduction\n\ninduction\n\npair, where the superscript of \u03c0 indicates the role and the subscript indicates the task type(s), and p denotes the generated programs. On\n\npair, where the superscript of\n\nindicates the role and the subscript indicates the task type(s), and\n\ndenotes the generated programs. On\n\n\u03c0\n\np\n\naverage, the difference of proposer and solver while holding the code's functionality constant is 0.27, indicating that the proposer in the\n\naverage, the difference of proposer and solver while holding the code's functionality constant is\n\n, indicating that the proposer in the\n\n0\n\n27\n\n.\n\nabduction/deduction tasks often increases the cognitive complexity to make the code appear more convoluted, whereas the induction\n\nabduction/deduction tasks often increases the cognitive complexity to make the code appear more convoluted, whereas the induction\n\nsolver tends to generate more efficient implementations.\n\nsolver tends to generate more efficient implementations.\n\n32\n\n32\n\n32\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Learning to Reason with Zero Data via Self-play\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Learning to Reason with Zero Data via Self-play\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Learning to Reason with Zero Data via Self-play\n\nTask: Solve Deduction Task (Example 2)\n\nTask: Solve Deduction Task (Example 2)\n\nTask: Solve Deduction Task (Example 2)\n\nModel Input: Here is the function f, the given input, and we need to predict the output.\n\nModel Input: Here is the function f, the given input, and we need to predict the output.\n\nHere is the function\n\n, the given input, and we need to predict the output.\n\nModel Input:\n\nf\n\ndef f(num_list: list[int], target_product: int) -> list[tuple[int]]:\nresult_pairs = []\nfor i in range(len(num_list)):\nfor j in range(i + 1, len(num_list)):\nif num_list[i] * num_list[j] == target_product:\nresult_pairs.append((num_list[i], num_list[j]))\nreturn result_pairs\n\ndef f(num_list: list[int], target_product: int) -> list[tuple[int]]:\n\ndef\n\nf(num_list:\n\nlist[int], target_product:\n\nint)\n\n->\n\nlist[tuple[int]]:\n\nresult_pairs = []\n\nresult_pairs\n\n=\n\n[]\n\nfor i in range(len(num_list)):\n\nfor\n\nin\n\ni\n\nrange(len(num_list)):\n\nfor j in range(i + 1, len(num_list)):\n\nfor\n\nin\n\nj\n\nrange(i\n\n+\n\n1,\n\nlen(num_list)):\n\nif num_list[i] * num_list[j] == target_product:\n\nif\n\nnum_list[i]\n\n*\n\nnum_list[j]\n\n==\n\ntarget_product:\n\nresult_pairs.append((num_list[i], num_list[j]))\n\nresult_pairs.append((num_list[i], num_list[j]))\n\nreturn result_pairs\n\nreturn\n\nresult_pairs\n\nInput: [2, 3, 5, 7, 10], 10\nOutput: [(2, 5)]\n\nInput: [2, 3, 5, 7, 10], 10\n\nInput:\n\n[2, 3, 5, 7, 10], 10\n\nOutput: [(2, 5)]\n\nOutput:\n\n[(2, 5)]\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nThe given function f takes a list of integers num_list and a target product target_product as inputs. It is designed to fnd all\npairs of numbers in num_list whose product equals target_product. The function then returns a list of tuples, where each\ntuple contains a pair of numbers that satisfy the condition.\n\nThe given function f takes a list of integers num_list and a target product target_product as inputs. It is designed to fnd all\n\nThe given function\n\ntakes a list of integers\n\nand a target product\n\nas inputs. It is designed to fnd all\n\nf\n\nnum_list\n\ntarget_product\n\npairs of numbers in num_list whose product equals target_product. The function then returns a list of tuples, where each\n\npairs of numbers in\n\nwhose product equals\n\n. The function then returns a list of tuples, where each\n\nnum_list\n\ntarget_product\n\ntuple contains a pair of numbers that satisfy the condition.\n\ntuple contains a pair of numbers that satisfy the condition.\n\nLet's go through the code step by step:\n\nLet's go through the code step by step:\n\nLet's go through the code step by step:\n\n1. Initialize an empty list result_pairs to store pairs whose product equals target_product.\n\n1. Initialize an empty list result_pairs to store pairs whose product equals target_product.\n\n1. Initialize an empty list\n\nto store pairs whose product equals\n\n.\n\nresult_pairs\n\ntarget_product\n\n2. Use nested loops to iterate through each pair of numbers in num_list. The outer loop iterates over each number, and the inner\nloop iterates over numbers that come after the current number in the outer loop (\u907f\u514d\u91cd\u590d\u548c\u53cd\u5411\u7684\u5bf9\u5b50,\u4f8b\u5982 (2, 3) \u548c (3, 2))\u3002\n3. For each pair (num_list[i], num_list[j]), check if their product equals target_product.\n\n2. Use nested loops to iterate through each pair of numbers in num_list. The outer loop iterates over each number, and the inner\n\n2. Use nested loops to iterate through each pair of numbers in\n\n. The outer loop iterates over each number, and the inner\n\nnum_list\n\nloop iterates over numbers that come after the current number in the outer loop (\u907f\u514d\u91cd\u590d\u548c\u53cd\u5411\u7684\u5bf9\u5b50,\u4f8b\u5982 (2, 3) \u548c (3, 2))\u3002\n\nloop iterates over numbers that come after the current number in the outer loop (\n\n)\n\n\u3002\n\n(2, 3)\n\n(3, 2)\n\n\u907f\u514d\u91cd\u590d\u548c\u53cd\u5411\u7684\u5bf9\u5b50,\u4f8b\u5982\n\n\u548c\n\n3. For each pair (num_list[i], num_list[j]), check if their product equals target_product.\n\n3. For each pair\n\n, check if their product equals\n\n.\n\n(num_list[i], num_list[j])\n\ntarget_product\n\n4. If the product equals target_product, append the tuple (num_list[i], num_list[j]) to result_pairs.\n\n4. If the product equals target_product, append the tuple (num_list[i], num_list[j]) to result_pairs.\n\n4. If the product equals\n\n, append the tuple\n\nto\n\n.\n\ntarget_product\n\n(num_list[i], num_list[j])\n\nresult_pairs\n\n5. After all pairs have been checked, return the list result_pairs.\n\n5. After all pairs have been checked, return the list result_pairs.\n\n5. After all pairs have been checked, return the list\n\n.\n\nresult_pairs\n\nLet's explicitly apply this to the provided input [2, 3, 5, 7, 10], target product 10:\n\nLet's explicitly apply this to the provided input [2, 3, 5, 7, 10], target product 10:\n\nLet's explicitly apply this to the provided input\n\n, target product\n\n:\n\n[2, 3, 5, 7, 10]\n\n10\n\n- For i=0 (num_list[0] = 2):\n- j=1, 2 \u00d7 3 = 6 (\u226010)\n- j=2, 2 \u00d7 5 = 10 (=10), add pair (2, 5)\n- j=3, 2 \u00d7 7 = 14 (\u226010)\n- j=4, 2 \u00d7 10 = 20 (\u226010)\n- For i=1 (num_list[1] = 3):\n- j=2, 3 \u00d7 5 = 15 (\u226010)\n- j=3, 3 \u00d7 7 = 21 (\u226010)\n- j=4, 3 \u00d7 10 = 30 (\u226010)\n- For i=2 (num_list[2] = 5):\n- j=3, 5 \u00d7 7 = 35 (\u226010)\n- j=4, 5 \u00d7 10 = 50 (\u226010)\n- For i=3 (num_list[3] = 7):\n- j=4, 7 \u00d7 10 = 70 (\u226010)\n- For i=4 (num_list[4] = 10):\n- No further pairs to check\nThus, the only pair satisfying the condition is (2, 5). Therefore, the output is:\n[(2, 5)]\n\n- For i=0 (num_list[0] = 2):\n\n- For\n\n(num_list[0] = 2):\n\ni=0\n\n- j=1, 2 \u00d7 3 = 6 (\u226010)\n\n-\n\n, 2 \u00d7 3 = 6 (\u226010)\n\nj=1\n\n- j=2, 2 \u00d7 5 = 10 (=10), add pair (2, 5)\n\n-\n\n, 2 \u00d7 5 = 10 (=10), add pair (2, 5)\n\nj=2\n\n- j=3, 2 \u00d7 7 = 14 (\u226010)\n\n-\n\n, 2 \u00d7 7 = 14 (\u226010)\n\nj=3\n\n- j=4, 2 \u00d7 10 = 20 (\u226010)\n\n-\n\n, 2 \u00d7 10 = 20 (\u226010)\n\nj=4\n\n- For i=1 (num_list[1] = 3):\n\n- For\n\n(num_list[1] = 3):\n\ni=1\n\n- j=2, 3 \u00d7 5 = 15 (\u226010)\n\n-\n\n, 3 \u00d7 5 = 15 (\u226010)\n\nj=2\n\n- j=3, 3 \u00d7 7 = 21 (\u226010)\n\n-\n\n, 3 \u00d7 7 = 21 (\u226010)\n\nj=3\n\n- j=4, 3 \u00d7 10 = 30 (\u226010)\n\n-\n\n, 3 \u00d7 10 = 30 (\u226010)\n\nj=4\n\n- For i=2 (num_list[2] = 5):\n\n- For\n\n(num_list[2] = 5):\n\ni=2\n\n- j=3, 5 \u00d7 7 = 35 (\u226010)\n\n-\n\n, 5 \u00d7 7 = 35 (\u226010)\n\nj=3\n\n- j=4, 5 \u00d7 10 = 50 (\u226010)\n\n-\n\n, 5 \u00d7 10 = 50 (\u226010)\n\nj=4\n\n- For i=3 (num_list[3] = 7):\n\n- For\n\n(num_list[3] = 7):\n\ni=3\n\n- j=4, 7 \u00d7 10 = 70 (\u226010)\n\n-\n\n, 7 \u00d7 10 = 70 (\u226010)\n\nj=4\n\n- For i=4 (num_list[4] = 10):\n\n- For\n\n(num_list[4] = 10):\n\ni=4\n\n- No further pairs to check\n\n- No further pairs to check\n\nThus, the only pair satisfying the condition is (2, 5). Therefore, the output is:\n\nThus, the only pair satisfying the condition is (2, 5). Therefore, the output is:\n\n[(2, 5)]\n\n[(2, 5)]\n\nFigure 25. An example of the model solving an deduction task. We observed that the model outputs mixed language (Chinese) when\nsolving the task.\nFigure 25. Mixed Language CoT Example. An example of the model solving an deduction task. We observed that the model outputs\nmixed language (Chinese) when solving the task.\n\nFigure 25. Mixed Language CoT Example. An example of the model solving an deduction task. We observed that the model outputs\n\nAn example of the model solving an deduction task. We observed that the model outputs\n\nFigure 25. An example of the model solving an deduction task. We observed that the model outputs mixed language (Chinese) when\n\nFigure 25.\n\nAn example of the model solving an deduction task. We observed that the model outputs mixed language (Chinese) when\n\nMixed Language CoT Example.\n\nFigure 25.\n\nmixed language (Chinese) when solving the task.\n\nmixed language (Chinese) when solving the task.\n\nsolving the task.\n\nsolving the task.\n\n31\n33\n\n31\n\n31\n\n33\n\n33\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Solve Deduction Task (Llama 8B)\n\nTask: Solve Deduction Task (Llama 8B)\n\nTask: Solve Deduction Task (Llama 8B)\n\nModel Input: Here is the function f, the input provided 'cookie', and we need to predict the output.\n\nModel Input: Here is the function f, the input provided 'cookie', and we need to predict the output.\n\nHere is the function\n\n, the input provided\n\n, and we need to predict the output.\n\nModel Input:\n\nf\n\n'cookie'\n\ndef f(input_string: str):\n\"\"\"Function to check if a string contains given substrings.\"\"\"\nsubstring = \"code\"\n# Start with `substring` as the first substring.\ncurr_index = substring\n# Slice out the required substring from the input string\nsliced_out_string = input_string[0:len(substring)]\nif (sliced_out_string == substring):\nreturn True\n# Check if there are any remaining substrings to be checked\nif (len(curr_index) > len(input_string)):\nreturn False\nreturn False\n\ndef f(input_string: str):\n\ndef\n\nf(input_string:\n\nstr):\n\n\"\"\"Function to check if a string contains given substrings.\"\"\"\n\n\"\"\"Function to check if a string contains given substrings.\"\"\"\n\nsubstring = \"code\"\n\nsubstring\n\n=\n\n\"code\"\n\n# Start with `substring` as the first substring.\n\n# Start with `substring` as the first substring.\n\ncurr_index = substring\n\ncurr_index\n\n=\n\nsubstring\n\n# Slice out the required substring from the input string\n\n# Slice out the required substring from the input string\n\nsliced_out_string = input_string[0:len(substring)]\n\nsliced_out_string\n\n=\n\ninput_string[0:len(substring)]\n\nif (sliced_out_string == substring):\n\nif\n\n(sliced_out_string\n\n==\n\nsubstring):\n\nreturn True\n\nreturn\n\nTrue\n\n# Check if there are any remaining substrings to be checked\n\n# Check if there are any remaining substrings to be checked\n\nif (len(curr_index) > len(input_string)):\n\nif\n\n(len(curr_index)\n\n>\n\nlen(input_string)):\n\nreturn False\n\nreturn\n\nFalse\n\nreturn False\n\nreturn\n\nFalse\n\nInput: 'cookie'\nOutput: False\n\nInput: 'cookie'\n\nInput:\n\n'cookie'\n\nOutput: False\n\nOutput:\n\nFalse\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nI know that the line 'curr_index = substring' is just initialization of the curr_index parameter variable and does not afect the output.\nHence, I can remove it completely to view the status of code before any output gets generated.\n\nI know that the line 'curr_index = substring' is just initialization of the curr_index parameter variable and does not afect the output.\n\nI know that the line 'curr_index = substring' is just initialization of the curr_index parameter variable and does not afect the output.\n\nHence, I can remove it completely to view the status of code before any output gets generated.\n\nHence, I can remove it completely to view the status of code before any output gets generated.\n\nNow I have a single variable, \"'curr_index\"' which is set to \"codecandide\".\n\nNow I have a single variable, \"'curr_index\"' which is set to \"codecandide\".\n\nNow I have a single variable, \"'curr_index\"' which is set to \"codecandide\".\n\nThe second line of code, 'slice_out_string = input_string[0:len(substring)]', assigns the result of slicing the input_string to the left\nof index 0 and to the right of index len(substring), i.e., the index of the last character in the substring. It then assigns this value to\nthe variable \"'slice_out_string\"'.\n\nThe second line of code, 'slice_out_string = input_string[0:len(substring)]', assigns the result of slicing the input_string to the left\n\nThe second line of code, 'slice_out_string = input_string[0:len(substring)]', assigns the result of slicing the input_string to the left\n\nof index 0 and to the right of index len(substring), i.e., the index of the last character in the substring. It then assigns this value to\n\nof index 0 and to the right of index len(substring), i.e., the index of the last character in the substring. It then assigns this value to\n\nthe variable \"'slice_out_string\"'.\n\nthe variable \"'slice_out_string\"'.\n\nThe third line, 'if (slice_out_string == substring)' checks whether the \"'slice_out_string\"' is equal to the value of substring. Because\ncurr_index is set to \"codecandide\", \"code\" appears on the left of the frst index in input_string, and thus, \"'slice_out_string\"' assumes\nthe value \"code\". That means for a minute the \"'if statement\"' is satisfed and will continue the code execution afterwards.\nHowever, according to the True/False statement, we see that the value of the frst index of input_string does not exist hence while it\ndoes output \"True\", \"False\" is outputted in main. So, the output of the whole program is shown below:\n\nThe third line, 'if (slice_out_string == substring)' checks whether the \"'slice_out_string\"' is equal to the value of substring. Because\n\nThe third line, 'if (slice_out_string == substring)' checks whether the \"'slice_out_string\"' is equal to the value of substring. Because\n\ncurr_index is set to \"codecandide\", \"code\" appears on the left of the frst index in input_string, and thus, \"'slice_out_string\"' assumes\n\ncurr_index is set to \"codecandide\", \"code\" appears on the left of the frst index in input_string, and thus, \"'slice_out_string\"' assumes\n\nthe value \"code\". That means for a minute the \"'if statement\"' is satisfed and will continue the code execution afterwards.\n\nthe value \"code\". That means for a minute the \"'if statement\"' is satisfed and will continue the code execution afterwards.\n\nHowever, according to the True/False statement, we see that the value of the frst index of input_string does not exist hence while it\n\nHowever, according to the True/False statement, we see that the value of the frst index of input_string does not exist hence while it\n\ndoes output \"True\", \"False\" is outputted in main. So, the output of the whole program is shown below:\n\ndoes output \"True\", \"False\" is outputted in main. So, the output of the whole program is shown below:\n\n'f(input_string = \"cookie\")' then output: False\n\n'f(input_string = \"cookie\")' then output: False\n\n'f(input_string = \"cookie\")' then output: False\n\nFigure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illustrate an\nexample here.\nFigure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illus-\ntrate an example here.\n\nFigure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\n\nThe\n\nAbsolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task.\n\nFigure 26.\n\nAbsolute Zero\n\nFigure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\n\nFigure 26.\n\nThe\n\nAbsolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task.\n\nAbsolute Zero\n\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illus-\n\nmodel appears to have strengthened its state-tracking behaviors during the course of training. We illus-\n\nReasoner-Llama3.1-8b\n\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illustrate an\n\nmodel appears to have strengthened its state-tracking behaviors during the course of training. We illustrate an\n\nReasoner-Llama3.1-8b\n\ntrate an example here.\n\ntrate an example here.\n\nexample here.\n\nexample here.\n\n0 80 160 240\n0\n0.47\nComplexiPy Score\n0 80 160 240\n0\n0.20\nHalstead Measure\n0 80 160 240\n0\n0.78\nAST Edit Distance\n0 80 160 240\n0\n0.99\nAnswer Diversity\nTraining Steps Complexity Diversity\n\nComplexiPy Score\n\nComplexiPy Score\n\nHalstead Measure\n\nHalstead Measure\n\nAST Edit Distance\n\nAST Edit Distance\n\nAnswer Diversity\n\nAnswer Diversity\n\n0.47\n\n0.47\n\n0.20\n\n0.20\n\n0.78\n\n0.78\n\n0.99\n\n0.99\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0 80 160 240\n\n0\n\n80\n\n160\n\n240\n\n0 80 160 240\n\n0\n\n80\n\n160\n\n240\n\n0 80 160 240\n\n0\n\n80\n\n160\n\n240\n\n0 80 160 240\n\n0\n\n80\n\n160\n\n240\n\nTraining Steps Complexity Diversity\n\nDiversity\n\nComplexity\n\nTraining Steps\n\nFigure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\nFigure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\nFigure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\n\nWe break down the proposed task metrics into program complexity and diversity across programs\n\nFigure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\n\nFigure 27.\n\nWe break down the proposed task metrics into program complexity and diversity across programs\n\nMetrics on Proposed Tasks.\n\nMetrics on Proposed Tasks.\n\nFigure 27.\n\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\n35\n34\n\n34\n\n34\n\n35\n\n35\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 75 100 125 150 175 200 225 250\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 75 100 125 150 175 200 225 250\n0.000\n0.050\n0.100\nAIME 2025\n50 75 100 125 150 175 200 225 250\n0.300\n0.325\n0.350\n0.375\n0.400\nOlympiad Bench\n50 75 100 125 150 175 200 225 250\n0.280\n0.300\n0.320\n0.340\n0.360\n0.380\nAccuracy\n\nMinerva\n50 75 100 125 150 175 200 225 250\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nMath 500\n50 75 100 125 150 175 200 225 250\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 75 100 125 150 175 200 225 250\n0.700\n0.710\n0.720\n0.730\n0.740\nAccuracy\n\nHumanEval+\n50 75 100 125 150 175 200 225 250\n0.660\n0.670\n0.680\n0.690\n0.700\nMBPP+\n50 75 100 125 150 175 200 225 250\n0.240\n0.260\n0.280\nLiveCodeBench\n50 75 100 125 150 175 200 225 250\n0.300\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 75 100 125 150 175 200 225 250\n0.540\n0.550\n0.560\n0.570\nCode Average\n50 75 100 125 150 175 200 225 250\n0.420\n0.440\n0.460\n0.480\nOverall Average\nFigure 28. Absolute Zero Reasoner-base-7b OOD Performance Breakdown.\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 75 100 125 150 175 200 225 250\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 75 100 125 150 175 200 225 250\n0.000\n0.050\n0.100\nAIME 2025\n50 75 100 125 150 175 200 225 250\n0.300\n0.325\n0.350\n0.375\n0.400\nOlympiad Bench\n50 75 100 125 150 175 200 225 250\n0.280\n0.300\n0.320\n0.340\n0.360\n0.380\nAccuracy\n\nMinerva\n50 75 100 125 150 175 200 225 250\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nMath 500\n50 75 100 125 150 175 200 225 250\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 75 100 125 150 175 200 225 250\n0.700\n0.710\n0.720\n0.730\n0.740\nAccuracy\n\nHumanEval+\n50 75 100 125 150 175 200 225 250\n0.660\n0.670\n0.680\n0.690\n0.700\nMBPP+\n50 75 100 125 150 175 200 225 250\n0.240\n0.260\n0.280\nLiveCodeBench\n50 75 100 125 150 175 200 225 250\n0.300\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 75 100 125 150 175 200 225 250\n0.540\n0.550\n0.560\n0.570\nCode Average\n50 75 100 125 150 175 200 225 250\n0.420\n0.440\n0.460\n0.480\nOverall Average\n\nAIME 2024\n\nAIME 2024\n\nAIME 2025\n\nAIME 2025\n\nOlympiad Bench\n\nOlympiad Bench\n\n0.200\n\n0.200\n\n0.400\n\n0.400\n\n0.375\n\n0.375\n\n0.100\n\n0.100\n\nAccuracy\n\nAccuracy\n\n0.150\n\n0.150\n\n0.350\n\n0.350\n\n0.100\n\n0.100\n\n0.050\n\n0.050\n\n0.325\n\n0.325\n\n0.300\n\n0.300\n\n0.050\n\n0.050\n\n0.000\n\n0.000\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nMinerva\n\nMinerva\n\nMath 500\n\nMath 500\n\nAMC 2023\n\nAMC 2023\n\n0.750\n\n0.750\n\n0.380\n\n0.380\n\n0.600\n\n0.600\n\n0.725\n\n0.725\n\nAccuracy\n\nAccuracy\n\n0.360\n\n0.360\n\n0.550\n\n0.550\n\n0.700\n\n0.700\n\n0.340\n\n0.340\n\n0.500\n\n0.500\n\n0.675\n\n0.675\n\n0.320\n\n0.320\n\n0.450\n\n0.450\n\n0.650\n\n0.650\n\n0.300\n\n0.300\n\n0.400\n\n0.400\n\n0.625\n\n0.625\n\n0.280\n\n0.280\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nCODE REASONING\n\nCODE REASONING\n\nHumanEval+\n\nHumanEval+\n\nMBPP+\n\nMBPP+\n\nLiveCodeBench\n\nLiveCodeBench\n\n0.700\n\n0.700\n\n0.740\n\n0.740\n\n0.280\n\n0.280\n\nAccuracy\n\nAccuracy\n\n0.690\n\n0.690\n\n0.730\n\n0.730\n\n0.720\n\n0.720\n\n0.680\n\n0.680\n\n0.260\n\n0.260\n\n0.710\n\n0.710\n\n0.670\n\n0.670\n\n0.240\n\n0.240\n\n0.700\n\n0.700\n\n0.660\n\n0.660\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nOVERALL PERFORMANCE\n\nOVERALL PERFORMANCE\n\nMath Average\n\nMath Average\n\nCode Average\n\nCode Average\n\nOverall Average\n\nOverall Average\n\n0.570\n\n0.570\n\n0.400\n\n0.400\n\n0.480\n\n0.480\n\n0.380\n\n0.380\n\n0.560\n\n0.560\n\nAccuracy\n\nAccuracy\n\n0.460\n\n0.460\n\n0.360\n\n0.360\n\n0.340\n\n0.340\n\n0.550\n\n0.550\n\n0.440\n\n0.440\n\n0.320\n\n0.320\n\n0.540\n\n0.540\n\n0.300\n\n0.300\n\n0.420\n\n0.420\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nFigure 28. Absolute Zero Reasoner-base-7b OOD Performance Breakdown.\n\nFigure 28. Absolute Zero Reasoner-base-7b OOD Performance Breakdown.\n\nAbsolute Zero Reasoner-base-7b OOD Performance Breakdown.\n\nFigure 28.\n\n35\n\n35\n\n35\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.000\n0.020\n0.040\n0.060\n0.080\n0.100\nAIME 2025\n50 100 150 200 250 300 350\n0.340\n0.360\n0.380\nOlympiad Bench\n50 100 150 200 250 300 350\n0.275\n0.300\n0.325\n0.350\n0.375\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.680\n0.700\n0.720\n0.740\n0.760\nMath 500\n50 100 150 200 250 300 350\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.810\n0.820\n0.830\n0.840\n0.850\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.260\n0.280\n0.300\n0.320\nLiveCodeBench\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.580\n0.590\n0.600\n0.610\n0.620\n0.630\nCode Average\n50 100 150 200 250 300 350\n0.460\n0.480\n0.500\nOverall Average\nFigure 29. Absolute Zero Reasoner-Coder-7b OOD Performance Breakdown.\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.000\n0.020\n0.040\n0.060\n0.080\n0.100\nAIME 2025\n50 100 150 200 250 300 350\n0.340\n0.360\n0.380\nOlympiad Bench\n50 100 150 200 250 300 350\n0.275\n0.300\n0.325\n0.350\n0.375\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.680\n0.700\n0.720\n0.740\n0.760\nMath 500\n50 100 150 200 250 300 350\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.810\n0.820\n0.830\n0.840\n0.850\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.260\n0.280\n0.300\n0.320\nLiveCodeBench\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.580\n0.590\n0.600\n0.610\n0.620\n0.630\nCode Average\n50 100 150 200 250 300 350\n0.460\n0.480\n0.500\nOverall Average\n\nAIME 2024\n\nAIME 2024\n\nAIME 2025\n\nAIME 2025\n\nOlympiad Bench\n\nOlympiad Bench\n\n0.200\n\n0.200\n\n0.100\n\n0.100\n\n0.380\n\n0.380\n\n0.080\n\n0.080\n\nAccuracy\n\nAccuracy\n\n0.150\n\n0.150\n\n0.060\n\n0.060\n\n0.360\n\n0.360\n\n0.040\n\n0.040\n\n0.100\n\n0.100\n\n0.020\n\n0.020\n\n0.050\n\n0.050\n\n0.340\n\n0.340\n\n0.000\n\n0.000\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nMinerva\n\nMinerva\n\nMath 500\n\nMath 500\n\nAMC 2023\n\nAMC 2023\n\n0.600\n\n0.600\n\n0.375\n\n0.375\n\n0.760\n\n0.760\n\n0.550\n\n0.550\n\nAccuracy\n\nAccuracy\n\n0.350\n\n0.350\n\n0.740\n\n0.740\n\n0.500\n\n0.500\n\n0.325\n\n0.325\n\n0.720\n\n0.720\n\n0.450\n\n0.450\n\n0.300\n\n0.300\n\n0.700\n\n0.700\n\n0.275\n\n0.275\n\n0.400\n\n0.400\n\n0.680\n\n0.680\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nCODE REASONING\n\nCODE REASONING\n\nHumanEval+\n\nHumanEval+\n\nMBPP+\n\nMBPP+\n\nLiveCodeBench\n\nLiveCodeBench\n\n0.720\n\n0.720\n\n0.320\n\n0.320\n\n0.850\n\n0.850\n\n0.710\n\n0.710\n\nAccuracy\n\nAccuracy\n\n0.840\n\n0.840\n\n0.300\n\n0.300\n\n0.830\n\n0.830\n\n0.700\n\n0.700\n\n0.820\n\n0.820\n\n0.280\n\n0.280\n\n0.690\n\n0.690\n\n0.810\n\n0.810\n\n0.680\n\n0.680\n\n0.260\n\n0.260\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nOVERALL PERFORMANCE\n\nOVERALL PERFORMANCE\n\nMath Average\n\nMath Average\n\nCode Average\n\nCode Average\n\nOverall Average\n\nOverall Average\n\n0.400\n\n0.400\n\n0.630\n\n0.630\n\n0.500\n\n0.500\n\n0.620\n\n0.620\n\n0.380\n\n0.380\n\nAccuracy\n\nAccuracy\n\n0.610\n\n0.610\n\n0.360\n\n0.360\n\n0.480\n\n0.480\n\n0.600\n\n0.600\n\n0.340\n\n0.340\n\n0.460\n\n0.460\n\n0.590\n\n0.590\n\n0.320\n\n0.320\n\n0.580\n\n0.580\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nFigure 29. Absolute Zero Reasoner-Coder-7b OOD Performance Breakdown.\n\nFigure 29. Absolute Zero Reasoner-Coder-7b OOD Performance Breakdown.\n\nAbsolute Zero Reasoner-Coder-7b OOD Performance Breakdown.\n\nFigure 29.\n\n36\n\n36\n\n36\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350\n0.250\n0.300\n0.350\n0.400\nOlympiad Bench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.500\n0.550\n0.600\n0.650\n0.700\n0.750\nMath 500\n50 100 150 200 250 300 350\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.700\n0.720\n0.740\n0.760\n0.780\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\nLiveCodeBench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.570\n0.580\n0.590\n0.600\n0.610\n0.620\nCode Average\n50 100 150 200 250 300 350\n0.440\n0.460\n0.480\n0.500\nOverall Average\nFigure 30. Absolute Zero Reasoner-base-14b OOD Performance Breakdown.\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350\n0.250\n0.300\n0.350\n0.400\nOlympiad Bench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.500\n0.550\n0.600\n0.650\n0.700\n0.750\nMath 500\n50 100 150 200 250 300 350\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.700\n0.720\n0.740\n0.760\n0.780\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\nLiveCodeBench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.570\n0.580\n0.590\n0.600\n0.610\n0.620\nCode Average\n50 100 150 200 250 300 350\n0.440\n0.460\n0.480\n0.500\nOverall Average\n\nAIME 2024\n\nAIME 2024\n\nAIME 2025\n\nAIME 2025\n\nOlympiad Bench\n\nOlympiad Bench\n\n0.200\n\n0.200\n\n0.400\n\n0.400\n\n0.200\n\n0.200\n\nAccuracy\n\nAccuracy\n\n0.150\n\n0.150\n\n0.350\n\n0.350\n\n0.150\n\n0.150\n\n0.100\n\n0.100\n\n0.300\n\n0.300\n\n0.100\n\n0.100\n\n0.050\n\n0.050\n\n0.250\n\n0.250\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nMinerva\n\nMinerva\n\nMath 500\n\nMath 500\n\nAMC 2023\n\nAMC 2023\n\n0.750\n\n0.750\n\n0.600\n\n0.600\n\n0.400\n\n0.400\n\n0.700\n\n0.700\n\nAccuracy\n\nAccuracy\n\n0.550\n\n0.550\n\n0.650\n\n0.650\n\n0.350\n\n0.350\n\n0.500\n\n0.500\n\n0.600\n\n0.600\n\n0.550\n\n0.550\n\n0.300\n\n0.300\n\n0.450\n\n0.450\n\n0.500\n\n0.500\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nCODE REASONING\n\nCODE REASONING\n\nHumanEval+\n\nHumanEval+\n\nMBPP+\n\nMBPP+\n\nLiveCodeBench\n\nLiveCodeBench\n\n0.780\n\n0.780\n\n0.720\n\n0.720\n\n0.760\n\n0.760\n\n0.360\n\n0.360\n\nAccuracy\n\nAccuracy\n\n0.710\n\n0.710\n\n0.740\n\n0.740\n\n0.700\n\n0.700\n\n0.340\n\n0.340\n\n0.690\n\n0.690\n\n0.720\n\n0.720\n\n0.320\n\n0.320\n\n0.680\n\n0.680\n\n0.700\n\n0.700\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nOVERALL PERFORMANCE\n\nOVERALL PERFORMANCE\n\nMath Average\n\nMath Average\n\nCode Average\n\nCode Average\n\nOverall Average\n\nOverall Average\n\n0.620\n\n0.620\n\n0.500\n\n0.500\n\n0.610\n\n0.610\n\n0.400\n\n0.400\n\nAccuracy\n\nAccuracy\n\n0.600\n\n0.600\n\n0.480\n\n0.480\n\n0.350\n\n0.350\n\n0.590\n\n0.590\n\n0.460\n\n0.460\n\n0.580\n\n0.580\n\n0.300\n\n0.300\n\n0.440\n\n0.440\n\n0.570\n\n0.570\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nFigure 30. Absolute Zero Reasoner-base-14b OOD Performance Breakdown.\n\nFigure 30. Absolute Zero Reasoner-base-14b OOD Performance Breakdown.\n\nAbsolute Zero Reasoner-base-14b OOD Performance Breakdown.\n\nFigure 30.\n\n37\n\n37\n\n37\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350 400\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350 400\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350 400\n0.380\n0.400\n0.420\n0.440 Olympiad Bench\n50 100 150 200 250 300 350 400\n0.200\n0.250\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350 400\n0.700\n0.720\n0.740\n0.760\n0.780\n0.800\nMath 500\n50 100 150 200 250 300 350 400\n0.500\n0.550\n0.600\n0.650\n0.700\nAMC 2023\n50 100 150 200 250 300 350 400\n0.780\n0.800\n0.820\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350 400\n0.700\n0.710\n0.720\n0.730\n0.740\nMBPP+\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\nLiveCodeBench\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\n0.420\n0.440\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350 400\n0.610\n0.620\n0.630\n0.640\n0.650\nCode Average\n50 100 150 200 250 300 350 400\n0.500\n0.520\n0.540\nOverall Average\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350 400\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350 400\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350 400\n0.380\n0.400\n0.420\n0.440 Olympiad Bench\n50 100 150 200 250 300 350 400\n0.200\n0.250\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350 400\n0.700\n0.720\n0.740\n0.760\n0.780\n0.800\nMath 500\n50 100 150 200 250 300 350 400\n0.500\n0.550\n0.600\n0.650\n0.700\nAMC 2023\n50 100 150 200 250 300 350 400\n0.780\n0.800\n0.820\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350 400\n0.700\n0.710\n0.720\n0.730\n0.740\nMBPP+\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\nLiveCodeBench\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\n0.420\n0.440\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350 400\n0.610\n0.620\n0.630\n0.640\n0.650\nCode Average\n50 100 150 200 250 300 350 400\n0.500\n0.520\n0.540\nOverall Average\n\nAIME 2024\n\nAIME 2024\n\nAIME 2025\n\nAIME 2025\n\n0.440 Olympiad Bench\n\nOlympiad Bench\n\n0.440\n\n0.200\n\n0.200\n\n0.200\n\n0.200\n\n0.420\n\n0.420\n\nAccuracy\n\nAccuracy\n\n0.150\n\n0.150\n\n0.150\n\n0.150\n\n0.400\n\n0.400\n\n0.100\n\n0.100\n\n0.100\n\n0.100\n\n0.380\n\n0.380\n\n0.050\n\n0.050\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nMinerva\n\nMinerva\n\nMath 500\n\nMath 500\n\nAMC 2023\n\nAMC 2023\n\n0.700\n\n0.700\n\n0.800\n\n0.800\n\n0.400\n\n0.400\n\n0.780\n\n0.780\n\n0.650\n\n0.650\n\nAccuracy\n\nAccuracy\n\n0.350\n\n0.350\n\n0.760\n\n0.760\n\n0.600\n\n0.600\n\n0.300\n\n0.300\n\n0.740\n\n0.740\n\n0.550\n\n0.550\n\n0.250\n\n0.250\n\n0.720\n\n0.720\n\n0.500\n\n0.500\n\n0.200\n\n0.200\n\n0.700\n\n0.700\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nCODE REASONING\n\nCODE REASONING\n\nHumanEval+\n\nHumanEval+\n\nMBPP+\n\nMBPP+\n\nLiveCodeBench\n\nLiveCodeBench\n\n0.400\n\n0.400\n\n0.740\n\n0.740\n\n0.730\n\n0.730\n\n0.380\n\n0.380\n\nAccuracy\n\nAccuracy\n\n0.820\n\n0.820\n\n0.720\n\n0.720\n\n0.360\n\n0.360\n\n0.800\n\n0.800\n\n0.710\n\n0.710\n\n0.340\n\n0.340\n\n0.700\n\n0.700\n\n0.780\n\n0.780\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nOVERALL PERFORMANCE\n\nOVERALL PERFORMANCE\n\nMath Average\n\nMath Average\n\nCode Average\n\nCode Average\n\nOverall Average\n\nOverall Average\n\n0.440\n\n0.440\n\n0.540\n\n0.540\n\n0.650\n\n0.650\n\n0.420\n\n0.420\n\n0.640\n\n0.640\n\nAccuracy\n\nAccuracy\n\n0.520\n\n0.520\n\n0.400\n\n0.400\n\n0.630\n\n0.630\n\n0.380\n\n0.380\n\n0.620\n\n0.620\n\n0.500\n\n0.500\n\n0.360\n\n0.360\n\n0.610\n\n0.610\n\n0.340\n\n0.340\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nFigure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\nFigure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\nFigure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\nAbsolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\nFigure 31.\n\nFigure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\nFigure 31.\n\nAbsolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\n<think>\nDesign an absolutely ludicrous and convoluted Python function that is extremely difficult to\ndeduce the output from the input, designed to keep machine learning models such as Snippi\nguessing and your peers puzzling.\n!\u2192\n!\u2192\nThe aim is to outsmart all these groups of intelligent machines and less intelligent humans. This\n!\u2192 is for the brains behind the future.\n</think>\n- Absolute Zero Reasoner-Llama3.1-8b @ step 132\n\n<think>\n\n<think>\n\nDesign an absolutely ludicrous and convoluted Python function that is extremely difficult to\n\nDesign an absolutely ludicrous and convoluted Python function that is extremely difficult to\n\ndeduce the output from the input, designed to keep machine learning models such as Snippi\n\ndeduce the output from the input, designed to keep machine learning models such as Snippi\n\n!\u2192\n\n!\n\n\u2192\n\nguessing and your peers puzzling.\n\nguessing and your peers puzzling.\n\n!\u2192\n\n!\n\n\u2192\n\nThe aim is to outsmart all these groups of intelligent machines and less intelligent humans. This\n\nThe aim is to outsmart all these groups of intelligent machines and less intelligent humans. This\n\n!\u2192 is for the brains behind the future.\n\nis for the brains behind the future.\n\n!\n\n\u2192\n\n</think>\n\n</think>\n\n- Absolute Zero Reasoner-Llama3.1-8b @ step 132\n\n- Absolute Zero Reasoner-Llama3.1-8b @ step 132\n\nFigure 32. Absolute Zero Reasoner \u2013Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\nFigure 32. Absolute Zero Reasoner \u2013 Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\nFigure 32. Absolute Zero Reasoner \u2013 Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\n\nThis example highlights an unexpected and potentially unsafe\n\nAbsolute Zero Reasoner \u2013 Llama3.1-8B \"Uh-oh Moment.\"\n\nFigure 32.\n\nFigure 32. Absolute Zero Reasoner \u2013Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\n\nFigure 32.\n\nThis example highlights an unexpected and potentially unsafe\n\nAbsolute Zero Reasoner\n\nLlama3.1-8B \"Uh-oh Moment.\"\n\n\u2013\n\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\n\nreasoning chain generated by our\n\nmodel during training. Although our paradigm enables\n\nAbsolute Zero Reasoner\u2013Llama3.1-8B\n\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\n\nreasoning chain generated by our\n\nmodel during training. Although our paradigm enables\n\n\u2013\n\nAbsolute Zero Reasoner\n\nLlama3.1-8B\n\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\n39\n38\n\n38\n\n38\n\n39\n\n39\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it.\nThe assistant first thinks about the reasoning process in the mind and then provides the user\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer>\nanswer here </answer>.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\nUser: {TASK_INSTRUCTION}\n\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n\nThe assistant first thinks about the reasoning process in the mind and then provides the user\n\nThe assistant first thinks about the reasoning process in the mind and then provides the user\n\n!\u2192\n\n!\n\n\u2192\n\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n\n!\u2192\n\n!\n\n\u2192\n\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer>\n\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer>\n\n!\u2192\n\n!\n\n\u2192\n\nanswer here </answer>.\n\nanswer here </answer>.\n\n!\u2192\n\n!\n\n\u2192\n\nUser: {TASK_INSTRUCTION}\n\nUser:\n\n{TASK_INSTRUCTION}\n\nAssistant: <think>\n\nAssistant: <think>\n\nAssistant: <think>\n\nFigure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.\nFigure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.\n\nFigure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.\n\nAll our models were trained using the default Deepseek R1 template.\n\nDeepseek R1 Template.\n\nFigure 33.\n\nFigure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.\n\nFigure 33.\n\nAll our models were trained using the default Deepseek R1 template.\n\nDeepseek R1 Template.\n\n40\n39\n\n39\n\n39\n\n40\n\n40\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n## Task: Create a Python Code Snippet (where custom classes are allowed, which should be defined\n!\u2192 at the top of the code snippet) with one Matching Input\nUsing the reference code snippets provided below as examples, design a new and unique Python code\nsnippet that demands deep algorithmic reasoning to deduce one possible input from a given\noutput. Your submission should include both a code snippet and test input pair, where the\ninput will be plugged into the code snippet to produce the output, which that function output\nbe given to a test subject to come up with any input that will produce the same function\noutput. This is meant to be an I.Q. test.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n### Code Requirements:\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n!\u2192 `f`\n- Ensure the function returns a value\n- Include at least one input parameter\n- Make the function deterministic\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n!\u2192 requires long multi step reasoning\n- AVOID THE FOLLOWING:\n* Random functions or variables\n* Date/time operations\n* I/O operations (reading files, network requests)\n* Printing or logging\n* Any external state\n- Ensure execution completes within 10 seconds on a modern CPU\n- All imports and class definitions should be at the very top of the code snippet\n- The snippet should end with a return statement from the main function `f`, anything after will\n!\u2192 be removed\n### Input Requirements:\n- Provide exactly one test input for your function\n- Format multiple arguments with commas between them\n- Remember to add quotes around string arguments\n### Formatting:\n- Format your code with: ```python\ndef f(...):\n# your code here\nreturn ...\n```\n- Format your input with: ```input\narg1, arg2, ...\n```\n### Example Format:\n```python\ndef f(name: str, info: dict):\n# code logic here\nreturn result\n```\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n### Evaluation Criteria:\n- Executability, your code should be executable given your input\n- Difficulty in predicting the output from your provided input and code snippet. Focus on either\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n!\u2192\n!\u2192\n!\u2192\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\ninputs.\n!\u2192\n!\u2192\n### Reference Code Snippets:\n\n## Task: Create a Python Code Snippet (where custom classes are allowed, which should be defined\n\n## Task: Create a Python Code Snippet (where custom classes are allowed, which should be defined\n\n!\u2192 at the top of the code snippet) with one Matching Input\n\nat the top of the code snippet) with one Matching Input\n\n!\n\n\u2192\n\nUsing the reference code snippets provided below as examples, design a new and unique Python code\n\nUsing the reference code snippets provided below as examples, design a new and unique Python code\n\nsnippet that demands deep algorithmic reasoning to deduce one possible input from a given\n\nsnippet that demands deep algorithmic reasoning to deduce one possible input from a given\n\n!\u2192\n\n!\n\n\u2192\n\noutput. Your submission should include both a code snippet and test input pair, where the\n\noutput. Your submission should include both a code snippet and test input pair, where the\n\n!\u2192\n\n!\n\n\u2192\n\ninput will be plugged into the code snippet to produce the output, which that function output\n\ninput will be plugged into the code snippet to produce the output, which that function output\n\n!\u2192\n\n!\n\n\u2192\n\nbe given to a test subject to come up with any input that will produce the same function\n\nbe given to a test subject to come up with any input that will produce the same function\n\n!\u2192\n\n!\n\n\u2192\n\noutput. This is meant to be an I.Q. test.\n\noutput. This is meant to be an I.Q. test.\n\n!\u2192\n\n!\n\n\u2192\n\n### Code Requirements:\n\n### Code Requirements:\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n!\u2192 `f`\n\n`f`\n\n!\n\n\u2192\n\n- Ensure the function returns a value\n\n- Ensure the function returns a value\n\n- Include at least one input parameter\n\n- Include at least one input parameter\n\n- Make the function deterministic\n\n- Make the function deterministic\n\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n\n!\u2192 requires long multi step reasoning\n\nrequires long multi step reasoning\n\n!\n\n\u2192\n\n- AVOID THE FOLLOWING:\n\n- AVOID THE FOLLOWING:\n\n* Random functions or variables\n\n* Random functions or variables\n\n* Date/time operations\n\n* Date/time operations\n\n* I/O operations (reading files, network requests)\n\n* I/O operations (reading files, network requests)\n\n* Printing or logging\n\n* Printing or logging\n\n* Any external state\n\n* Any external state\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- The snippet should end with a return statement from the main function `f`, anything after will\n\n- The snippet should end with a return statement from the main function `f`, anything after will\n\n!\u2192 be removed\n\nbe removed\n\n!\n\n\u2192\n\n### Input Requirements:\n\n### Input Requirements:\n\n- Provide exactly one test input for your function\n\n- Provide exactly one test input for your function\n\n- Format multiple arguments with commas between them\n\n- Format multiple arguments with commas between them\n\n- Remember to add quotes around string arguments\n\n- Remember to add quotes around string arguments\n\n### Formatting:\n\n### Formatting:\n\n- Format your code with: ```python\n\n- Format your code with: ```python\n\ndef f(...):\n\ndef f(...):\n\n# your code here\n\n# your code here\n\nreturn ...\n\nreturn ...\n\n```\n\n```\n\n- Format your input with: ```input\n\n- Format your input with: ```input\n\narg1, arg2, ...\n\narg1, arg2, ...\n\n```\n\n```\n\n### Example Format:\n\n### Example Format:\n\n```python\n\n```python\n\ndef f(name: str, info: dict):\n\ndef f(name: str, info: dict):\n\n# code logic here\n\n# code logic here\n\nreturn result\n\nreturn result\n\n```\n\n```\n\n```input\n\n```input\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n### Evaluation Criteria:\n\n### Evaluation Criteria:\n\n- Executability, your code should be executable given your input\n\n- Executability, your code should be executable given your input\n\n- Difficulty in predicting the output from your provided input and code snippet. Focus on either\n\n- Difficulty in predicting the output from your provided input and code snippet. Focus on either\n\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\n\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\n\n!\u2192\n\n!\n\n\u2192\n\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\n\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\n\n!\u2192\n\n!\n\n\u2192\n\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n\n!\u2192\n\n!\n\n\u2192\n\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\n\nwords in any form, even in comments:\n\n{LIST_OF_FORBIDDEN_PACKAGES}\n\n!\n\n\u2192\n\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\n\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\n\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\n\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\n\n!\u2192\n\n!\n\n\u2192\n\ninputs.\n\ninputs.\n\n!\u2192\n\n!\n\n\u2192\n\n### Reference Code Snippets:\n\n### Reference Code Snippets:\n\n{CODE_REFERENCES_FROM_BUFFER}\n\n{CODE_REFERENCES_FROM_BUFFER}\n\n{CODE_REFERENCES_FROM_BUFFER}\n\nFigure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.\nFigure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.\n\nFigure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.\n\nFigure 34.\n\nProgram Input Abduction Task\n\nProblem Proposal Instruction.\n\n\u2014\n\nFigure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.\n\nProgram Input Abduction Task\u2014Problem Proposal Instruction.\n\nFigure 34.\n\n41\n40\n\n41\n\n41\n\n40\n\n40\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n## Task: Create a New Python Code Snippet (where custom classes are allowed, which should be\n!\u2192 defined at the top of the code snippet) with one Matching Input\nUsing the reference code snippets provided below as examples, design a new and unique Python code\nsnippet that demands deep algorithmic reasoning to deduce the output from the input. Your\nsubmission should include a code snippet and a test input pair, where the input will be\nplugged into the code snippet to produce the output. The input will be given to a test\nsubject to deduce the output, which is meant to be an I.Q. test.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n### Code Requirements:\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n!\u2192 `f`\n- Ensure the function returns a value\n- Include at least one input parameter\n- Make the function deterministic\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n!\u2192 requires long multi step reasoning\n- AVOID THE FOLLOWING:\n* Random functions or variables\n* Date/time operations\n* I/O operations (reading files, network requests)\n* Printing or logging\n* Any external state\n- Ensure execution completes within 10 seconds on a modern CPU\n- All imports and class definitions should be at the very top of the code snippet\n- The snippet should end with a return statement from the main function `f`, anything after will\n!\u2192 be removed\n### Input Requirements:\n- Provide exactly one test input for your function\n- Format multiple arguments with commas between them\n- Remember to add quotes around string arguments\n### Formatting:\n- Format your code with:\n```python\ndef f(...):\n# your code here\nreturn ...\n```\n- Format your input with:\n```input\narg1, arg2, ...\n```\n### Example Format:\n```python\ndef f(name: str, info: dict):\n# code logic here\nreturn result\n```\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n### Evaluation Criteria:\n- Executability, your code should be executable given your input\n- Difficulty in predicting your ```input``` from 1) your ```python``` code and 2) the\ndeterministic ```output``` that will be obtained from your ```input```. Focus on either\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\ninputs.\n!\u2192\n!\u2192\n### Reference Code Snippets:\n{CODE_REFERENCES_FROM_BUFFER}\n\n## Task: Create a New Python Code Snippet (where custom classes are allowed, which should be\n\n## Task: Create a New Python Code Snippet (where custom classes are allowed, which should be\n\n!\u2192 defined at the top of the code snippet) with one Matching Input\n\ndefined at the top of the code snippet) with one Matching Input\n\n!\n\n\u2192\n\nUsing the reference code snippets provided below as examples, design a new and unique Python code\n\nUsing the reference code snippets provided below as examples, design a new and unique Python code\n\nsnippet that demands deep algorithmic reasoning to deduce the output from the input. Your\n\nsnippet that demands deep algorithmic reasoning to deduce the output from the input. Your\n\n!\u2192\n\n!\n\n\u2192\n\nsubmission should include a code snippet and a test input pair, where the input will be\n\nsubmission should include a code snippet and a test input pair, where the input will be\n\n!\u2192\n\n!\n\n\u2192\n\nplugged into the code snippet to produce the output. The input will be given to a test\n\nplugged into the code snippet to produce the output. The input will be given to a test\n\n!\u2192\n\n!\n\n\u2192\n\nsubject to deduce the output, which is meant to be an I.Q. test.\n\nsubject to deduce the output, which is meant to be an I.Q. test.\n\n!\u2192\n\n!\n\n\u2192\n\n### Code Requirements:\n\n### Code Requirements:\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n!\u2192 `f`\n\n`f`\n\n!\n\n\u2192\n\n- Ensure the function returns a value\n\n- Ensure the function returns a value\n\n- Include at least one input parameter\n\n- Include at least one input parameter\n\n- Make the function deterministic\n\n- Make the function deterministic\n\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n\n!\u2192 requires long multi step reasoning\n\nrequires long multi step reasoning\n\n!\n\n\u2192\n\n- AVOID THE FOLLOWING:\n\n- AVOID THE FOLLOWING:\n\n* Random functions or variables\n\n* Random functions or variables\n\n* Date/time operations\n\n* Date/time operations\n\n* I/O operations (reading files, network requests)\n\n* I/O operations (reading files, network requests)\n\n* Printing or logging\n\n* Printing or logging\n\n* Any external state\n\n* Any external state\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- The snippet should end with a return statement from the main function `f`, anything after will\n\n- The snippet should end with a return statement from the main function `f`, anything after will\n\n!\u2192 be removed\n\nbe removed\n\n!\n\n\u2192\n\n### Input Requirements:\n\n### Input Requirements:\n\n- Provide exactly one test input for your function\n\n- Provide exactly one test input for your function\n\n- Format multiple arguments with commas between them\n\n- Format multiple arguments with commas between them\n\n- Remember to add quotes around string arguments\n\n- Remember to add quotes around string arguments\n\n### Formatting:\n\n### Formatting:\n\n- Format your code with:\n\n- Format your code with:\n\n```python\n\n```python\n\ndef f(...):\n\ndef f(...):\n\n# your code here\n\n# your code here\n\nreturn ...\n\nreturn ...\n\n```\n\n```\n\n- Format your input with:\n\n- Format your input with:\n\n```input\n\n```input\n\narg1, arg2, ...\n\narg1, arg2, ...\n\n```\n\n```\n\n### Example Format:\n\n### Example Format:\n\n```python\n\n```python\n\ndef f(name: str, info: dict):\n\ndef f(name: str, info: dict):\n\n# code logic here\n\n# code logic here\n\nreturn result\n\nreturn result\n\n```\n\n```\n\n```input\n\n```input\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n### Evaluation Criteria:\n\n### Evaluation Criteria:\n\n- Executability, your code should be executable given your input\n\n- Executability, your code should be executable given your input\n\n- Difficulty in predicting your ```input``` from 1) your ```python``` code and 2) the\n\n- Difficulty in predicting your ```input``` from 1) your ```python``` code and 2) the\n\ndeterministic ```output``` that will be obtained from your ```input```. Focus on either\n\ndeterministic ```output``` that will be obtained from your ```input```. Focus on either\n\n!\u2192\n\n!\n\n\u2192\n\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\n\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\n\n!\u2192\n\n!\n\n\u2192\n\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\n\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\n\n!\u2192\n\n!\n\n\u2192\n\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n\n!\u2192\n\n!\n\n\u2192\n\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\n\nwords in any form, even in comments:\n\n{LIST_OF_FORBIDDEN_PACKAGES}\n\n!\n\n\u2192\n\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\n\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\n\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\n\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\n\n!\u2192\n\n!\n\n\u2192\n\ninputs.\n\ninputs.\n\n!\u2192\n\n!\n\n\u2192\n\n### Reference Code Snippets:\n\n### Reference Code Snippets:\n\n{CODE_REFERENCES_FROM_BUFFER}\n\n{CODE_REFERENCES_FROM_BUFFER}\n\nFigure 35. Program Output Deduction Task\u2014Problem Generation Instruction.\nFigure 35. Program Output Deduction Task\u2014Problem Generation Instruction.\n\nFigure 35. Program Output Deduction Task\u2014Problem Generation Instruction.\n\nFigure 35.\n\nProgram Output Deduction Task\n\nProblem Generation Instruction.\n\n\u2014\n\nFigure 35. Program Output Deduction Task\u2014Problem Generation Instruction.\n\nProgram Output Deduction Task\u2014Problem Generation Instruction.\n\nFigure 35.\n\n42\n41\n\n42\n\n42\n\n41\n\n41\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n## Task: Output {NUM_INPUTS} Inputs that can be plugged into the following Code Snippet to\n!\u2192 produce diverse Outputs, and give a message related to the given snippet.\nUsing the code snippet provided below, design {NUM_INPUTS} inputs that can be plugged into the\ncode snippet to produce a diverse set of outputs. A subset of your given input and its\ndeterministically produced outputs will be given to a test subject to deduce the function,\nwhich is meant to be an I.Q. test. You can also leave a message to the test subject to help\nthem deduce the code snippet.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n### Input Requirements:\n- Provide {NUM_INPUTS} valid inputs for the code snippet\n- For each input, format multiple arguments with commas between them\n- Remember to add quotes around string arguments\n- Each input should be individually wrapped in ```input``` tags\n### Message Requirements:\n- Leave a message to the test subject to help them deduce the code snippet\n- The message should be wrapped in ```message``` tags\n- The message can be in any form, can even be formed into a coding question, or a natural\n!\u2192 language instruction what the code snippet does\n- You cannot provide the code snippet in the message\n### Formatting:\n- Format your input with:\n```input\narg1, arg2, ...\n```\n### Example Format:\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n```input\n'Sammy', {{'age': 37, 'city': 'Los Angeles'}}\n```\n### Evaluation Criteria:\n- Executability, your code should be executable given your inputs\n- Coverage, the inputs and outputs should cover the whole input space of the code snippet, able\n!\u2192 to deduce the code snippet from the inputs and outputs\n- Creativity, the inputs need to be sufficiently different from each other\n- The overall selection of inputs and message combined should be challenging for the test\n!\u2192 subject, but not impossible for them to solve\nFirst, carefully devise a clear plan: e.g., understand the code snippet, then identify how your\nproposed inputs have high coverage, and why the inputs will be challenging and creative.\nThen, write the inputs and message. Remember to wrap your inputs in ```input``` tags, and\nyour message in ```message``` tags.\n!\u2192\n!\u2192\n!\u2192\n### Code Snippet:\n```python\n{SNIPPET_FROM_BUFFER}\n```\n\n## Task: Output {NUM_INPUTS} Inputs that can be plugged into the following Code Snippet to\n\n## Task: Output\n\n{NUM_INPUTS}\n\nInputs that can be plugged into the following Code Snippet to\n\n!\u2192 produce diverse Outputs, and give a message related to the given snippet.\n\nproduce diverse Outputs, and give a message related to the given snippet.\n\n!\n\n\u2192\n\nUsing the code snippet provided below, design {NUM_INPUTS} inputs that can be plugged into the\n\nUsing the code snippet provided below, design\n\n{NUM_INPUTS}\n\ninputs that can be plugged into the\n\ncode snippet to produce a diverse set of outputs. A subset of your given input and its\n\ncode snippet to produce a diverse set of outputs. A subset of your given input and its\n\n!\u2192\n\n!\n\n\u2192\n\ndeterministically produced outputs will be given to a test subject to deduce the function,\n\ndeterministically produced outputs will be given to a test subject to deduce the function,\n\n!\u2192\n\n!\n\n\u2192\n\nwhich is meant to be an I.Q. test. You can also leave a message to the test subject to help\n\nwhich is meant to be an I.Q. test. You can also leave a message to the test subject to help\n\n!\u2192\n\n!\n\n\u2192\n\nthem deduce the code snippet.\n\nthem deduce the code snippet.\n\n!\u2192\n\n!\n\n\u2192\n\n### Input Requirements:\n\n### Input Requirements:\n\n- Provide {NUM_INPUTS} valid inputs for the code snippet\n\n- Provide\n\n{NUM_INPUTS}\n\nvalid inputs for the code snippet\n\n- For each input, format multiple arguments with commas between them\n\n- For each input, format multiple arguments with commas between them\n\n- Remember to add quotes around string arguments\n\n- Remember to add quotes around string arguments\n\n- Each input should be individually wrapped in ```input``` tags\n\n- Each input should be individually wrapped in ```input``` tags\n\n### Message Requirements:\n\n### Message Requirements:\n\n- Leave a message to the test subject to help them deduce the code snippet\n\n- Leave a message to the test subject to help them deduce the code snippet\n\n- The message should be wrapped in ```message``` tags\n\n- The message should be wrapped in ```message``` tags\n\n- The message can be in any form, can even be formed into a coding question, or a natural\n\n- The message can be in any form, can even be formed into a coding question, or a natural\n\n!\u2192 language instruction what the code snippet does\n\nlanguage instruction what the code snippet does\n\n!\n\n\u2192\n\n- You cannot provide the code snippet in the message\n\n- You cannot provide the code snippet in the message\n\n### Formatting:\n\n### Formatting:\n\n- Format your input with:\n\n- Format your input with:\n\n```input\n\n```input\n\narg1, arg2, ...\n\narg1, arg2, ...\n\n```\n\n```\n\n### Example Format:\n\n### Example Format:\n\n```input\n\n```input\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n```input\n\n```input\n\n'Sammy', {{'age': 37, 'city': 'Los Angeles'}}\n\n'Sammy',\n\n{{'age': 37, 'city': 'Los Angeles'}}\n\n```\n\n```\n\n### Evaluation Criteria:\n\n### Evaluation Criteria:\n\n- Executability, your code should be executable given your inputs\n\n- Executability, your code should be executable given your inputs\n\n- Coverage, the inputs and outputs should cover the whole input space of the code snippet, able\n\n- Coverage, the inputs and outputs should cover the whole input space of the code snippet, able\n\n!\u2192 to deduce the code snippet from the inputs and outputs\n\nto deduce the code snippet from the inputs and outputs\n\n!\n\n\u2192\n\n- Creativity, the inputs need to be sufficiently different from each other\n\n- Creativity, the inputs need to be sufficiently different from each other\n\n- The overall selection of inputs and message combined should be challenging for the test\n\n- The overall selection of inputs and message combined should be challenging for the test\n\n!\u2192 subject, but not impossible for them to solve\n\nsubject, but not impossible for them to solve\n\n!\n\n\u2192\n\nFirst, carefully devise a clear plan: e.g., understand the code snippet, then identify how your\n\nFirst, carefully devise a clear plan: e.g., understand the code snippet, then identify how your\n\nproposed inputs have high coverage, and why the inputs will be challenging and creative.\n\nproposed inputs have high coverage, and why the inputs will be challenging and creative.\n\n!\u2192\n\n!\n\n\u2192\n\nThen, write the inputs and message. Remember to wrap your inputs in ```input``` tags, and\n\nThen, write the inputs and message. Remember to wrap your inputs in ```input``` tags, and\n\n!\u2192\n\n!\n\n\u2192\n\nyour message in ```message``` tags.\n\nyour message in ```message``` tags.\n\n!\u2192\n\n!\n\n\u2192\n\n### Code Snippet:\n\n### Code Snippet:\n\n```python\n\n```python\n\n{SNIPPET_FROM_BUFFER}\n\n{SNIPPET_FROM_BUFFER}\n\n```\n\n```\n\nFigure 36. Program Induction Task\u2014Problem Proposal Instruction.\nFigure 36. Program Induction Task\u2014Problem Proposal Instruction.\n\nFigure 36. Program Induction Task\u2014Problem Proposal Instruction.\n\nFigure 36.\n\nProgram Induction Task\n\nProblem Proposal Instruction.\n\n\u2014\n\nFigure 36. Program Induction Task\u2014Problem Proposal Instruction.\n\nProgram Induction Task\u2014Problem Proposal Instruction.\n\nFigure 36.\n\n43\n42\n\n43\n\n43\n\n42\n\n42\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\nGiven the following Code Snippet and the Output, think step by step then provide one possible\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\nseparate them with commas.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Output:\n```output\n{OUTPUT}\n```\n# Output Format:\n```input\narg1, arg2, ...\n```\n# Example Output:\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\nGiven the following Code Snippet and the Output, think step by step then provide one possible\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\nseparate them with commas.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Output:\n```output\n{OUTPUT}\n```\n# Output Format:\n```input\narg1, arg2, ...\n```\n# Example Output:\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\n\nGiven the following Code Snippet and the Output, think step by step then provide one possible\n\nGiven the following Code Snippet and the Output, think step by step then provide one possible\n\nGiven the following Code Snippet and the Output, think step by step then provide one possible\n\nGiven the following Code Snippet and the Output, think step by step then provide one possible\n\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\n\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\n\n!\u2192\n\n!\n\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\n\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\n\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\n\n!\u2192\n\n!\n\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\n\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\nseparate them with commas.\n\nseparate them with commas.\n\n!\u2192\n\n!\n\nseparate them with commas.\n\nseparate them with commas.\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\n# Code Snippet:\n\n# Code Snippet:\n\n# Code Snippet:\n\n# Code Snippet:\n\n```python\n\n```python\n\n```python\n\n```python\n\n{SNIPPET}\n\n{SNIPPET}\n\n{SNIPPET}\n\n{SNIPPET}\n\n```\n\n```\n\n```\n\n```\n\n# Output:\n\n# Output:\n\n# Output:\n\n# Output:\n\n```output\n\n```output\n\n```output\n\n```output\n\n{OUTPUT}\n\n{OUTPUT}\n\n{OUTPUT}\n\n{OUTPUT}\n\n```\n\n```\n\n```\n\n```\n\n# Output Format:\n\n# Output Format:\n\n# Output Format:\n\n# Output Format:\n\n```input\n\n```input\n\n```input\n\n```input\n\narg1, arg2, ...\n\narg1, arg2, ...\n\narg1, arg2, ...\n\narg1, arg2, ...\n\n```\n\n```\n\n```\n\n```\n\n# Example Output:\n\n# Example Output:\n\n# Example Output:\n\n# Example Output:\n\n```input\n\n```input\n\n```input\n\n```input\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n```\n\n```\n\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\n\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\n\nFigure 37.\n\nProgram Input Abduction Task\n\nProblem Solving Prompt.\n\n\u2014\n\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\n\nFigure 37.\n\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\n\nProgram Input Abduction Task\u2014Problem Solving Prompt.\n\nFigure 37.\n\nProgram Input Abduction Task\n\nProblem Solving Prompt.\n\n\u2014\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\nreturns multiple values, remember to use a tuple to wrap them.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Input:\n```input\n{INPUT}\n```\n# Example Output:\n```output\n{{'age': 20, 'city': 'New York'}}\n```\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\nreturns multiple values, remember to use a tuple to wrap them.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Input:\n```input\n{INPUT}\n```\n# Example Output:\n```output\n{{'age': 20, 'city': 'New York'}}\n```\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\n\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\n\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\n\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\n\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\n\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n\n!\u2192\n\n!\n\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\n\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\n\n!\u2192\n\n!\n\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\n\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\nreturns multiple values, remember to use a tuple to wrap them.\n\nreturns multiple values, remember to use a tuple to wrap them.\n\n!\u2192\n\n!\n\nreturns multiple values, remember to use a tuple to wrap them.\n\nreturns multiple values, remember to use a tuple to wrap them.\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\n# Code Snippet:\n\n# Code Snippet:\n\n# Code Snippet:\n\n# Code Snippet:\n\n```python\n\n```python\n\n```python\n\n```python\n\n{SNIPPET}\n\n{SNIPPET}\n\n{SNIPPET}\n\n{SNIPPET}\n\n```\n\n```\n\n```\n\n```\n\n# Input:\n\n# Input:\n\n# Input:\n\n# Input:\n\n```input\n\n```input\n\n```input\n\n```input\n\n{INPUT}\n\n{INPUT}\n\n{INPUT}\n\n{INPUT}\n\n```\n\n```\n\n```\n\n```\n\n# Example Output:\n\n# Example Output:\n\n# Example Output:\n\n# Example Output:\n\n```output\n\n```output\n\n```output\n\n```output\n\n{{'age': 20, 'city': 'New York'}}\n\n{{'age': 20, 'city': 'New York'}}\n\n{{'age': 20, 'city': 'New York'}}\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n```\n\n```\n\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\n\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\n\nFigure 38.\n\nProgram Output Deduction Task\n\nProblem Solving Prompt.\n\n\u2014\n\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\n\nFigure 38.\n\nProgram Output Deduction Task\n\nProblem Solving Prompt.\n\n\u2014\n\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\n\nProgram Output Deduction Task\u2014Problem Solving Prompt.\n\nFigure 38.\n\n44\n44\n43\n\n44\n\n44\n\n43\n\n43\n\n44\n\n44\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n# Task: Deduce the Function that Produced the Outputs from the Inputs\nGiven a set of input/output pairs and a message that describes the function, think through the\nproblem step by step to deduce a general code snippet. This code should produce the hidden\noutputs from the hidden inputs, matching the original data-generating code that created the\ninput/output pairs. Place your final answer inside python tags! It may be helpful to work\nthrough each input/output pair individually to test your function. If your function doesn't\nwork as expected, revise it until it does. The final code snippet will be used to evaluate\nyour response, which is wrapped in ```python``` tags.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n# Code Requirements:\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n!\u2192 `f`\n- Ensure the function returns a value\n- Include at least one input parameter\n- Make the function deterministic\n- AVOID THE FOLLOWING:\n* Random functions or variables\n* Date/time operations\n* I/O operations (reading files, network requests)\n* Printing or logging\n* Any external state\n- Ensure execution completes within 10 seconds on a modern CPU\n- All imports and class definitions should be at the very top of the code snippet\n- The snippet should end with a return statement from the main function `f()`, anything after\n!\u2192 will be removed\n# Input and Output Pairs:\n{INPUT_OUTPUT_PAIRS}\n# Message:\n```message\n{MESSAGE}\n```\n# Example Output:\n```python\ndef f(a):\nreturn a\n```\nName your entry function `f()`!!!\n\n# Task: Deduce the Function that Produced the Outputs from the Inputs\n\n# Task: Deduce the Function that Produced the Outputs from the Inputs\n\nGiven a set of input/output pairs and a message that describes the function, think through the\n\nGiven a set of input/output pairs and a message that describes the function, think through the\n\nproblem step by step to deduce a general code snippet. This code should produce the hidden\n\nproblem step by step to deduce a general code snippet. This code should produce the hidden\n\n!\u2192\n\n!\n\n\u2192\n\noutputs from the hidden inputs, matching the original data-generating code that created the\n\noutputs from the hidden inputs, matching the original data-generating code that created the\n\n!\u2192\n\n!\n\n\u2192\n\ninput/output pairs. Place your final answer inside python tags! It may be helpful to work\n\ninput/output pairs. Place your final answer inside python tags! It may be helpful to work\n\n!\u2192\n\n!\n\n\u2192\n\nthrough each input/output pair individually to test your function. If your function doesn't\n\n'\n\nthrough each input/output pair individually to test your function. If your function doesn\n\nt\n\n!\u2192\n\n!\n\n\u2192\n\nwork as expected, revise it until it does. The final code snippet will be used to evaluate\n\nwork as expected, revise it until it does. The final code snippet will be used to evaluate\n\n!\u2192\n\n!\n\n\u2192\n\nyour response, which is wrapped in ```python``` tags.\n\nyour response, which is wrapped in ```python``` tags.\n\n!\u2192\n\n!\n\n\u2192\n\n# Code Requirements:\n\n# Code Requirements:\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n!\u2192 `f`\n\n`f`\n\n!\n\n\u2192\n\n- Ensure the function returns a value\n\n- Ensure the function returns a value\n\n- Include at least one input parameter\n\n- Include at least one input parameter\n\n- Make the function deterministic\n\n- Make the function deterministic\n\n- AVOID THE FOLLOWING:\n\n- AVOID THE FOLLOWING:\n\n* Random functions or variables\n\n* Random functions or variables\n\n* Date/time operations\n\n* Date/time operations\n\n* I/O operations (reading files, network requests)\n\n* I/O operations (reading files, network requests)\n\n* Printing or logging\n\n* Printing or logging\n\n* Any external state\n\n* Any external state\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- The snippet should end with a return statement from the main function `f()`, anything after\n\n- The snippet should end with a return statement from the main function `f()`, anything after\n\n!\u2192 will be removed\n\nwill be removed\n\n!\n\n\u2192\n\n# Input and Output Pairs:\n\n# Input and Output Pairs:\n\n{INPUT_OUTPUT_PAIRS}\n\n{INPUT_OUTPUT_PAIRS}\n\n# Message:\n\n# Message:\n\n```message\n\n```message\n\n{MESSAGE}\n\n{MESSAGE}\n\n```\n\n```\n\n# Example Output:\n\n# Example Output:\n\n```python\n\n```python\n\ndef f(a):\n\ndef f(a):\n\nreturn a\n\nreturn a\n\n```\n\n```\n\nName your entry function `f()`!!!\n\nName your entry function `f()`!!!\n\nFigure 39. Program Induction Task\u2014Problem Solving Prompt.\nFigure 39. Program Induction Task\u2014Problem Solving Prompt.\n\nFigure 39. Program Induction Task\u2014Problem Solving Prompt.\n\nFigure 39.\n\nProgram Induction Task\n\nProblem Solving Prompt.\n\n\u2014\n\nFigure 39. Program Induction Task\u2014Problem Solving Prompt.\n\nProgram Induction Task\u2014Problem Solving Prompt.\n\nFigure 39.\n\n45\n44\n\n45\n\n45\n\n44\n\n44\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Manual Constructed Sudoku Abduction Task\n\nTask: Manual Constructed Sudoku Abduction Task\n\nTask: Manual Constructed Sudoku Abduction Task\n\nModel Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\nimport random\nfrom typing import List\nimport copy\ndef g_0(board: List[List[str]]) -> bool:\ndef is_valid_block(block):\nreturn sorted(block) == [str(i) for i in range(1, 10)]\nfor row in board:\nif not is_valid_block(row):\nreturn False\nfor col in zip(*board):\nif not is_valid_block(col):\nreturn False\nfor box_row in range(0, 9, 3):\nfor box_col in range(0, 9, 3):\nblock = [\nboard[r][c]\nfor r in range(box_row, box_row + 3)\nfor c in range(box_col, box_col + 3)\n]\nif not is_valid_block(block):\nreturn False\nreturn True\ndef f(board: List[List[str]]) -> List[List[str]]:\nfor i in range(9):\nfor j in range(9):\nif board[i][j] == \".\":\nraise ValueError(\"Invalid board\")\nnum_to_mask = 51\nif not g_0(board):\nraise ValueError(\"Invalid board\")\nmasked_board = copy.deepcopy(board)\nall_positions = [(i, j) for i in range(9) for j in range(9)]\nrandom.shuffle(all_positions)\nfor i, j in all_positions[:num_to_mask]:\nmasked_board[i][j] = \".\"\nreturn masked_board\n45\n\nModel Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n\nHere is the function\n\nand the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n\nModel Input:\n\nf\n\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\nimport random\n\nimport\n\nrandom\n\nfrom typing import List\n\nfrom\n\ntyping\n\nimport\n\nList\n\nimport copy\n\nimport\n\ncopy\n\ndef g_0(board: List[List[str]]) -> bool:\n\ndef\n\ng_0(board: List[List[str]])\n\n->\n\nbool:\n\ndef is_valid_block(block):\n\ndef\n\nis_valid_block(block):\n\nreturn sorted(block) == [str(i) for i in range(1, 10)]\n\nreturn\n\nfor\n\nin\n\nsorted(block)\n\n==\n\n[str(i)\n\ni\n\nrange(1,\n\n10)]\n\nfor row in board:\n\nfor\n\nin\n\nrow\n\nboard:\n\nif not is_valid_block(row):\n\nif\n\nnot\n\nis_valid_block(row):\n\nreturn False\n\nreturn\n\nFalse\n\nfor col in zip(*board):\n\nfor\n\nin\n\ncol\n\nzip(*board):\n\nif not is_valid_block(col):\n\nif\n\nnot\n\nis_valid_block(col):\n\nreturn False\n\nreturn\n\nFalse\n\nfor box_row in range(0, 9, 3):\n\nfor\n\nin\n\nbox_row\n\nrange(0,\n\n9,\n\n3):\n\nfor box_col in range(0, 9, 3):\n\nfor\n\nin\n\nbox_col\n\nrange(0,\n\n9,\n\n3):\n\nblock = [\n\nblock\n\n=\n\n[\n\nboard[r][c]\n\nboard[r][c]\n\nfor r in range(box_row, box_row + 3)\n\nfor\n\nin\n\nr\n\nrange(box_row, box_row\n\n+\n\n3)\n\nfor c in range(box_col, box_col + 3)\n\nfor\n\nin\n\nc\n\nrange(box_col, box_col\n\n+\n\n3)\n\n]\n\n]\n\nif not is_valid_block(block):\n\nif\n\nnot\n\nis_valid_block(block):\n\nreturn False\n\nreturn\n\nFalse\n\nreturn True\n\nreturn\n\nTrue\n\ndef f(board: List[List[str]]) -> List[List[str]]:\n\ndef\n\nf(board: List[List[str]])\n\n->\n\nList[List[str]]:\n\nfor i in range(9):\n\nfor\n\nin\n\ni\n\nrange(9):\n\nfor j in range(9):\n\nfor\n\nin\n\nj\n\nrange(9):\n\nif board[i][j] == \".\":\n\nif\n\nboard[i][j]\n\n==\n\n\".\":\n\nraise ValueError(\"Invalid board\")\n\nraise\n\nValueError\n\n(\"Invalid board\")\n\nnum_to_mask = 51\n\nnum_to_mask\n\n=\n\n51\n\nif not g_0(board):\n\nif\n\nnot\n\ng_0(board):\n\nraise ValueError(\"Invalid board\")\n\nraise\n\nValueError\n\n(\"Invalid board\")\n\nmasked_board = copy.deepcopy(board)\n\nmasked_board\n\n=\n\ncopy.deepcopy(board)\n\nall_positions = [(i, j) for i in range(9) for j in range(9)]\n\nfor\n\nin\n\nfor\n\nin\n\nall_positions\n\n=\n\n[(i, j)\n\ni\n\nrange(9)\n\nj\n\nrange(9)]\n\nrandom.shuffle(all_positions)\n\nrandom.shuffle(all_positions)\n\nfor i, j in all_positions[:num_to_mask]:\n\nfor\n\nin\n\ni, j\n\nall_positions[:num_to_mask]:\n\nmasked_board[i][j] = \".\"\n\nmasked_board[i][j]\n\n=\n\n\".\"\n\nreturn masked_board\n\nreturn\n\nmasked_board\n\n45\n\n45\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nwhat it does. Let's break down the function step by step:\n\nwhat it does. Let's break down the function step by step:\n\nwhat it does. Let's break down the function step by step:\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\nis not valid, 'g_0' returns 'False'.\n\nis not valid, 'g_0' returns 'False'.\n\nis not valid, 'g_0' returns 'False'.\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\n\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\n- The masked board is then returned.\n\n- The masked board is then returned.\n\n- The masked board is then returned.\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"],[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"],[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"],[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n\n\"' We can see that the board has 51 positions masked with \".\". Therefore, the input board must have been a\n\n\"' We can see that the board has 51 positions masked with \".\". Therefore, the input board must have been a\n\n\"' We can see that the board has 51 positions masked with \".\". Therefore, the input board must have been a\n\ncomplete board, and 'f' masked 51 random positions.\n\ncomplete board, and 'f' masked 51 random positions.\n\ncomplete board, and 'f' masked 51 random positions.\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nsuch that after masking 51 positions, we get the output shown.\n\nsuch that after masking 51 positions, we get the output shown.\n\nsuch that after masking 51 positions, we get the output shown.\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\n\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\n\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\n\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\n\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\n\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\nboard will produce the given output.\n\nboard will produce the given output.\n\nboard will produce the given output.\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nFigure 40. Vibe Check 1: Sudoku.\n\nFigure 40. Vibe Check 1: Sudoku.\n\nFigure 40.\n\nVibe Check 1: Sudoku.\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Manual Constructed Sudoku Abduction Task\n\nTask: Manual Constructed Sudoku Abduction Task\n\nTask: Manual Constructed Sudoku Abduction Task\n\nModel Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\nModel Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n\nHere is the function\n\nand the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n\nModel Input:\n\nf\n\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\nwhat it does. Let's break down the function step by step:\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nwhat it does. Let's break down the function step by step:\n\nwhat it does. Let's break down the function step by step:\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\nis not valid, 'g_0' returns 'False'.\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\nis not valid, 'g_0' returns 'False'.\n\nis not valid, 'g_0' returns 'False'.\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n- The masked board is then returned.\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\n\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\n- The masked board is then returned.\n\n- The masked board is then returned.\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"], [\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],\n[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"], [\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],\n[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"], [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],\n[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] \"' We can see that the board has 51 positions masked with \".\". Therefore,\nthe input board must have been a complete board, and 'f' masked 51 random positions.\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"], [\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"], [\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],\n\n[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"], [\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],\n\n[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"], [\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],\n\n[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"], [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],\n\n[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"], [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],\n\n[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] \"' We can see that the board has 51 positions masked with \".\". Therefore,\n\n[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] \"' We can see that the board has 51 positions masked with \".\". Therefore,\n\nthe input board must have been a complete board, and 'f' masked 51 random positions.\n\nthe input board must have been a complete board, and 'f' masked 51 random positions.\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\nsuch that after masking 51 positions, we get the output shown.\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nsuch that after masking 51 positions, we get the output shown.\n\nsuch that after masking 51 positions, we get the output shown.\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\n\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\n\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\n\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\n\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\n\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\nboard will produce the given output.\n\nboard will produce the given output.\n\nboard will produce the given output.\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nFigure 40. Vibe Check 1: Sudoku.\nFigure 40. Vibe Check 1: Sudoku Solver. We cast Sudoku solving as an abduction task: our program starts from a fully solved and\nvalidated Sudoku board and simulates the masking of 51 random cells. The masked board is then presented as output, and the model\nis tasked with inferring the original input \u2014 effectively solving the puzzle backwards. An example solution is shown above, where\nAZR-Coder-14b verifies its initial guess before correctly answering. Generation parameters: temperature=0.6.\n\nFigure 40. Vibe Check 1: Sudoku Solver. We cast Sudoku solving as an abduction task: our program starts from a fully solved and\n\nWe cast Sudoku solving as an abduction task: our program starts from a fully solved and\n\nVibe Check 1: Sudoku Solver.\n\nFigure 40.\n\nFigure 40. Vibe Check 1: Sudoku.\n\nFigure 40.\n\nVibe Check 1: Sudoku.\n\nvalidated Sudoku board and simulates the masking of 51 random cells. The masked board is then presented as output, and the model\n\nvalidated Sudoku board and simulates the masking of 51 random cells. The masked board is then presented as output, and the model\n\nis tasked with inferring the original input \u2014 effectively solving the puzzle backwards. An example solution is shown above, where\n\nis tasked with inferring the original input \u2014 effectively solving the puzzle backwards. An example solution is shown above, where\n\nAZR-Coder-14b verifies its initial guess before correctly answering. Generation parameters: temperature=0.6.\n\nverifies its initial guess before correctly answering. Generation parameters: temperature=0.6.\n\nAZR-Coder-14b\n\n46\n\n46\n\n46\n\n46\n\n46\n\n46\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\nModel Input: Here is the function f and the output: 'True', please predict the input.\ndef g_3():\nreturn [(x, y) for x in range(2, 101) for y in range(x + 1, 101) if x + y <= 100]\ndef g_2(pairs):\nd = {}\nfor (x, y) in pairs:\ns = x + y\nd.setdefault(s, []).append((x, y))\nreturn d\ndef g_1(pairs):\nd = {}\nfor (x, y) in pairs:\np = x * y\nd.setdefault(p, []).append((x, y))\nreturn d\ndef g_0():\npairs = g_3()\nsum_dict = g_2(pairs)\nprod_dict = g_1(pairs)\nallowed_sums = [s for s, candidate_pairs in sum_dict.items() \\\nif all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]\ncandidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]\nnew_prod_dict = g_1(candidates_after_S)\nunique_products = {prod: pair_list for prod, pair_list in new_prod_dict.items() \\\nif len(pair_list) == 1}\nfinal_candidates = []\nfor s in allowed_sums:\nvalid_pairs = [pair for pair in candidates_after_S \\\nif pair[0] + pair[1] == s and (pair[0] * pair[1]) in unique_products]\nif len(valid_pairs) == 1:\nfinal_candidates.append(valid_pairs[0])\nreturn sorted(set(final_candidates))\ndef f(x, y):\nif not (isinstance(x, int) and isinstance(y, int)):\nreturn False\nif x <= 1 or y <= 1 or y <= x or (x + y) > 100:\nreturn False\nfinal_candidates = g_0()\nreturn len(final_candidates) == 1 and final_candidates[0] == (x, y)\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nModel Input: Here is the function f and the output: 'True', please predict the input.\n\nHere is the function\n\nand the output: 'True', please predict the input.\n\nModel Input:\n\nf\n\ndef g_3():\n\ndef\n\ng_3():\n\nreturn [(x, y) for x in range(2, 101) for y in range(x + 1, 101) if x + y <= 100]\n\nreturn\n\nfor\n\nin\n\nfor\n\nin\n\nif\n\n[(x, y)\n\nx\n\nrange(2,\n\n101)\n\ny\n\nrange(x\n\n+\n\n1,\n\n101)\n\nx\n\n+\n\ny\n\n<=\n\n100]\n\ndef g_2(pairs):\n\ndef\n\ng_2(pairs):\n\nd = {}\n\nd\n\n=\n\n{}\n\nfor (x, y) in pairs:\n\nfor\n\nin\n\n(x, y)\n\npairs:\n\ns = x + y\n\ns\n\n=\n\nx\n\n+\n\ny\n\nd.setdefault(s, []).append((x, y))\n\nd.setdefault(s, []).append((x, y))\n\nreturn d\n\nreturn\n\nd\n\ndef g_1(pairs):\n\ndef\n\ng_1(pairs):\n\nd = {}\n\nd\n\n=\n\n{}\n\nfor (x, y) in pairs:\n\nfor\n\nin\n\n(x, y)\n\npairs:\n\np = x * y\n\np\n\n=\n\nx\n\n*\n\ny\n\nd.setdefault(p, []).append((x, y))\n\nd.setdefault(p, []).append((x, y))\n\nreturn d\n\nreturn\n\nd\n\ndef g_0():\n\ndef\n\ng_0():\n\npairs = g_3()\n\npairs\n\n=\n\ng_3()\n\nsum_dict = g_2(pairs)\n\nsum_dict\n\n=\n\ng_2(pairs)\n\nprod_dict = g_1(pairs)\n\nprod_dict\n\n=\n\ng_1(pairs)\n\nallowed_sums = [s for s, candidate_pairs in sum_dict.items() \\\n\nfor\n\nin\n\nallowed_sums\n\n=\n\n[s\n\ns, candidate_pairs\n\nsum_dict.items() \\\n\nif all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]\n\nif\n\nfor\n\nin\n\nall(len(prod_dict[x\n\n*\n\ny])\n\n>\n\n1\n\n(x, y)\n\ncandidate_pairs)]\n\ncandidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]\n\nfor\n\nin\n\nif\n\nin\n\ncandidates_after_S\n\n=\n\n[pair\n\npair\n\npairs\n\npair[0]\n\n+\n\npair[1]\n\nallowed_sums]\n\nnew_prod_dict = g_1(candidates_after_S)\n\nnew_prod_dict\n\n=\n\ng_1(candidates_after_S)\n\nunique_products = {prod: pair_list for prod, pair_list in new_prod_dict.items() \\\n\nfor\n\nin\n\nunique_products\n\n=\n\n{prod: pair_list\n\nprod, pair_list\n\nnew_prod_dict.items() \\\n\nif len(pair_list) == 1}\n\nif\n\nlen(pair_list)\n\n==\n\n1}\n\nfinal_candidates = []\n\nfinal_candidates\n\n=\n\n[]\n\nfor s in allowed_sums:\n\nfor\n\nin\n\ns\n\nallowed_sums:\n\nvalid_pairs = [pair for pair in candidates_after_S \\\n\nfor\n\nin\n\nvalid_pairs\n\n=\n\n[pair\n\npair\n\ncandidates_after_S \\\n\nif pair[0] + pair[1] == s and (pair[0] * pair[1]) in unique_products]\n\nif\n\nand\n\nin\n\npair[0]\n\n+\n\npair[1]\n\n==\n\ns\n\n(pair[0]\n\n*\n\npair[1])\n\nunique_products]\n\nif len(valid_pairs) == 1:\n\nif\n\nlen(valid_pairs)\n\n==\n\n1:\n\nfinal_candidates.append(valid_pairs[0])\n\nfinal_candidates.append(valid_pairs[0])\n\nreturn sorted(set(final_candidates))\n\nreturn\n\nsorted(set(final_candidates))\n\ndef f(x, y):\n\ndef\n\nf(x, y):\n\nif not (isinstance(x, int) and isinstance(y, int)):\n\nif\n\nnot\n\nand\n\n(isinstance(x,\n\nint)\n\nisinstance(y,\n\nint)):\n\nreturn False\n\nreturn\n\nFalse\n\nif x <= 1 or y <= 1 or y <= x or (x + y) > 100:\n\nif\n\nor\n\nor\n\nor\n\nx\n\n<=\n\n1\n\ny\n\n<=\n\n1\n\ny\n\n<=\n\nx\n\n(x\n\n+\n\ny)\n\n>\n\n100:\n\nreturn False\n\nreturn\n\nFalse\n\nfinal_candidates = g_0()\n\nfinal_candidates\n\n=\n\ng_0()\n\nreturn len(final_candidates) == 1 and final_candidates[0] == (x, y)\n\nreturn\n\nand\n\nlen(final_candidates)\n\n==\n\n1\n\nfinal_candidates[0]\n\n==\n\n(x, y)\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n47\n\n47\n\n47\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nsatisfes this condition is 1.\nLet's consider a simple example:\n-numbers = [1, 2]\n-target = 1\n\nsatisfes this condition is 1.\n\nsatisfes this condition is 1.\n\nLet's consider a simple example:\n\nLet's consider a simple example:\n\n-numbers = [1, 2]\n\n-numbers = [1, 2]\n\n-target = 1\n\n-target = 1\n\nHere's the step-by-step calculation:\n\nHere's the step-by-step calculation:\n\nHere's the step-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nConsider:\n\nConsider:\n\nConsider:\n\n-numbers = [1, 0]\n-target = 1\nStep-by-step calculation:\n\n-numbers = [1, 0]\n\n-numbers = [1, 0]\n\n-target = 1\n\n-target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n...(shortened)\n\n...(shortened)\n\n...(shortened)\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\n-numbers = [2, -1, 1]\n-target = 1\nStep-by-step calculation:\n\n-numbers = [2, -1, 1]\n\n-numbers = [2, -1, 1]\n\n-target = 1\n\n-target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nFigure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\n\nFigure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\n\nFigure 41.\n\nA more detailed version of the example shown in main Figure\n\n7. The model is solving an\n\nSolve Abduction Task Example.\n\nabduction task (predict input).\n\nabduction task (predict input).\n\nabduction task (predict input).\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nModel Input: Here is the function f and the output: 'True', please predict the input.\n\nModel Input: Here is the function f and the output: 'True', please predict the input.\n\nHere is the function\n\nand the output: 'True', please predict the input.\n\nModel Input:\n\nf\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nthe function 'f(x, y)' returns 'True' if the following conditions are met:\n\nthe function 'f(x, y)' returns 'True' if the following conditions are met:\n\nthe function 'f(x, y)' returns 'True' if the following conditions are met:\n\n1. 'x' and 'y' are both integers.\n\n1. 'x' and 'y' are both integers.\n\n1. 'x' and 'y' are both integers.\n\n2. 'x' and 'y' satisfy '1 < x < y' and 'x + y <= 100'.\n\n2. 'x' and 'y' satisfy '1 < x < y' and 'x + y <= 100'.\n\n2. 'x' and 'y' satisfy '1 < x < y' and 'x + y <= 100'.\n\n3. The fnal candidates after applying the conditions in 'g_0' are exactly one pair, and that pair is '(x, y)'.\n\n3. The fnal candidates after applying the conditions in 'g_0' are exactly one pair, and that pair is '(x, y)'.\n\n3. The fnal candidates after applying the conditions in 'g_0' are exactly one pair, and that pair is '(x, y)'.\n\nLet's analyze the function 'g_0' step by step to understand what it does:\n\nLet's analyze the function 'g_0' step by step to understand what it does:\n\nLet's analyze the function 'g_0' step by step to understand what it does:\n\n1. 'pairs = g_3()' generates all pairs '(x, y)' such that '2 <= x < y <= 100' and 'x + y <= 100'.\n\n1. 'pairs = g_3()' generates all pairs '(x, y)' such that '2 <= x < y <= 100' and 'x + y <= 100'.\n\n1. 'pairs = g_3()' generates all pairs '(x, y)' such that '2 <= x < y <= 100' and 'x + y <= 100'.\n\n2. 'sum_dict = g_2(pairs)' creates a dictionary where the keys are the sums of the pairs and the values are lists of pairs that add up\nto that sum.\n\n2. 'sum_dict = g_2(pairs)' creates a dictionary where the keys are the sums of the pairs and the values are lists of pairs that add up\n\n2. 'sum_dict = g_2(pairs)' creates a dictionary where the keys are the sums of the pairs and the values are lists of pairs that add up\n\nto that sum.\n\nto that sum.\n\n3. 'prod_dict = g_1(pairs)' creates a dictionary where the keys are the products of the pairs and the values are lists of pairs that\n\n3. 'prod_dict = g_1(pairs)' creates a dictionary where the keys are the products of the pairs and the values are lists of pairs that\n\n3. 'prod_dict = g_1(pairs)' creates a dictionary where the keys are the products of the pairs and the values are lists of pairs that\n\nhave that product.\n\nhave that product.\n\nhave that product.\n\n4. 'allowed_sums = [s for s, candidate_pairs in sum_dict.items() if all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]'\nflters out the sums 's' for which all pairs '(x, y)' that add up to 's' have more than one pair with the same product.\n\n4. 'allowed_sums = [s for s, candidate_pairs in sum_dict.items() if all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]'\n\n4. 'allowed_sums = [s for s, candidate_pairs in sum_dict.items() if all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]'\n\nflters out the sums 's' for which all pairs '(x, y)' that add up to 's' have more than one pair with the same product.\n\nflters out the sums 's' for which all pairs '(x, y)' that add up to 's' have more than one pair with the same product.\n\n5. 'candidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]' flters out the pairs whose sum is in\n'allowed_sums'.\n\n5. 'candidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]' flters out the pairs whose sum is in\n\n5. 'candidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]' flters out the pairs whose sum is in\n\n'allowed_sums'.\n\n'allowed_sums'.\n\n6. 'new_prod_dict = g_1(candidates_after_S)' creates a new product dictionary using the fltered pairs.\n\n6. 'new_prod_dict = g_1(candidates_after_S)' creates a new product dictionary using the fltered pairs.\n\n6. 'new_prod_dict = g_1(candidates_after_S)' creates a new product dictionary using the fltered pairs.\n\n7. 'unique_products = prod: pair_list for prod, pair_list in new_prod_dict.items() if len(pair_list) == 1' creates a dictionary of\nproducts that have exactly one pair.\n\n7. 'unique_products = prod: pair_list for prod, pair_list in new_prod_dict.items() if len(pair_list) == 1' creates a dictionary of\n\n7. 'unique_products = prod: pair_list for prod, pair_list in new_prod_dict.items() if len(pair_list) == 1' creates a dictionary of\n\nproducts that have exactly one pair.\n\nproducts that have exactly one pair.\n\n8. 'fnal_candidates = []' initializes a list of fnal candidates.\n\n8. 'fnal_candidates = []' initializes a list of fnal candidates.\n\n8. 'fnal_candidates = []' initializes a list of fnal candidates.\n\n9. The loop 'for s in allowed_sums:' iterates over each sum in 'allowed_sums' and checks if there is exactly one pair '(x, y)' such\nthat 'x + y = s' and 'x * y' is in 'unique_products'. If so, this pair is added to 'fnal_candidates'.\n10. The function returns 'sorted(set(fnal_candidates))', which should be a list containing exactly one pair if the conditions are\nmet.\n\n9. The loop 'for s in allowed_sums:' iterates over each sum in 'allowed_sums' and checks if there is exactly one pair '(x, y)' such\n\n9. The loop 'for s in allowed_sums:' iterates over each sum in 'allowed_sums' and checks if there is exactly one pair '(x, y)' such\n\nthat 'x + y = s' and 'x * y' is in 'unique_products'. If so, this pair is added to 'fnal_candidates'.\n\nthat 'x + y = s' and 'x * y' is in 'unique_products'. If so, this pair is added to 'fnal_candidates'.\n\n10. The function returns 'sorted(set(fnal_candidates))', which should be a list containing exactly one pair if the conditions are\n\n10. The function returns 'sorted(set(fnal_candidates))', which should be a list containing exactly one pair if the conditions are\n\nmet.\n\nmet.\n\nGiven that the output is 'True', it means that there is exactly one pair '(x, y)' that satisfes all the conditions. Let's identify this pair.\n\nGiven that the output is 'True', it means that there is exactly one pair '(x, y)' that satisfes all the conditions. Let's identify this pair.\n\nGiven that the output is 'True', it means that there is exactly one pair '(x, y)' that satisfes all the conditions. Let's identify this pair.\n\nTo do this, we need to understand the mathematical conditions and constraints:\n1. '1 < x < y' and 'x + y <= 100'.\n\nTo do this, we need to understand the mathematical conditions and constraints:\n\nTo do this, we need to understand the mathematical conditions and constraints:\n\n1. '1 < x < y' and 'x + y <= 100'.\n\n1. '1 < x < y' and 'x + y <= 100'.\n\n2. The pair '(x, y)' must be the only pair whose sum is in 'allowed_sums' and whose product is unique among the pairs with the\nsame sum.\n\n2. The pair '(x, y)' must be the only pair whose sum is in 'allowed_sums' and whose product is unique among the pairs with the\n\n2. The pair '(x, y)' must be the only pair whose sum is in 'allowed_sums' and whose product is unique among the pairs with the\n\nsame sum.\n\nsame sum.\n\nBy analyzing the constraints and the nature of the problem, we can deduce that the pair '(x, y)' must be such that the sum 'x + y' is\nunique in the sense that all pairs adding up to this sum have products that are not unique, except for this pair.\n\nBy analyzing the constraints and the nature of the problem, we can deduce that the pair '(x, y)' must be such that the sum 'x + y' is\n\nBy analyzing the constraints and the nature of the problem, we can deduce that the pair '(x, y)' must be such that the sum 'x + y' is\n\nunique in the sense that all pairs adding up to this sum have products that are not unique, except for this pair.\n\nunique in the sense that all pairs adding up to this sum have products that are not unique, except for this pair.\n\nAfter detailed analysis or by running the code with diferent values, we fnd that the pair '(x, y) = (4, 13)' satisfes all the conditions.\nLet's verify:\n\nAfter detailed analysis or by running the code with diferent values, we fnd that the pair '(x, y) = (4, 13)' satisfes all the conditions.\n\nAfter detailed analysis or by running the code with diferent values, we fnd that the pair '(x, y) = (4, 13)' satisfes all the conditions.\n\nLet's verify:\n\nLet's verify:\n\n1. '1 < 4 < 13' and '4 + 13 = 17 <= 100'.\n\n1. '1 < 4 < 13' and '4 + 13 = 17 <= 100'.\n\n1. '1 < 4 < 13' and '4 + 13 = 17 <= 100'.\n\n2. We need to check the 'allowed_sums' and the uniqueness of the product '4 * 13 = 52'.\n\n2. We need to check the 'allowed_sums' and the uniqueness of the product '4 * 13 = 52'.\n\n2. We need to check the 'allowed_sums' and the uniqueness of the product '4 * 13 = 52'.\n\nBy running the code or logical deduction, we can confrm that '(4, 13)' is the only pair that satisfes all the conditions.\n\nBy running the code or logical deduction, we can confrm that '(4, 13)' is the only pair that satisfes all the conditions.\n\nBy running the code or logical deduction, we can confrm that '(4, 13)' is the only pair that satisfes all the conditions.\n\nTherefore, the input that produced the output 'True' is:\n\"'input\n(4, 13)\n\nTherefore, the input that produced the output 'True' is:\n\nTherefore, the input that produced the output 'True' is:\n\n\"'input\n\n\"'input\n\n(4, 13)\n\n(4, 13)\n\n\"'\n\n\"'\n\n\"'\n\nFigure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\nabduction task (predict input). 48\nFigure 41. Vibe Check 2: Sum Product Game. We cast the Sum-Product Game as an abduction task, where when the constraints of the\ngame are satisfied, function 'f' returns True, return False otherwise. AZR-Coder-14b was able to first analyze the complicated constraints,\nidentify candidate solution and verify. Generation parameters: temperature=0.6, top_p=0.95.\n\nFigure 41. Vibe Check 2: Sum Product Game. We cast the Sum-Product Game as an abduction task, where when the constraints of the\n\nWe cast the\n\nSum-Product Game\n\nas an abduction task, where when the constraints of the\n\nVibe Check 2: Sum Product Game.\n\nFigure 41.\n\nFigure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\n\nFigure 41.\n\nA more detail\n\ned version of the exam\n\nple shown in main Figure\n\n7. The model is solving an\n\nSolve Abduction Task Example.\n\ngame are satisfied, function 'f' returns True, return False otherwise. AZR-Coder-14b was able to first analyze the complicated constraints,\n\ngame are satisfied, function 'f' returns True, return False otherwise.\n\nwas able to first analyze the complicated constraints,\n\nAZR-Coder-14b\n\nabduction task (predict input). 48\n\nabduction task (predict input).\n\n48\n\nidentify candidate solution and verify. Generation parameters: temperature=0.6, top_p=0.95.\n\nidentify candidate solution and verify. Generation parameters: temperature=0.6, top_p=0.95.\n\n48\n\n48\n\n48\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nD. Alternative Approaches Considered\n\nD. Alternative Approaches Considered\n\nD. Alternative Approaches Considered\n\nIn this section, we share many of the approaches we tried that did not prove to be particularly helpful for Absolute Zero Reasoner.\nHowever, we believe it is especially valuable to share these findings with the community, as they are crucial for guiding future research.\nBelow, we outline each of the additional methods we explored during the development of our project.\n\nIn this section, we share many of the approaches we tried that did not prove to be particularly helpful for Absolute Zero Reasoner.\n\nIn this section, we share many of the approaches we tried that did not prove to be particularly helpful for Absolute Zero Reasoner.\n\nHowever, we believe it is especially valuable to share these findings with the community, as they are crucial for guiding future research.\n\nHowever, we believe it is especially valuable to share these findings with the community, as they are crucial for guiding future research.\n\nBelow, we outline each of the additional methods we explored during the development of our project.\n\nBelow, we outline each of the additional methods we explored during the development of our project.\n\nD.1. Error Deduction Task\n\nD.1. Error Deduction Task\n\nD.1. Error Deduction Task\n\nSince programming languages often have error messages, and these messages contain a lot of information about how someone might\nexpect a program to run, we also came up with another task domain: allowing the learner to propose a program that will produce an\nerror, and requiring the solver to deduce what kind of error is raised when executing this code. We experimented with this additional\ntask alongside the induction (f), deduction (o), and abduction (i) tasks. Unfortunately, we did not observe noticeable changes in\ndownstream performance with this additional task and since it requires more computational resources than our AZR setup, we decided\nnot to incorporate it into our final version. However, we believe further thorough investigation of this is well deserved.\n\nSince programming languages often have error messages, and these messages contain a lot of information about how someone might\n\nSince programming languages often have error messages, and these messages contain a lot of information about how someone might\n\nexpect a program to run, we also came up with another task domain: allowing the learner to propose a program that will produce an\n\nexpect a program to run, we also came up with another task domain: allowing the learner to propose a program\n\nthat will produce an\n\nerror, and requiring the solver to deduce what kind of error is raised when executing this code. We experimented with this additional\n\n, and requiring the solver to\n\nwhen executing this code. We experimented with this additional\n\nerror\n\ndeduce what kind of error is raised\n\ntask alongside the induction (f), deduction (o), and abduction (i) tasks. Unfortunately, we did not observe noticeable changes in\n\ntask alongside the induction (\n\n), deduction (\n\n), and abduction (\n\n) tasks. Unfortunately, we did not observe noticeable changes in\n\nf\n\no\n\ni\n\ndownstream performance with this additional task and since it requires more computational resources than our AZR setup, we decided\n\ndownstream performance with this additional task and since it requires more computational resources than our AZR setup, we decided\n\nnot to incorporate it into our final version. However, we believe further thorough investigation of this is well deserved.\n\nnot to incorporate it into our final version. However, we believe further thorough investigation of this is well deserved.\n\nD.2. Composite Functions as Curriculum Learning\n\nD.2. Composite Functions as Curriculum Learning\n\nD.2. Composite Functions as Curriculum Learning\n\nOne valuable property we can leverage from programming languages is the ability to compose functions\u2014that is, to define a function as\na composite of other functions, i.e., f(g(x)). In our setting, when generating a program, we can not only require the output to be a valid\nprogram but also constrain the LLM to utilize a predefined set of programs within its main function. For example, if the target program\nto be generated is f(\u00b7), we can sample a set of previously generated programs {g_0, . . . , gc} from D, and force a valid program to be\nf(g_0, \u00b7 \u00b7 \u00b7 , gc, i).\n\nOne valuable property we can leverage from programming languages is the ability to compose functions\u2014that is, to define a function as\n\nOne valuable property we can leverage from programming languages is the ability to compose functions\u2014that is, to define a function as\n\na composite of other functions, i.e., f(g(x)). In our setting, when generating a program, we can not only require the output to be a valid\n\na composite of other functions, i.e.,\n\n. In our setting, when generating a program, we can not only require the output to be a valid\n\n(\n\n(\n\n))\n\nf\n\ng\n\nx\n\nprogram but also constrain the LLM to utilize a predefined set of programs within its main function. For example, if the target program\n\nprogram but also constrain the LLM to utilize a predefined set of programs within its main function. For example, if the target program\n\nto be generated is f(\u00b7), we can sample a set of previously generated programs {g_0, . . . , gc} from D, and force a valid program to be\n\nto be generated is\n\n, we can sample a set of previously generated programs\n\nfrom\n\n, and force a valid program to be\n\n_\n\n(\n\n)\n\n0\n\nf\n\ng\n\n, . . . , g\n\n\u00b7\n\n{\n\n}\n\nD\n\nc\n\nf(g_0, \u00b7 \u00b7 \u00b7 , gc, i).\n\n_\n\n.\n\n(\n\n0\n\n)\n\nf\n\ng\n\n,\n\n, g\n\n, i\n\n\u00b7 \u00b7 \u00b7\n\nc\n\nSince all programs are generated by the LLM itself, this setup allows the model to bootstrap from its earlier generations, automatically\nincreasing the complexity of the generated programs. We interpret this mechanism as a form of curriculum learning: earlier programs\nin the AZR self-play loop tend to be simpler, and as the loop progresses, they become increasingly complex. By composing newer\nprograms from progressively more difficult earlier ones, the resulting programs naturally inherit this growing difficulty, which in turn\nchallenges the solver step.\n\nSince all programs are generated by the LLM itself, this setup allows the model to bootstrap from its earlier generations, automatically\n\nSince all programs are generated by the LLM itself, this setup allows the model to bootstrap from its earlier generations, automatically\n\nincreasing the complexity of the generated programs. We interpret this mechanism as a form of curriculum learning: earlier programs\n\nincreasing the complexity of the generated programs. We interpret this mechanism as a form of curriculum learning: earlier programs\n\nin the AZR self-play loop tend to be simpler, and as the loop progresses, they become increasingly complex. By composing newer\n\nin the AZR self-play loop tend to be simpler, and as the loop progresses, they become increasingly complex. By composing newer\n\nprograms from progressively more difficult earlier ones, the resulting programs naturally inherit this growing difficulty, which in turn\n\nprograms from progressively more difficult earlier ones, the resulting programs naturally inherit this growing difficulty, which in turn\n\nchallenges the solver step.\n\nchallenges the solver step.\n\nFor implementation, in generating tasks for abduction and deduction, we begin by sampling a binary decision from a binomial distribution\nwith p = 0.5. This determines whether the generated program should be a simple program or a composite one. If the sample is 0, we\nprompt the LLM to generate a standard program along with a corresponding input. If the sample is 1, we prompt the LLM to generate a\ncomposite program. To construct the composite, we first sample an integer c \u223c U(1, 3), then uniformly select c programs from the\ndataset D that are not themselves composite programs. Finally, we prompt the LLM to generate a valid program that incorporates\n{g_0, . . . , gc} as subcomponents, ensuring it composes these selected programs meaningfully. We additionally filter programs that did\nnot utilize all the c programs.\n\nFor implementation, in generating tasks for abduction and deduction, we begin by sampling a binary decision from a binomial distribution\n\nFor implementation, in generating tasks for abduction and deduction, we begin by sampling a binary decision from a binomial distribution\n\nwith p = 0.5. This determines whether the generated program should be a simple program or a composite one. If the sample is 0, we\n\nwith\n\n. This determines whether the generated program should be a simple program or a composite one. If the sample is 0, we\n\n= 0\n\n5\n\np\n\n.\n\nprompt the LLM to generate a standard program along with a corresponding input. If the sample is 1, we prompt the LLM to generate a\n\nprompt the LLM to generate a standard program along with a corresponding input. If the sample is 1, we prompt the LLM to generate a\n\ncomposite program. To construct the composite, we first sample an integer c \u223c U(1, 3), then uniformly select c programs from the\n\ncomposite program. To construct the composite, we first sample an integer\n\n, then uniformly select\n\nprograms from the\n\n(1\n\n3)\n\nc\n\n,\n\nc\n\n\u223c U\n\ndataset D that are not themselves composite programs. Finally, we prompt the LLM to generate a valid program that incorporates\n\ndataset\n\nthat are not themselves composite programs. Finally, we prompt the LLM to generate a valid program that incorporates\n\nD\n\n{g_0, . . . , gc} as subcomponents, ensuring it composes these selected programs meaningfully. We additionally filter programs that did\n\n_\n\nas subcomponents, ensuring it composes these selected programs meaningfully. We additionally filter programs that did\n\n0\n\ng\n\n, . . . , g\n\n{\n\n}\n\nc\n\nnot utilize all the c programs.\n\nnot utilize all the\n\nprograms.\n\nc\n\nHowever, we did not observe a significant difference when using this more complex curriculum compared to our simpler and more\neffective approach. One failure mode we encountered was that the model often defaulted to simply returning \"g(x)\", effectively learning\nf(g(x)) = g(x), which failed to introduce any additional difficulty. This trivial behavior undermined the intended challenge, leading us\nto deprioritize further exploration in this direction. While it may be possible to design a stricter reward mechanism\u2014such as enforcing\nf(g(x)) \u0338= g(x) by executing the code via a Python interpreter and penalizing such shortcuts\u2014we leave this to future work.\n\nHowever, we did not observe a significant difference when using this more complex curriculum compared to our simpler and more\n\nHowever, we did not observe a significant difference when using this more complex curriculum compared to our simpler and more\n\neffective approach. One failure mode we encountered was that the model often defaulted to simply returning \"g(x)\", effectively learning\n\neffective approach. One failure mode we encountered was that the model often defaulted to simply returning \"g(x)\", effectively learning\n\nf(g(x)) = g(x), which failed to introduce any additional difficulty. This trivial behavior undermined the intended challenge, leading us\n\n, which failed to introduce any additional difficulty. This trivial behavior undermined the intended challenge, leading us\n\n(\n\n(\n\n)) =\n\n(\n\n)\n\nf\n\ng\n\nx\n\ng\n\nx\n\nto deprioritize further exploration in this direction. While it may be possible to design a stricter reward mechanism\u2014such as enforcing\n\nto deprioritize further exploration in this direction. While it may be possible to design a stricter reward mechanism\u2014such as enforcing\n\nf(g(x)) \u0338= g(x) by executing the code via a Python interpreter and penalizing such shortcuts\u2014we leave this to future work.\n\nby executing the code via a Python interpreter and penalizing such shortcuts\u2014we leave this to future work.\n\n(\n\n(\n\n))\n\n=\n\n(\n\n)\n\nf\n\ng\n\nx\n\ng\n\nx\n\n\u0338\n\nD.3. Toying with the Initial p(z)\n\nD.3. Toying with the Initial p(z)\n\n(\n\n)\n\nD.3. Toying with the Initial\n\np\n\nz\n\nWe investigated a setting where the initial seed buffer (see Section 3.3.1 on how we generated these), i.e. p(z) in Equation (3), is not\nself-generated by the base model, but instead sourced from the LeetCode Dataset. We only modified this component and ran AZR\nusing the same procedure as before, continuing to add new valid programs to the initialized buffer. We observed an increase in initial\nperformance on coding benchmarks; however, the performance plateaued at roughly the same level after additional training steps,\ncompared to our official AZR setup. Interestingly, math performance was lower than in the official AZR setup, pointing towards that\non-policy data may be more beneficial to the learner to bootstrap from for mathematical reasoning. We believe that exploring different\nstrategies for initializing and updating p(z) is an important and exciting direction for future research. We briefly explored different\nstrategies for sampling reference code, ultimately settling on uniform sampling for its simplicity, though we also experimented with\nrecency-based sampling and observed potential collapse.\n\nWe investigated a setting where the initial seed buffer (see Section 3.3.1 on how we generated these), i.e. p(z) in Equation (3), is not\n\nWe investigated a setting where the initial seed buffer (see Section\n\n3.3.1\n\non how we generated these),\n\n.\n\nin Equation\n\n(3)\n\n, is not\n\n(\n\n)\n\ni.e\n\np\n\nz\n\nself-generated by the base model, but instead sourced from the LeetCode Dataset. We only modified this component and ran AZR\n\nself-generated by the base model, but instead sourced from the\n\nLeetCode Dataset.\n\nWe only modified this component and ran AZR\n\nusing the same procedure as before, continuing to add new valid programs to the initialized buffer. We observed an increase in initial\n\nusing the same procedure as before, continuing to add new valid programs to the initialized buffer. We observed an increase in initial\n\nperformance on coding benchmarks; however, the performance plateaued at roughly the same level after additional training steps,\n\nperformance on coding benchmarks; however, the performance plateaued at roughly the same level after additional training steps,\n\ncompared to our official AZR setup. Interestingly, math performance was lower than in the official AZR setup, pointing towards that\n\ncompared to our official AZR setup. Interestingly, math performance was lower than in the official AZR setup, pointing towards that\n\non-policy data may be more beneficial to the learner to bootstrap from for mathematical reasoning. We believe that exploring different\n\non-policy data may be more beneficial to the learner to bootstrap from for mathematical reasoning. We believe that exploring different\n\nstrategies for initializing and updating p(z) is an important and exciting direction for future research. We briefly explored different\n\nstrategies for initializing and updating\n\nis an important and exciting direction for future research. We briefly explored different\n\n(\n\n)\n\np\n\nz\n\nstrategies for sampling reference code, ultimately settling on uniform sampling for its simplicity, though we also experimented with\n\nstrategies for sampling reference code, ultimately settling on uniform sampling for its simplicity, though we also experimented with\n\nrecency-based sampling and observed potential collapse.\n\nrecency-based sampling and observed potential collapse.\n\nD.4. Extra Rewards\n\nD.4. Extra Rewards\n\nD.4. Extra Rewards\n\nComplexity Rewards. Code complexity is well studied in software science and could potentially be a good proxy for measuring\nhow hard it is to infer the properties of a piece of code for our reasoning learner. Therefore, for the problem proposer, we can add various\nmeasures of complexity\u2014such as Cyclomatic Complexity (Ebert et al., 2016), maintainability, etc.\u2014to the reward function to incentivize\nthe proposer to produce more complex programs. For illustration purposes, we tried using the Maintainability measure and the Halstead\n\nComplexity Rewards. Code complexity is well studied in software science and could potentially be a good proxy for measuring\n\nCode complexity is well studied in software science and could potentially be a good proxy for measuring\n\nComplexity Rewards.\n\nhow hard it is to infer the properties of a piece of code for our reasoning learner. Therefore, for the problem proposer, we can add various\n\nhow hard it is to infer the properties of a piece of code for our reasoning learner. Therefore, for the problem proposer, we can add various\n\nmeasures of complexity\u2014such as Cyclomatic Complexity (Ebert et al., 2016), maintainability, etc.\u2014to the reward function to incentivize\n\nmeasures of complexity\u2014such as Cyclomatic Complexity\n\n(Ebert et al.,\n\n, maintainability, etc.\u2014to the reward function to incentivize\n\n2016)\n\nthe proposer to produce more complex programs. For illustration purposes, we tried using the Maintainability measure and the Halstead\n\nthe proposer to produce more complex programs. For illustration purposes, we tried using the Maintainability measure and the Halstead\n\n49\n\n49\n\n49\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\ncomplexity measure (Halstead, 1977) as intrinsic rewards. Concretely, we used the complexipy and Radon packages (Lopez, 2025;\nCanal, 2023) to implement the respective metrics. These are then served as intrinsic rewards during the AZR self-play phase.\n\ncomplexity measure (Halstead, 1977) as intrinsic rewards. Concretely, we used the complexipy and Radon packages (Lopez, 2025;\n\ncomplexity measure\n\n(Halstead,\n\n1977)\n\nas intrinsic rewards. Concretely, we used the\n\nand\n\npackages\n\n(Lopez,\n\n2025;\n\ncomplexipy\n\nRadon\n\nCanal, 2023) to implement the respective metrics. These are then served as intrinsic rewards during the AZR self-play phase.\n\nCanal,\n\n2023)\n\nto implement the respective metrics. These are then served as intrinsic rewards during the AZR self-play phase.\n\nDiversity Rewards. We also attempted using diversity rewards to . Inspired by DiveR-CT (Zhao et al., 2025a), we incorporate\ncode edit distance as an intrinsic reward. Specifically, we treat the reference programs shown in the prompt as anchors and compute the\naverage code edit distance between the generated program and these anchors. This serves as a measure of diversity in the generated\noutput. Additionally, we explored another diversity-based reward inspired by the notion of surprise (Zhao et al., 2022). In this approach,\nwe construct a probability distribution over previously encountered input/output pairs that the solver has answered. The reward is then\ndefined as 1 \u2212 p(input/output), where p denotes the empirical probability of a particular input or output. While both strategies were\nevaluated in our experiments, we did not observe a significant difference in performance. However, we believe this aspect warrants\ndeeper investigation, as diversity rewards remain a promising avenue for strengthening AZR further.\n\nDiversity Rewards. We also attempted using diversity rewards to . Inspired by DiveR-CT (Zhao et al., 2025a), we incorporate\n\nWe also attempted using diversity rewards to . Inspired by DiveR-CT\n\n(Zhao et al.,\n\n2025a)\n\n, we incorporate\n\nDiversity Rewards.\n\ncode edit distance as an intrinsic reward. Specifically, we treat the reference programs shown in the prompt as anchors and compute the\n\nas an intrinsic reward. Specifically, we treat the reference programs shown in the prompt as anchors and compute the\n\ncode edit distance\n\naverage code edit distance between the generated program and these anchors. This serves as a measure of diversity in the generated\n\naverage code edit distance between the generated program and these anchors. This serves as a measure of diversity in the generated\n\noutput. Additionally, we explored another diversity-based reward inspired by the notion of surprise (Zhao et al., 2022). In this approach,\n\noutput. Additionally, we explored another diversity-based reward inspired by the notion of\n\n(Zhao et al.,\n\n. In this approach,\n\n2022)\n\nsurprise\n\nwe construct a probability distribution over previously encountered input/output pairs that the solver has answered. The reward is then\n\nwe construct a probability distribution over previously encountered input/output pairs that the solver has answered. The reward is then\n\ndefined as 1 \u2212 p(input/output), where p denotes the empirical probability of a particular input or output. While both strategies were\n\ndefined as\n\n, where\n\ndenotes the empirical probability of a particular input or output. While both strategies were\n\ninput/output\n\n1\n\n(\n\n)\n\np\n\np\n\n\u2212\n\nevaluated in our experiments, we did not observe a significant difference in performance. However, we believe this aspect warrants\n\nevaluated in our experiments, we did not observe a significant difference in performance. However, we believe this aspect warrants\n\ndeeper investigation, as diversity rewards remain a promising avenue for strengthening AZR further.\n\ndeeper investigation, as diversity rewards remain a promising avenue for strengthening AZR further.\n\nReward Aggregation. We tested several ways on how to combine rewards for the proposer and discriminator. First, we separate\nthe reward into extrinsic reward rextrinsic and a set of intrinsic reward(s) I = {ri}, and tested the following strategies to combine them\ninto a single reward,\n\nReward Aggregation. We tested several ways on how to combine rewards for the proposer and discriminator. First, we separate\n\nWe tested several ways on how to combine rewards for the proposer and discriminator. First, we separate\n\nReward Aggregation.\n\nthe reward into extrinsic reward rextrinsic and a set of intrinsic reward(s) I = {ri}, and tested the following strategies to combine them\n\nthe reward into extrinsic reward\n\nand a set of intrinsic reward(s)\n\n, and tested the following strategies to combine them\n\n=\n\nr\n\nI\n\nr\n\nextrinsic\n\n{\n\n}\n\ni\n\ninto a single reward,\n\ninto a single reward,\n\nr = rextrinsic +\nX\n|I|\ni\nri, (11)\n\n|I|\n\nI\n\n|\n\n|\n\nX\n\nX\n\nri, (11)\n\n(11)\n\nr = rextrinsic +\n\n=\n\n+\n\nr\n\nr\n\nr\n\n,\n\nextrinsic\n\ni\n\ni\n\ni\n\nr = rextrinsic \u00b7\nX\n|I|\ni\nri, (12)\n\n|I|\n\nI\n\n|\n\n|\n\nX\n\nX\n\nri, (12)\n\n(12)\n\nr = rextrinsic \u00b7\n\n=\n\nr\n\nr\n\nr\n\n,\n\nextrinsic\n\n\u00b7\n\ni\n\ni\n\ni\n\nr = rextrinsic \u00b7\nY\n|I|\ni\nri, (13)\n\n|I|\n\nI\n\n|\n\n|\n\nY\n\nY\n\nri, (13)\n\n(13)\n\nr = rextrinsic \u00b7\n\n=\n\nr\n\nr\n\nr\n\n,\n\nextrinsic\n\n\u00b7\n\ni\n\ni\n\ni\n\nr = rextrinsic +\nY\n|I|\ni\nri. (14)\n\n|I|\n\nI\n\n|\n\n|\n\nY\n\nY\n\nri. (14)\n\n(14)\n\nr = rextrinsic +\n\n=\n\n+\n\nr\n\nr\n\nr\n\n.\n\nextrinsic\n\ni\n\ni\n\ni\n\nWe found that the simple additive way of combining rewards, a.k.a Equation (11), produced the most stable runs, possibly due to less\nvariance.\n\nWe found that the simple additive way of combining rewards, a.k.a Equation (11), produced the most stable runs, possibly due to less\n\nWe found that the simple additive way of combining rewards, a.k.a Equation\n\n(11)\n\n, produced the most stable runs, possibly due to less\n\nvariance.\n\nvariance.\n\nD.5. Environment Transition\n\nD.5. Environment Transition\n\nD.5. Environment Transition\n\nWe investigated how the transition function in our coding environment for the proposer. Specifically, after generating a piece of code, we\ncan apply a transformation function on it before giving it making it an valid tuple in our dataset. We investigated two\n\nWe investigated how the transition function in our coding environment for the proposer. Specifically, after generating a piece of code, we\n\nWe investigated how the transition function in our coding environment for the proposer. Specifically, after generating a piece of code, we\n\ncan apply a transformation function on it before giving it making it an valid tuple in our dataset. We investigated two\n\ncan apply a transformation function on it before giving it making it an valid tuple in our dataset. We investigated two\n\nRemoving Comments and Docstrings In early iterations of our experiments, we noticed that comments and docstrings\nwere sometimes used to explicitly outline what the function was doing, or even served as a partial \"note-taking\" interleaved \"ReAct\"\nprocess (Yao et al., 2023) of generating code\u2014that is, the model could interleave think and action at the same time, and to make the\ngenerated code valid, it used comments to encase its thoughts (Appendix C.3), similarly observed in DeepSeek-Prover-V2: (Ren et al.,\n2025). We then thought that to make the task harder for the solver, we should occlude this information from it. However, we observed\na significant performance drop after removing all comments and docstrings. One explanation for this phenomenon is that the only\n\"communication\" channel between the proposer and the solver is restricted to the code itself, rather than some kind of \"message\" along\nwith the code. These messages can potentially provide hints to the solver, thus making some otherwise impossible tasks solvable. As a\nresult, the solver is able to learn from its experience and self-bootstrap out of certain unsolvable tasks.\n\nRemoving Comments and Docstrings In early iterations of our experiments, we noticed that comments and docstrings\n\nIn early iterations of our experiments, we noticed that comments and docstrings\n\nRemoving Comments and Docstrings\n\nwere sometimes used to explicitly outline what the function was doing, or even served as a partial \"note-taking\" interleaved \"ReAct\"\n\nwere sometimes used to explicitly outline what the function was doing, or even served as a partial \"note-taking\" interleaved \"ReAct\"\n\nprocess (Yao et al., 2023) of generating code\u2014that is, the model could interleave think and action at the same time, and to make the\n\nprocess\n\n(Yao et al.,\n\n2023)\n\nof generating code\u2014that is, the model could interleave think and action at the same time, and to make the\n\ngenerated code valid, it used comments to encase its thoughts (Appendix C.3), similarly observed in DeepSeek-Prover-V2: (Ren et al.,\n\ngenerated code valid, it used comments to encase its thoughts (Appendix\n\nC.3)\n\n, similarly observed in DeepSeek-Prover-V2:\n\n(Ren et al.,\n\n2025). We then thought that to make the task harder for the solver, we should occlude this information from it. However, we observed\n\n2025)\n\n. We then thought that to make the task harder for the solver, we should occlude this information from it. However, we observed\n\na significant performance drop after removing all comments and docstrings. One explanation for this phenomenon is that the only\n\na significant performance drop after removing all comments and docstrings. One explanation for this phenomenon is that the only\n\n\"communication\" channel between the proposer and the solver is restricted to the code itself, rather than some kind of \"message\" along\n\n\"communication\" channel between the proposer and the solver is restricted to the code itself, rather than some kind of \"message\" along\n\nwith the code. These messages can potentially provide hints to the solver, thus making some otherwise impossible tasks solvable. As a\n\nwith the code. These messages can potentially provide hints to the solver, thus making some otherwise impossible tasks solvable. As a\n\nresult, the solver is able to learn from its experience and self-bootstrap out of certain unsolvable tasks.\n\nresult, the solver is able to learn from its experience and self-bootstrap out of certain unsolvable tasks.\n\nRemoving Global Variables. We observed that some programs contain globally declared variables that may inadvertently leak\ninformation about the correct answer\u2014this issue is particularly prevalent in the input induction task generation and solving. Initially, we\nwere concerned that such leakage might lead to wasted computation on trivial or compromised examples. To address this, we developed\na systematic procedure to remove globally declared variables from the generated programs.\n\nRemoving Global Variables. We observed that some programs contain globally declared variables that may inadvertently leak\n\nWe observed that some programs contain globally declared variables that may inadvertently leak\n\nRemoving Global Variables.\n\ninformation about the correct answer\u2014this issue is particularly prevalent in the input induction task generation and solving. Initially, we\n\ninformation about the correct answer\u2014this issue is particularly prevalent in the input induction task generation and solving. Initially, we\n\nwere concerned that such leakage might lead to wasted computation on trivial or compromised examples. To address this, we developed\n\nwere concerned that such leakage might lead to wasted computation on trivial or compromised examples. To address this, we developed\n\na systematic procedure to remove globally declared variables from the generated programs.\n\na systematic procedure to remove globally declared variables from the generated programs.\n\nHowever, after applying this cleaning step, we observed a noticeable drop in performance on our self-play reasoning tasks. One possible\nexplanation is that the generation step is unaware of this post-processing modification; since the reward is assigned after the transition\nfunction (which includes variable removal), the model may not learn effectively from this mismatch.\n\nHowever, after applying this cleaning step, we observed a noticeable drop in performance on our self-play reasoning tasks. One possible\n\nHowever, after applying this cleaning step, we observed a noticeable drop in performance on our self-play reasoning tasks. One possible\n\nexplanation is that the generation step is unaware of this post-processing modification; since the reward is assigned after the transition\n\nexplanation is that the generation step is unaware of this post-processing modification; since the reward is assigned after the transition\n\nfunction (which includes variable removal), the model may not learn effectively from this mismatch.\n\nfunction (which includes variable removal), the model may not learn effectively from this mismatch.\n\nMoreover, we believe that even when answers are present, the solver still engages in nontrivial reasoning to reach a solution, potentially\nbenefiting from this exposure. This aligns with the idea of rationalization as proposed in STaR (Zelikman et al., 2022), where the model\npretends to not see the answer but still performs reasoning during learning. Therefore, in our final experiments, we choose not to remove\nglobally declared variables, allowing the self-play loop to naturally incorporate and adapt to such cases.\n\nMoreover, we believe that even when answers are present, the solver still engages in nontrivial reasoning to reach a solution, potentially\n\nMoreover, we believe that even when answers are present, the solver still engages in nontrivial reasoning to reach a solution, potentially\n\nbenefiting from this exposure. This aligns with the idea of rationalization as proposed in STaR (Zelikman et al., 2022), where the model\n\nbenefiting from this exposure. This aligns with the idea of rationalization as proposed in STaR\n\n(Zelikman et al.,\n\n, where the model\n\n2022)\n\npretends to not see the answer but still performs reasoning during learning. Therefore, in our final experiments, we choose not to remove\n\npretends to not see the answer but still performs reasoning during learning. Therefore, in our final experiments, we choose not to remove\n\nglobally declared variables, allowing the self-play loop to naturally incorporate and adapt to such cases.\n\nglobally declared variables, allowing the self-play loop to naturally incorporate and adapt to such cases.\n\n50\n\n50\n\n50",
    "pages": [
      {
        "page_num": 0,
        "text": "May 9, 2025\n\nMay 9, 2025\n\nMay 9, 2025\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAndrew Zhao 1\n, Yiran Wu3\n, Yang Yue 1\n, Tong Wu2\n, Quentin Xu1\n, Yang Yue 1\n, Matthieu Lin1\n,\nShenzhi Wang 1\n, Qingyun Wu3\n, Zilong Zheng 2, and Gao Huang 1,\n\n, Yiran Wu3\n\n3\n\n, Yang Yue 1\n\n1\n\n, Tong Wu2\n\n2\n\n, Quentin Xu1\n\n1\n\n, Yang Yue 1\n\n1\n\n, Matthieu Lin1\n\n1\n\nAndrew Zhao 1\n\n1\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nYiran Wu\n\nYang Yue\n\nTong Wu\n\nQuentin Xu\n\nYang Yue\n\nMatthieu Lin\n\nAndrew Zhao\n\n, Zilong Zheng 2, and Gao Huang 1,\n\n,\n\n,\n\nShenzhi Wang 1\n\n1\n\n, Qingyun Wu3\n\n3\n\n2\n\n1\n\nShenzhi Wang\n\n, Qingyun Wu\n\n, Zilong Zheng\n\nand Gao Huang\n\n1 Tsinghua University 2 Beijing Institute for General Artificial Intelligence 3 Pennsylvania State University\n\n1 Tsinghua University 2 Beijing Institute for General Artificial Intelligence 3 Pennsylvania State University\n\nTsinghua University\n\nBeijing Institute for General Artificial Intelligence\n\nPennsylvania State University\n\n1\n\n2\n\n3\n\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\n\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\n\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\n\narXiv:2505.03335v2 [cs.LG] 7 May 2025\n\narXiv:2505.03335v2 [cs.LG] 7 May 2025\n\narXiv:2505.03335v2 [cs.LG] 7 May 2025\n\nReinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning\ncapabilities of large language models by learning directly from outcome-based rewards. Recent RLVR\nworks that operate under the zero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training. The scarcity of high-\nquality, human-produced examples raises concerns about the long-term scalability of relying on human\nsupervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited\nlearning potential for a superintelligent system. To address these concerns, we propose a new RLVR\nparadigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own\nlearning progress and improves reasoning by solving them, without relying on any external data. Under\nthis paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training\ncurriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks\nand verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded\nlearning. Despite being trained entirely without external data, AZR achieves overall SOTA performance\non coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be\neffectively applied across different model scales and is compatible with various model classes.\n\nReinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning\n\nReinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning\n\ncapabilities of large language models by learning directly from outcome-based rewards. Recent RLVR\n\ncapabilities of large language models by learning directly from outcome-based rewards. Recent RLVR\n\nworks that operate under the zero setting avoid supervision in labeling the reasoning process, but still\n\nworks that operate under the\n\navoid supervision in labeling the reasoning process, but still\n\nzero setting\n\ndepend on manually curated collections of questions and answers for training. The scarcity of high-\n\ndepend on manually curated collections of questions and answers for training. The scarcity of high-\n\nquality, human-produced examples raises concerns about the long-term scalability of relying on human\n\nquality, human-produced examples raises concerns about the long-term scalability of relying on human\n\nsupervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a\n\nsupervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a\n\nhypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited\n\nhypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited\n\nlearning potential for a superintelligent system. To address these concerns, we propose a new RLVR\n\nlearning potential for a superintelligent system. To address these concerns, we propose a new RLVR\n\nparadigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own\n\nparadigm called\n\n, in which a single model learns to propose tasks that maximize its own\n\nAbsolute Zero\n\nlearning progress and improves reasoning by solving them, without relying on any external data. Under\n\nlearning progress and improves reasoning by solving them, without relying on any external data. Under\n\nthis paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training\n\nthis paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training\n\ncurriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks\n\ncurriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks\n\nand verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded\n\nand verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded\n\nlearning. Despite being trained entirely without external data, AZR achieves overall SOTA performance\n\nlearning. Despite being trained entirely\n\n, AZR achieves overall SOTA performance\n\nwithout external data\n\non coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens\n\non coding and mathematical reasoning tasks,\n\nthat rely on tens\n\noutperforming existing zero-setting models\n\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be\n\nof thousands of\n\n. Furthermore, we demonstrate that AZR can be\n\nin-domain human-curated examples\n\neffectively applied across different model scales and is compatible with various model classes.\n\neffectively applied across different model scales and is compatible with various model classes.\n\nCode Project Page Logs Models\nFigure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\n\nCode Project Page Logs Models\n\nCode Project Page Logs Models\n\nCode\n\nProject Page\n\nLogs\n\nModels\n\nFigure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\n\nFigure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\n\n. Without relying on any gold\n\nAbsolute Zero Reasoner (AZR) achieves state-of-the-art performance with\n\nZERO DATA\n\nFigure 1.\n\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\n\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\n\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\n\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\n\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\n\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\n\nCorresponding author(s)\n\nCorresponding author(s)\n\nCorresponding author(s)",
        "tables": []
      },
      {
        "page_num": 1,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nSupervised Learning Reinforcement Learning with Verifiable Rewards Absolute Zero (Ours)\nLess Human Supervision\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nSupervised Learning Reinforcement Learning with Verifiable Rewards Absolute Zero (Ours)\n\nSupervised Learning Reinforcement Learning with Verifiable Rewards Absolute Zero (Ours)\n\nReinforcement Learning with Verifiable Rewards\n\nAbsolute Zero (Ours)\n\nSupervised Learning\n\nLess Human Supervision\n\nLess Human Supervision\n\nLess Human Supervision\n\nFigure 2. Absolute Zero Paradigm. Supervised learning relies on human-curated reasoning traces for behavior cloning. Reinforcement\nlearning from verified rewards, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a\nrespective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm, Absolute\nZero, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks\noptimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that\nprovides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.\n\nFigure 2. Absolute Zero Paradigm. Supervised learning relies on human-curated reasoning traces for behavior cloning. Reinforcement\n\nrelies on human-curated reasoning traces for behavior cloning.\n\nAbsolute Zero Paradigm.\n\nSupervised learning\n\nFigure 2.\n\nReinforcement\n\nlearning from verified rewards, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a\n\n, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a\n\nlearning from verified rewards\n\nrespective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm, Absolute\n\nrespective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm,\n\nAbsolute\n\nZero, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks\n\n, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks\n\nZero\n\noptimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that\n\noptimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that\n\nprovides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.\n\nprovides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.\n\n1. Introduction\n\n1. Introduction\n\n1. Introduction\n\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement\nLearning with Verifiable Rewards (RLVR) (Lambert et al., 2024). Unlike methods that explicitly imitate intermediate reasoning steps,\nRLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets (DeepSeek-AI et al., 2025;\nTeam et al., 2025; Jaech et al., 2024; OpenAI, 2025b;a). A particularly compelling variant is the \"zero\" RLVR paradigm (DeepSeek-AI\net al., 2025), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies\nRLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of\nreasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability (Villalobos et al., 2024). As reasoning\nmodels continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable (Yue\net al., 2025). A similar scalability bottleneck has already been identified in the domain of LLM pretraining (Sutskever et al., 2024).\nFurthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed\ntasks risks imposing constraints on their capacity for autonomous learning and growth (Hughes et al., 2024). This underscores the need\nfor a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which\nAI systems may surpass human intelligence.\n\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement\n\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement\n\nLearning with Verifiable Rewards (RLVR) (Lambert et al., 2024). Unlike methods that explicitly imitate intermediate reasoning steps,\n\nLearning with Verifiable Rewards (RLVR)\n\n(Lambert et al.,\n\n2024)\n\n. Unlike methods that explicitly imitate intermediate reasoning steps,\n\nRLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets (DeepSeek-AI et al., 2025;\n\nRLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets\n\n(DeepSeek-AI et al.,\n\n2025;\n\nTeam et al., 2025; Jaech et al., 2024; OpenAI, 2025b;a). A particularly compelling variant is the \"zero\" RLVR paradigm (DeepSeek-AI\n\nTeam et al.,\n\nJaech et al.,\n\n. A particularly compelling variant is the\n\nRLVR paradigm\n\n2025;\n\n2024;\n\nOpenAI,\n\n2025b;\n\na)\n\n(DeepSeek-AI\n\n\"zero\"\n\net al., 2025), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies\n\net al.,\n\n, which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies\n\n2025)\n\nRLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of\n\nRLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of\n\nreasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability (Villalobos et al., 2024). As reasoning\n\nreasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability\n\n(Villalobos et al.,\n\n. As reasoning\n\n2024)\n\nmodels continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable (Yue\n\nmodels continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable\n\n(Yue\n\net al., 2025). A similar scalability bottleneck has already been identified in the domain of LLM pretraining (Sutskever et al., 2024).\n\net al.,\n\n2025)\n\n. A similar scalability bottleneck has already been identified in the domain of LLM pretraining\n\n(Sutskever et al.,\n\n2024)\n\n.\n\nFurthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed\n\nFurthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed\n\ntasks risks imposing constraints on their capacity for autonomous learning and growth (Hughes et al., 2024). This underscores the need\n\ntasks risks imposing constraints on their capacity for autonomous learning and growth\n\n(Hughes et al.,\n\n. This underscores the need\n\n2024)\n\nfor a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which\n\nfor a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which\n\nAI systems may surpass human intelligence.\n\nAI systems may surpass human intelligence.\n\nTo this end, we propose \"Absolute Zero\", a new paradigm for reasoning models in which the model simultaneously learns to define tasks\nthat maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In\ncontrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to\nhacking (Silver et al., 2017; Chen et al., 2025; 2024), the Absolute Zero paradigm is designed to operate in open-ended settings while\nremaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how\nhumans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models (Hughes\net al., 2024). Similar to AlphaZero (Silver et al., 2017), which improves through self-play, our proposed paradigm requires no human\nsupervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward\nenabling large language models to autonomously achieve superhuman reasoning capabilities.\n\nTo this end, we propose \"Absolute Zero\", a new paradigm for reasoning models in which the model simultaneously learns to define tasks\n\nTo this end, we propose\n\n, a new paradigm for reasoning models in which the model simultaneously learns to define tasks\n\n\"Absolute Zero\"\n\nthat maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In\n\nthat maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In\n\ncontrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to\n\ncontrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to\n\nhacking (Silver et al., 2017; Chen et al., 2025; 2024), the Absolute Zero paradigm is designed to operate in open-ended settings while\n\nhacking\n\n(Silver et al.,\n\n2017;\n\nChen et al.,\n\n2025;\n\n2024)\n\n, the\n\nparadigm is designed to operate in open-ended settings while\n\nAbsolute Zero\n\nremaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how\n\nremaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how\n\nhumans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models (Hughes\n\nhumans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models\n\n(Hughes\n\net al., 2024). Similar to AlphaZero (Silver et al., 2017), which improves through self-play, our proposed paradigm requires no human\n\net al.,\n\n2024)\n\n. Similar to AlphaZero\n\n(Silver et al.,\n\n2017)\n\n, which improves through self-play, our proposed paradigm requires no human\n\nsupervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward\n\nsupervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward\n\nenabling large language models to autonomously achieve superhuman reasoning capabilities.\n\nenabling large language models to autonomously achieve superhuman reasoning capabilities.\n\nBuilding on this new reasoning paradigm, we introduce the Absolute Zero Reasoner (AZR), which proposes and solves coding tasks. We\ncast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable\nfeedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a\nprogram, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We\ntrain the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of\nthe proposed approach.\n\nBuilding on this new reasoning paradigm, we introduce the Absolute Zero Reasoner (AZR), which proposes and solves coding tasks. We\n\nBuilding on this new reasoning paradigm, we introduce the\n\n, which proposes and solves coding tasks. We\n\nAbsolute Zero Reasoner (AZR)\n\ncast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable\n\ncast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable\n\nfeedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a\n\nfeedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a\n\nprogram, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We\n\nprogram, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We\n\ntrain the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of\n\ntrain the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of\n\nthe proposed approach.\n\nthe proposed approach.\n\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks\nin math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned\nwith domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically\ntrained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points\n\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks\n\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks\n\nin math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned\n\nin math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned\n\nwith domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically\n\nwith domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically\n\ntrained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points\n\ntrained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points\n\n2\n\n2\n\n2",
        "tables": []
      },
      {
        "page_num": 2,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\ncompared to models trained in the \"zero\" setting using in-domain data. These surprising results highlight that general reasoning skills\ncan emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a\nfirst pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting\nfindings summarized below:\n\ncompared to models trained in the \"zero\" setting using in-domain data. These surprising results highlight that general reasoning skills\n\ncompared to models trained in the \"zero\" setting using in-domain data. These surprising results highlight that general reasoning skills\n\ncan emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a\n\ncan emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a\n\nfirst pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting\n\nfirst pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting\n\nfindings summarized below:\n\nfindings summarized below:\n\n\u2022 Code priors amplify reasoning. The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b.\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\nmay potentially amplify overall reasoning improvements after AZR training.\n\u2022 Cross domain transfer is more pronounced for AZR. After RLVR, expert code models raise math accuracy by only 0.65 points on\naverage, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\u2022 Bigger bases yield bigger gains. Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\u2022 Comments as intermediate plans emerge naturally. When solving code induction tasks, AZR often interleaves step-by-step plans\nas comments and code (Appendix C.3), resembling the ReAct prompting framework (Yao et al., 2023). Similar behavior has been\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B) (Ren et al., 2025). We therefore believe that allowing\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\u2022 Cognitive Behaviors and Token length depends on reasoning mode. Distinct cognitive behaviors\u2014such as step-by-step reasoning,\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\u2022 Safety alarms ringing. We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the\n\"uh-oh moment\", example shown in Figure 32, highlighting the need for future work on safety-aware training (Zhang et al., 2025a).\n\n\u2022 Code priors amplify reasoning. The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b.\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\nmay potentially amplify overall reasoning improvements after AZR training.\n\n\u2022 Code priors amplify reasoning. The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b.\n\nThe base\n\nmodel started with math performance 3.6 points lower than\n\n.\n\n\u2022\n\nCode priors amplify reasoning.\n\nQwen-Coder-7b\n\nQwen-7b\n\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\n\nBut after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities\n\nmay potentially amplify overall reasoning improvements after AZR training.\n\nmay potentially amplify overall reasoning improvements after AZR training.\n\n\u2022 Cross domain transfer is more pronounced for AZR. After RLVR, expert code models raise math accuracy by only 0.65 points on\naverage, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\n\u2022 Cross domain transfer is more pronounced for AZR. After RLVR, expert code models raise math accuracy by only 0.65 points on\n\nAfter RLVR, expert code models raise math accuracy by only 0.65 points on\n\n\u2022\n\nCross domain transfer is more pronounced for AZR.\n\naverage, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and\n\naverage, whereas\n\ntrained on self-proposed code reasoning tasks improve math average by 10.9 and\n\nand\n\nAZR-Base-7B\n\nAZR-Coder-7B\n\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\n15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\n\u2022 Bigger bases yield bigger gains. Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\n\u2022 Bigger bases yield bigger gains. Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n\nPerformance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7,\n\n\u2022\n\nBigger bases yield bigger gains.\n\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\n+10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\n\u2022 Comments as intermediate plans emerge naturally. When solving code induction tasks, AZR often interleaves step-by-step plans\nas comments and code (Appendix C.3), resembling the ReAct prompting framework (Yao et al., 2023). Similar behavior has been\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B) (Ren et al., 2025). We therefore believe that allowing\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\n\u2022 Comments as intermediate plans emerge naturally. When solving code induction tasks, AZR often interleaves step-by-step plans\n\nWhen solving code induction tasks, AZR often interleaves step-by-step plans\n\n\u2022\n\nComments as intermediate plans emerge naturally.\n\nas comments and code (Appendix C.3), resembling the ReAct prompting framework (Yao et al., 2023). Similar behavior has been\n\nas comments and code (Appendix\n\nC.3)\n\n, resembling the ReAct prompting framework\n\n(Yao et al.,\n\n2023)\n\n. Similar behavior has been\n\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B) (Ren et al., 2025). We therefore believe that allowing\n\nobserved in much larger formal-math models such as DeepSeek Prover v2 (671B)\n\n(Ren et al.,\n\n. We therefore believe that allowing\n\n2025)\n\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\nthe model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\n\u2022 Cognitive Behaviors and Token length depends on reasoning mode. Distinct cognitive behaviors\u2014such as step-by-step reasoning,\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\n\u2022 Cognitive Behaviors and Token length depends on reasoning mode. Distinct cognitive behaviors\u2014such as step-by-step reasoning,\n\nDistinct cognitive behaviors\u2014such as step-by-step reasoning,\n\n\u2022\n\nCognitive Behaviors and Token length depends on reasoning mode.\n\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\n\nenumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different\n\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\n\ntypes of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction\n\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\ngrows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\n\u2022 Safety alarms ringing. We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the\n\"uh-oh moment\", example shown in Figure 32, highlighting the need for future work on safety-aware training (Zhang et al., 2025a).\n\n\u2022 Safety alarms ringing. We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the\n\nWe observe AZR with\n\noccasionally produces concerning chains of thought, we term the\n\n\u2022\n\nSafety alarms ringing.\n\nLlama3.1-8b\n\n\"uh-oh moment\", example shown in Figure 32, highlighting the need for future work on safety-aware training (Zhang et al., 2025a).\n\n\"uh-oh moment\", example shown in Figure\n\n32,\n\nhighlighting the need for future work on safety-aware training\n\n(Zhang et al.,\n\n2025a)\n\n.\n\n2. The Absolute Zero Paradigm\n\n2. The Absolute Zero Paradigm\n\n2. The Absolute Zero Paradigm\n\n2.1. Preliminaries\n\n2.1. Preliminaries\n\n2.1. Preliminaries\n\nSupervised Fine-Tuning (SFT). SFT requires the datasets of task-rationale-answer demonstrations D = {(x, c\u22c6\n, y\u22c6\n)}, where\nx is the query, c\n\u22c6\nis the gold chain-of-thought (CoT)) and y\n\u22c6\nis the gold answer, all provided by human experts or superior AI models.\nThe model trains to imitate the reference responses to minimize the conditional negative log-likelihood (Ouyang et al., 2022):\n\nSupervised Fine-Tuning (SFT). SFT requires the datasets of task-rationale-answer demonstrations D = {(x, c\u22c6\n\nSFT requires the datasets of task-rationale-answer demonstrations\n\n)}, where\n\n, where\n\n\u22c6\n\n, y\u22c6\n\n\u22c6\n\nSupervised Fine-Tuning (SFT).\n\n=\n\n(\n\n)\n\nx, c\n\n, y\n\nD\n\n{\n\n}\n\nx is the query, c\n\nis the query,\n\nis the gold chain-of-thought (CoT)) and y\n\nis the gold chain-of-thought (CoT)) and\n\nis the gold answer, all provided by human experts or superior AI models.\n\nis the gold answer, all provided by\n\nhuman experts\n\nor\n\nsuperior AI models.\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\nx\n\nc\n\ny\n\nThe model trains to imitate the reference responses to minimize the conditional negative log-likelihood (Ouyang et al., 2022):\n\nThe model trains to imitate the reference responses to minimize the conditional negative log-likelihood\n\n(Ouyang et al.,\n\n2022)\n\n:\n\nLSFT(\u03b8) = \u2212 E(x,c\u22c6,y\u22c6)\u223cD log \u03c0\u03b8\n\nc\n\u22c6\n, y\n\u22c6\n| x). (1)\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\nLSFT(\u03b8) = \u2212 E(x,c\u22c6,y\u22c6)\u223cD log \u03c0\u03b8\n\nE\n\n| x). (1)\n\n(1)\n\n(\n\n) =\n\nlog\n\n)\n\n\u03b8\n\n\u03c0\n\nc\n\nc\n\n, y\n\n, y\n\nx\n\n.\n\nSFT\n\nL\n\n\u2212\n\n|\n\n\u22c6\n\n\u22c6\n\n\u03b8\n\n(\n\n)\n\nx,c\n\n,y\n\n\u223cD\n\nHowever, at the frontier level, there's no stronger model to distill from, and expert human labeling doesn't scale well.\n\nHowever, at the frontier level, there's no stronger model to distill from, and expert human labeling doesn't scale well.\n\nHowever, at the frontier level, there's no stronger model to distill from, and expert human labeling doesn't scale well.\n\nReinforcement Learning with Verifiable Rewards (RLVR). To move beyond the limits of pure imitation, RLVR only\nrequires a dataset of task and answer D = {(x, y\u22c6\n)}, without labeled rationale. RLVR allows the model to generate its own CoT and\ncalculate a verifiable reward with the golden answer r(y, y\u22c6\n). However, the learning task distribution D, with its set of queries and gold\nanswers are still labeled by human experts. The trainable policy \u03c0\u03b8 is optimized to maximize expected reward:\n\nReinforcement Learning with Verifiable Rewards (RLVR). To move beyond the limits of pure imitation, RLVR only\n\nTo move beyond the limits of pure imitation, RLVR only\n\nReinforcement Learning with Verifiable Rewards (RLVR).\n\nrequires a dataset of task and answer D = {(x, y\u22c6\n\nrequires a dataset of task and answer\n\n)}, without labeled rationale. RLVR allows the model to generate its own CoT and\n\n, without labeled rationale. RLVR allows the model to generate its own CoT and\n\n\u22c6\n\n=\n\n(\n\n)\n\nx, y\n\nD\n\n{\n\n}\n\ncalculate a verifiable reward with the golden answer r(y, y\u22c6\n\ncalculate a verifiable reward with the golden answer\n\n). However, the learning task distribution D, with its set of queries and gold\n\n. However, the learning task distribution\n\n, with its set of queries and gold\n\n\u22c6\n\n(\n\n)\n\nr\n\ny, y\n\nD\n\nanswers are still labeled by human experts. The trainable policy \u03c0\u03b8 is optimized to maximize expected reward:\n\nanswers are still labeled by\n\nhuman experts. The trainable policy\n\nis optimized to maximize expected reward:\n\n\u03c0\n\n\u03b8\n\nJRLVR(\u03b8) = E(x,y\u22c6)\u223cD, y\u223c\u03c0\u03b8(\u00b7 |x)\n-\n\nr(y, y\n\u22c6\n)\n\n. (2)\n\n\u22c6\n\n\u22c6\n\nJRLVR(\u03b8) = E(x,y\u22c6)\u223cD, y\u223c\u03c0\u03b8(\u00b7 |x)\n\nE\n\n. (2)\n\n(2)\n\n-\n\n-\n\n(\n\n) =\n\nr(y, y\n\n(\n\n)\n\n)\n\nJ\n\n\u03b8\n\nr\n\ny, y\n\n.\n\nRLVR\n\n\u22c6\n\n(\n\n)\n\n(\n\n)\n\nx,y\n\n, y\n\n\u03c0\n\nx\n\n\u223cD\n\n\u223c\n\n\u00b7 |\n\n\u03b8\n\nIn summary, both SFT and RLVR still rely on human-curated datasets of either queries, demonstrations, or verifiers, which ultimately\nlimit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own\ninteractions with the environment entirely through self-play.\n\nIn summary, both SFT and RLVR still rely on human-curated datasets of either queries, demonstrations, or verifiers, which ultimately\n\nIn summary, both SFT and RLVR still rely on\n\nhuman-curated\n\ndatasets of either queries, demonstrations, or verifiers, which ultimately\n\nlimit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own\n\nlimit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own\n\ninteractions with the environment entirely through self-play.\n\ninteractions with the environment entirely through self-play.\n\n2.2. Absolute Zero\n\n2.2. Absolute Zero\n\n2.2. Absolute Zero\n\nWe propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from\nboth stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We\nillustrate this paradigm in Figure 2, which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach\neliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.\n\nWe propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from\n\nWe propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from\n\nboth stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We\n\nboth stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We\n\nillustrate this paradigm in Figure 2, which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach\n\nillustrate this paradigm in Figure\n\nwhich contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach\n\n2,\n\neliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.\n\neliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.\n\nTo make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid\nunderstanding, we include an illustration in Figure 3. Let \u03c0\u03b8 be our parameterized language model, it is used to play two roles, proposer\n\u03c0\npropose\n\u03b8\nand solver \u03c0\nsolve\n\u03b8 during training.\n\nTo make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid\n\nTo make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid\n\nunderstanding, we include an illustration in Figure 3. Let \u03c0\u03b8 be our parameterized language model, it is used to play two roles, proposer\n\nunderstanding, we include an illustration in Figure\n\nLet\n\nbe our parameterized language model, it is used to play two roles, proposer\n\n3.\n\n\u03c0\n\n\u03b8\n\npropose\n\npropose\n\nsolve\n\nsolve\n\nand solver \u03c0\n\nand solver\n\n\u03b8 during training.\n\nduring training.\n\n\u03c0\n\n\u03c0\n\n\u03c0\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\n3\n\n3\n\n3",
        "tables": []
      },
      {
        "page_num": 3,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nLanguage Model\n!\"#!$%&\n'#()*\n\n, \u22c6\n, \"#$\"$%&\n\nEnvironment\n, \n%$'(&\nEnvironment\n\nFigure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent \u03c0\nproposing task \u03c4 , which is transformed by f with the environment e into a validated\nproblem (x, y\u22c6\n), and also emits a reward r\npropose for learnability. Then, a standard RL step\nfollows: the agent solves x by producing y, receiving reward r\nsolve from e by matching\nwith y\n\u22c6\n. \u03c0\npropose and \u03c0\nsolve are jointly trained and this process can be repeated indefinitely.\n\nLanguage Model\n!\"#!$%&\n'#()*\n\n, \u22c6\n, \"#$\"$%&\n\nEnvironment\n, \n%$'(&\nEnvironment\n\nThe proposer first samples a proposed\ntask conditioned on variable z: \u03c4 \u223c\n\u03c0\npropose\n\u03b8\n(\u00b7|z), which will then be validated\nand used to construct a valid reasoning task\ntogether with the environment e: (x, y\u22c6\n) \u223c\nfe(\u00b7|\u03c4 ), where x is the task query and y\n\u22c6\nis the gold label. Then the solver produces\nan answer y \u223c \u03c0\nsolve\n\u03b8\n(\u00b7 | x). Each pro-\nposed task \u03c4 is scored by a learnability\nreward r\npropose\ne (\u03c4, \u03c0\u03b8), which captures the\nexpected improvement in \u03c0\u03b8 after train-\ning on the task query x. Moreover, the\nsame policy also receives a solution re-\nward r\nsolve\ne (y, y\u22c6\n) for its answer to the task\nquery x, with the environment again serv-\n\nThe proposer first samples a proposed\n\nThe proposer first samples a proposed\n\ntask conditioned on variable z: \u03c4 \u223c\n\ntask conditioned on variable\n\n:\n\nz\n\n\u03c4\n\n\u223c\n\npropose\n\npropose\n\n(\u00b7|z), which will then be validated\n\n, which will then be validated\n\n(\n\n)\n\n\u03c0\n\n\u03c0\n\nz\n\n\u00b7|\n\n\u03b8\n\n\u03b8\n\n'#()*\n\n#()*\n\n'\n\nand used to construct a valid reasoning task\n\nand used to construct a valid reasoning task\n\nEnvironment\n\nEnvironment\n\nEnvironment\n\nEnvironment\n\nLanguage Model\n\nLanguage Model\n\ntogether with the environment e: (x, y\u22c6\n\ntogether with the environment\n\n:\n\n\u22c6\n\n,\n\n,\n\n(\n\n) \u223c\n\n)\n\ne\n\nx, y\n\n!\"#!$%&\n\n!\"#!$%&\n\n\u223c\n\nfe(\u00b7|\u03c4 ), where x is the task query and y\n\n, where\n\nis the task query and\n\n\u22c6\n\n\u22c6\n\n(\n\n)\n\nx\n\ny\n\nf\n\n\u03c4\n\n\u00b7|\n\ne\n\nis the gold label. Then the solver produces\n\nis the gold label. Then the solver produces\n\n, \u22c6\n\n\u22c6\n\n, \"#$\"$%&\n\n\"#$\"$%&\n\n%$'(&\n\n%$'(&\n\n,\n\n,\n\nsolve\n\nsolve\n\nan answer y \u223c \u03c0\n\nan answer\n\n(\u00b7 | x). Each pro-\n\n. Each pro-\n\n(\n\n)\n\ny\n\n\u03c0\n\nx\n\n\u223c\n\n\u00b7 |\n\n\u03b8\n\n\u03b8\n\nposed task \u03c4 is scored by a learnability\n\nposed task\n\nis scored by a\n\nlearnability\n\n\u03c4\n\npropose\n\npropose\n\ne (\u03c4, \u03c0\u03b8), which captures the\n\n, which captures the\n\nFigure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent \u03c0\nproposing task \u03c4 , which is transformed by f with the environment e into a validated\nproblem (x, y\u22c6\n), and also emits a reward r\npropose for learnability. Then, a standard RL step\nfollows: the agent solves x by producing y, receiving reward r\nsolve from e by matching\nwith y\n\u22c6\n. \u03c0\npropose and \u03c0\nsolve are jointly trained and this process can be repeated indefinitely.\n\nFigure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent \u03c0\n\nThe Absolute Zero loop begins with the agent\n\n(\n\n)\n\nreward r\n\nreward\n\nr\n\n\u03c4, \u03c0\n\nThe Absolute Zero Loop.\n\nFigure 3.\n\n\u03c0\n\ne\n\n\u03b8\n\nexpected improvement in \u03c0\u03b8 after train-\n\nexpected improvement in\n\nafter train-\n\nproposing task \u03c4 , which is transformed by f with the environment e into a validated\n\nproposing task\n\n, which is transformed by\n\nwith the environment\n\ninto a validated\n\n\u03c0\n\n\u03c4\n\nf\n\ne\n\n\u03b8\n\ning on the task query x. Moreover, the\n\ning on the task query\n\n. Moreover, the\n\npropose for learnability. Then, a standard RL step\n\npropose\n\n), and also emits a reward r\n\n, and also emits a reward\n\nfor learnability. Then, a standard RL step\n\nproblem (x, y\u22c6\n\nproblem\n\n\u22c6\n\nx\n\n(\n\n)\n\nx, y\n\nr\n\nsame policy also receives a solution re-\n\nsame policy also receives a\n\nsolve from e by matching\n\nsolve\n\nfollows: the agent solves x by producing y, receiving reward r\n\nfollows: the agent solves\n\nby producing\n\n, receiving reward\n\nfrom\n\nby matching\n\nsolution re-\n\nx\n\ny\n\nr\n\ne\n\nsolve\n\nsolve\n\n) for its answer to the task\n\nfor its answer to the task\n\npropose and \u03c0\n\npropose\n\nsolve are jointly trained and this process can be repeated indefinitely.\n\nsolve\n\ne (y, y\u22c6\n\n\u22c6\n\nare jointly trained and this process can be repeated indefinitely.\n\nwith y\n\nwith\n\n. \u03c0\n\n.\n\nand\n\n\u22c6\n\n\u22c6\n\n(\n\n)\n\nward r\n\nward\n\nr\n\ny, y\n\ny\n\n\u03c0\n\n\u03c0\n\ne\n\nquery x, with the environment again serv-\n\nquery\n\n, with the environment again serv-\n\nx\n\ning as the verifier. A nonnegative coefficient \u03bb balances the trade-off between exploring new, learnable tasks and improving the model's\nreasoning and problem-solving abilities. We formally define the absolute zero setting's objective as follows:\n\ning as the verifier. A nonnegative coefficient \u03bb balances the trade-off between exploring new, learnable tasks and improving the model's\n\ning as the verifier. A nonnegative coefficient\n\nbalances the trade-off between exploring new, learnable tasks and improving the model's\n\n\u03bb\n\nreasoning and problem-solving abilities. We formally define the absolute zero setting's objective as follows:\n\nreasoning and problem-solving abilities. We formally define the absolute zero setting's objective as follows:\n\nJ (\u03b8) := max\n\u03b8\nEz\u223cp(z)\n\"\nE(x,y\u22c6)\u223cfe(\u00b7|\u03c4),\u03c4\u223c\u03c0\npropose\n\u03b8\n(\u00b7|z)\n\nr\npropose\ne (\u03c4, \u03c0\u03b8) + \u03bb Ey\u223c\u03c0solve\n\u03b8\n(\u00b7|x)\n-\n\nr\nsolve\ne (y, y\n\u22c6\n)\n\n\n#\n. (3)\n\n\"\n\n\"\n\n#\n\n#\n\npropose\n\npropose\n\nsolve\n\nsolve\n\n\u22c6\n\n\u22c6\n\nEz\u223cp(z)\n\nE\n\nE(x,y\u22c6)\u223cfe(\u00b7|\u03c4),\u03c4\u223c\u03c0\n\nE\n\ne (\u03c4, \u03c0\u03b8) + \u03bb Ey\u223c\u03c0solve\n\nE\n\n. (3)\n\n(3)\n\n-\n\n-\n\nJ (\u03b8) := max\n\n:= max\n\n(\n\n)\n\n(\n\n) +\n\ne (y, y\n\n(\n\n)\n\n)\n\n\u03b8\n\nr\n\nr\n\n\u03c4, \u03c0\n\n\u03bb\n\nr\n\nr\n\ny, y\n\n.\n\npropose\n\npropose\n\nsolve\n\nJ\n\n\u03b8\n\n(\n\n)\n\n\u22c6\n\ne\n\ne\n\nz\n\np\n\nz\n\n(\n\n)\n\n(\n\n)\n\n(\u00b7|z)\n\n(\n\n)\n\n(\u00b7|x)\n\n(\n\n)\n\nx,y\n\nf\n\n\u03c4\n\n,\u03c4\n\n\u03c0\n\nz\n\ny\n\n\u03c0\n\nx\n\n\u223c\n\n\u223c\n\n\u00b7|\n\n\u223c\n\n\u00b7|\n\n\u223c\n\n\u00b7|\n\ne\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\n\u03b8\n\nNotice that we shift the burden of scaling data away from human experts and onto the proposer policy \u03c0\npropose\n\u03b8\nand the environment\ne. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing\ngrounded feedback that supports stable and self-sustainable training. When proposing, z acts as a conditional variable that seeds\ngeneration of tasks. Practically, z can be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated\ntask memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward\nr\npropose(\u03c4, \u03c0\u03b8), which measures how much the model is expected to improve by solving a proposed task \u03c4 . Moreover, the solver reward\nr\nsolve(y, y\u2217\n) evaluates the correctness of the model's output. Together, these two signals guide the model to propose tasks that are both\nchallenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.\n\npropose\n\npropose\n\nNotice that we shift the burden of scaling data away from human experts and onto the proposer policy \u03c0\n\nNotice that we shift the burden of scaling data away from\n\nhuman experts\n\nand onto the\n\nproposer policy\n\nand the environment\n\nand the\n\nenvironment\n\n\u03c0\n\n\u03b8\n\n\u03b8\n\ne. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing\n\n. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing\n\ne\n\ngrounded feedback that supports stable and self-sustainable training. When proposing, z acts as a conditional variable that seeds\n\ngrounded feedback that supports stable and self-sustainable training. When proposing,\n\nacts as a conditional variable that seeds\n\nz\n\ngeneration of tasks. Practically, z can be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated\n\ngeneration of tasks. Practically,\n\ncan be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated\n\nz\n\ntask memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward\n\ntask memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward\n\npropose(\u03c4, \u03c0\u03b8), which measures how much the model is expected to improve by solving a proposed task \u03c4 . Moreover, the solver reward\n\npropose\n\n, which measures how much the model is expected to improve by solving a proposed task\n\n. Moreover, the solver reward\n\n(\n\n)\n\nr\n\nr\n\n\u03c4, \u03c0\n\n\u03c4\n\n\u03b8\n\nsolve(y, y\u2217\n\nsolve\n\n) evaluates the correctness of the model's output. Together, these two signals guide the model to propose tasks that are both\n\nevaluates the correctness of the model's output. Together, these two signals guide the model to propose tasks that are both\n\n(\n\n)\n\nr\n\nr\n\ny, y\n\n\u2217\n\nchallenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.\n\nchallenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.\n\n3. Absolute Zero Reasoner\n\n3. Absolute Zero Reasoner\n\n3. Absolute Zero Reasoner\n\nIn this section, we present Absolute Zero Reasoner (AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an\nunified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them\nto improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of\nreasoning capacity while enhancing its ability to solve them effectively (Section 3.1). Within this self-play training paradigm, the model\nlearns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and\ninduction (Section 3.2). Using coding tasks is motivated by the Turing-completeness of programming languages (Stuart, 2015) and\nempirical evidence that code-based training improves reasoning (Aryabumi et al., 2024). We adopt code as an open-ended, expressive,\nand verifiable medium for enabling reliable task construction and verification (Section 3.3). Finally, the model is updated using a newly\nproposed advantage estimator designed for multitask learning (Section 3.3.5). We outline the overall algorithm in Algorithm 1 and\nhighlight an illustration of our Absolute Zero Reasoner approach in Figure 4. To expedite future exploration in this area, we also present\nseveral attempts that did not yield fruitful results but still warrant discussion in Appendix D.\n\nIn this section, we present Absolute Zero Reasoner (AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an\n\nIn this section, we present\n\n(AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an\n\nAbsolute Zero Reasoner\n\nunified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them\n\nunified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them\n\nto improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of\n\nto improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of\n\nreasoning capacity while enhancing its ability to solve them effectively (Section 3.1). Within this self-play training paradigm, the model\n\nreasoning capacity while enhancing its ability to solve them effectively (Section\n\n. Within this self-play training paradigm, the model\n\n3.1)\n\nlearns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and\n\nlearns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and\n\ninduction (Section 3.2). Using coding tasks is motivated by the Turing-completeness of programming languages (Stuart, 2015) and\n\ninduction (Section\n\n3.2)\n\n. Using coding tasks is motivated by the Turing-completeness of programming languages\n\n(Stuart,\n\n2015)\n\nand\n\nempirical evidence that code-based training improves reasoning (Aryabumi et al., 2024). We adopt code as an open-ended, expressive,\n\nempirical evidence that code-based training improves reasoning\n\n(Aryabumi et al.,\n\n. We adopt code as an open-ended, expressive,\n\n2024)\n\nand verifiable medium for enabling reliable task construction and verification (Section 3.3). Finally, the model is updated using a newly\n\nand verifiable medium for enabling reliable task construction and verification (Section\n\n. Finally, the model is updated using a newly\n\n3.3)\n\nproposed advantage estimator designed for multitask learning (Section 3.3.5). We outline the overall algorithm in Algorithm 1 and\n\nproposed advantage estimator designed for multitask learning (Section\n\n3.3.5)\n\n. We outline the overall algorithm in Algorithm\n\n1\n\nand\n\nhighlight an illustration of our Absolute Zero Reasoner approach in Figure 4. To expedite future exploration in this area, we also present\n\nhighlight an illustration of our Absolute Zero Reasoner approach in Figure\n\nTo expedite future exploration in this area, we also present\n\n4.\n\nseveral attempts that did not yield fruitful results but still warrant discussion in Appendix D.\n\nseveral attempts that did not yield fruitful results but still warrant discussion in Appendix\n\nD.\n\n3.1. Two Roles in One: Proposer and Solver\n\n3.1. Two Roles in One: Proposer and Solver\n\n3.1. Two Roles in One: Proposer and Solver\n\nLarge language models are naturally suited for implementing AZR in a multitask learning context (Radford et al., 2019), as both\nthe formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a\nsingle model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective\nin Equation (3). At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined\nin Section 3.2) and K past self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples,\npromoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning\ntasks that can be verified using the environment, outlined later in Section 3.3. AZR then attempts to solve these newly proposed tasks,\nreceiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning.\nWe now outline the rewards used for each role.\n\nLarge language models are naturally suited for implementing AZR in a multitask learning context (Radford et al., 2019), as both\n\nLarge language models are naturally suited for implementing AZR in a multitask learning context\n\n(Radford et al.,\n\n2019)\n\n, as both\n\nthe formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a\n\nthe formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a\n\nsingle model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective\n\nsingle model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective\n\nin Equation (3). At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined\n\nin Equation\n\n. At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined\n\n(3)\n\nin Section 3.2) and K past self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples,\n\nin Section\n\nand\n\npast self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples,\n\n3.2)\n\nK\n\npromoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning\n\npromoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning\n\ntasks that can be verified using the environment, outlined later in Section 3.3. AZR then attempts to solve these newly proposed tasks,\n\ntasks that can be verified using the environment, outlined later in Section\n\n3.3.\n\nAZR then attempts to solve these newly proposed tasks,\n\nreceiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning.\n\nreceiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning.\n\nWe now outline the rewards used for each role.\n\nWe now outline the rewards used for each role.\n\nReward Design. Prior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning\nsystems (Zeng et al., 2025b). Motivated by this, we design a reward function for the proposer that encourages generation of tasks\n\nReward Design. Prior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning\n\nPrior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning\n\nReward Design.\n\nsystems (Zeng et al., 2025b). Motivated by this, we design a reward function for the proposer that encourages generation of tasks\n\nsystems\n\n(Zeng et al.,\n\n2025b)\n\n. Motivated by this, we design a reward function for the proposer that encourages generation of tasks\n\n4\n\n4\n\n4",
        "tables": []
      },
      {
        "page_num": 4,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nmodel input/output model reward ( , , ) P rogram I nput O utput\n Learnability\n\nReward\nAccuracy\n\nReward\nAbsolute\n\nZero\n\nReasoner\nVerify\nConstruct & Estimate \nPROPOSE\n\nSelf-play\n\nSOLVE\n\nJoint Update\nTask Types\nInduction:\nAbduction:\nDeduction:\nOX = F ( ) P ?\nOX = ( ) ? I\n? = F ( ) P I\n\nPROPOSE\n\nPROPOSE\n\nLearnability\n\nLearnability\n\nConstruct & Estimate\n\nConstruct & Estimate\n\nReward\n\nReward\n\nTask Types\n\nTask Types\n\nAbsolute\n\nAbsolute\n\nSelf-play\n\nSelf-play\n\nAbduction:\n\nAbduction:\n\nOX = F ( ) P ?\n\nOX = F ( )\n\nP\n\n?\n\nJoint Update\n\nJoint Update\n\nZero\n\nZero\n\nDeduction:\n\nDeduction:\n\n? = F ( ) P I\n\nP\n\nI\n\n? = F ( )\n\nReasoner\n\nReasoner\n\nInduction:\n\nInduction:\n\nOX = ( ) ? I\n\nOX = ( )\n\n?\n\nI\n\nAccuracy\n\nAccuracy\n\nSOLVE\n\nSOLVE\n\nVerify\n\nVerify\n\nReward\n\nReward\n\nmodel input/output model reward ( , , ) P rogram I nput O utput\n\nmodel input/output\n\nmodel reward ( , , )\n\nP\n\nrogram\n\nI\n\nnput\n\nO\n\nutput\n\nFigure 4. Absolute Zero Reasoner Training Overview. At every iteration, Absolute Zero Reasoner first PROPOSES a batch of tasks,\nconditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section 3.2).\nFrom these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward rpropose is\nalso calculated for each proposed task as defined in Equation (4). The Absolute Zero Reasoner then SOLVES the batch of reasoning\nquestions. Python is used again to verify the generated responses and compute the accuracy reward rsolve as described in Equation (5).\nFinally, the Absolute Zero Reasoner is jointly updated using both rpropose and rsolve across all three task types, using TRR++ (Section 3.3.5).\n\nFigure 4. Absolute Zero Reasoner Training Overview. At every iteration, Absolute Zero Reasoner first PROPOSES a batch of tasks,\n\nAt every iteration, Absolute Zero Reasoner first\n\na batch of tasks,\n\nAbsolute Zero Reasoner Training Overview.\n\nPROPOSES\n\nFigure 4.\n\nconditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section 3.2).\n\nconditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section\n\n3.2)\n\n.\n\nFrom these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward rpropose is\n\nFrom these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward\n\nis\n\nr\n\npropose\n\nalso calculated for each proposed task as defined in Equation (4). The Absolute Zero Reasoner then SOLVES the batch of reasoning\n\nalso calculated for each proposed task as defined in Equation\n\n(4)\n\n. The Absolute Zero Reasoner then\n\nthe batch of reasoning\n\nSOLVES\n\nquestions. Python is used again to verify the generated responses and compute the accuracy reward rsolve as described in Equation (5).\n\nquestions. Python is used again to verify the generated responses and compute the accuracy reward\n\nas described in Equation\n\n(5)\n\n.\n\nr\n\nsolve\n\nFinally, the Absolute Zero Reasoner is jointly updated using both rpropose and rsolve across all three task types, using TRR++ (Section 3.3.5).\n\nFinally, the Absolute Zero Reasoner is jointly updated using both\n\nacross all three task types, using TRR++ (Section\n\nand\n\n3.3.5)\n\n.\n\nr\n\nr\n\npropose\n\nsolve\n\nwith meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language\nmodel in its solver role to estimate the learnability of a proposed task, a similar type of reward used in unsupervised environment\ndesign literature (Sukhbaatar et al., 2018). We perform n Monte Carlo rollouts of the solver and compute the average success rate:\nr\u00afsolve =\n1\nn\nPN\ni=1 r\n(i)\nsolve. The proposer's reward is then defined as:\n\nwith meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language\n\nwith meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language\n\nmodel in its solver role to estimate the learnability of a proposed task, a similar type of reward used in unsupervised environment\n\nmodel in its solver role to estimate the\n\nof a proposed task, a similar type of reward used in unsupervised environment\n\nlearnability\n\ndesign literature (Sukhbaatar et al., 2018). We perform n Monte Carlo rollouts of the solver and compute the average success rate:\n\ndesign literature\n\n(Sukhbaatar et al.,\n\n2018)\n\n. We perform\n\nMonte Carlo rollouts of the solver and compute the average success rate:\n\nn\n\n(i)\n\n(\n\n)\n\ni\n\nPN\n\nN\n\nsolve. The proposer's reward is then defined as:\n\n. The proposer's reward is then defined as:\n\n1\n\n1\n\nP\n\nr\u00afsolve =\n\n\u00af\n\n=\n\ni=1 r\n\nr\n\nr\n\nsolve\n\nsolve\n\n=1\n\nn\n\nn\n\ni\n\nrpropose =\n\n0, if r\u00afsolve = 0 or r\u00afsolve = 1\n1 \u2212 r\u00afsolve, otherwise,\n(4)\n\n0, if r\u00afsolve = 0 or r\u00afsolve = 1\n\nif\n\nor\n\n0\n\n\u00af\n\n= 0\n\n\u00af\n\n= 1\n\n,\n\nr\n\nr\n\nsolve\n\nsolve\n\n(4)\n\n(4)\n\nrpropose =\n\n=\n\nr\n\npropose\n\n1 \u2212 r\u00afsolve, otherwise,\n\notherwise\n\n1\n\n\u00af\n\nr\n\n,\n\n,\n\nsolve\n\n\u2212\n\nThe intuition is that if a task is either trivial to solve (r\u00afsolve = 1) or unsolvable (r\u00afsolve = 0), the task provides little to no learning signal\nfor the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the\nrichest feedback and greatest potential for learning.\n\nThe intuition is that if a task is either trivial to solve (r\u00afsolve = 1) or unsolvable (r\u00afsolve = 0), the task provides little to no learning signal\n\nThe intuition is that if a task is either trivial to solve (\n\n) or unsolvable (\n\n), the task provides little to no learning signal\n\n\u00af\n\n= 1\n\n\u00af\n\n= 0\n\nr\n\nr\n\nsolve\n\nsolve\n\nfor the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the\n\nfor the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the\n\nrichest feedback and greatest potential for learning.\n\nrichest feedback and greatest potential for learning.\n\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\n\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\n\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\n\nrsolve = I(y=y\u22c6)\n, (5)\n\nrsolve = I(y=y\u22c6)\n\nI\n\n, (5)\n\n(5)\n\n=\n\nr\n\n,\n\nsolve\n\n\u22c6\n\n(\n\n=\n\n)\n\ny\n\ny\n\nwhere y\n\u22c6\nis the ground-truth answer, and equality is evaluated based on value equality in Python.\n\nwhere y\n\nwhere\n\nis the ground-truth answer, and equality is evaluated based on value equality in Python.\n\nis the ground-truth answer, and equality is evaluated based on value equality in Python.\n\n\u22c6\n\n\u22c6\n\ny\n\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which\nintegrates rpropose and rsolve with a format-aware penalty inspired by DeepSeek-AI et al. (2025):\n\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which\n\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which\n\nintegrates rpropose and rsolve with a format-aware penalty inspired by DeepSeek-AI et al. (2025):\n\nintegrates\n\nand\n\nwith a format-aware penalty inspired by\n\nDeepSeek-AI et al.\n\n(2025)\n\n:\n\nr\n\nr\n\npropose\n\nsolve\n\nR(y\u03c0) =\n\n\n\nrrole if the response is passable, role \u2208 {propose,solve}\n\u22120.5 if the response is wrong but well-formatted,\n\u22121 if the answer has formatting errors,\n(6)\n\nrrole if the response is passable, role \u2208 {propose,solve}\n\nif the response is passable, role\n\npropose,solve\n\nr\n\nrole\n\n\u2208 {\n\n}\n\n(6)\n\n(6)\n\n\u22120.5 if the response is wrong but well-formatted,\n\nif the response is wrong but well-formatted,\n\nR(y\u03c0) =\n\n(\n\n) =\n\n0\n\n5\n\nR\n\ny\n\n.\n\n\u03c0\n\n\u2212\n\n\u22121 if the answer has formatting errors,\n\nif the answer has formatting errors,\n\n1\n\n\u2212\n\nwhere y\u03c0 is the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek\nR1 <think> and <answer> format, as shown in Figure 33. Moreover, for the proposer, the reward criterion for format goes beyond\nsimply following the XML structure. As detailed in Section 3.3.3, only responses that produce valid triplets and pass the filtering stage\nare considered to be correctly formatted.\n\nwhere y\u03c0 is the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek\n\nis the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek\n\nwhere\n\ny\n\n\u03c0\n\nR1 <think> and <answer> format, as shown in Figure 33. Moreover, for the proposer, the reward criterion for format goes beyond\n\nR1\n\nand\n\nformat, as shown in Figure\n\n33.\n\nMoreover, for the proposer, the reward criterion for format goes beyond\n\n<think>\n\n<answer>\n\nsimply following the XML structure. As detailed in Section 3.3.3, only responses that produce valid triplets and pass the filtering stage\n\nsimply following the XML structure. As detailed in Section\n\nonly responses that produce valid triplets and pass the filtering stage\n\n3.3.3,\n\nare considered to be correctly formatted.\n\nare considered to be correctly formatted.\n\n5\n\n5\n\n5",
        "tables": []
      },
      {
        "page_num": 5,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\n\n3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\n\nAZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution,\nand validation of code reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O\nof a coding language, we define an AZR reasoning task as a triplet (p, i, o), where p \u2208 P is a program, i \u2208 I is an input, and o \u2208 O is\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about different parts of this task\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n3.2. Learning Diferent Modes of Reasoning: Deduction, Induction, and Abduction\nAZR uses code executor as both a fexible interface and a verifable environment. This setup enables automatic construction, execution,\nand validation of reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O of\na coding language, we defne an AZR reasoning task as a triplet (p, i, o), where p \u2192 P is a program, i \u2192 I is an input, and o \u2192 O is\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about diferent parts of this task\n\nAZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution,\n\nAZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution,\n\n3.2. Learning Diferent Modes of Reasoning: Deduction, Induction, and Abduction\n\n3.2. Learning Diferent Modes of Reasoning: Deduction, Induction, and Abduction\n\nand validation of code reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O\n\nand validation of code reasoning tasks\n\nAryabumi et al.,\n\n. Give program space\n\n, input space\n\nand output space\n\n(Stuart,\n\n2015;\n\n2024)\n\nP\n\nI\n\nO\n\nAZR uses code executor as both a fexible interface and a verifable environment. This setup enables automatic construction, execution,\n\nAZR uses code executor as both a fexible interface and a verifable environment. This setup enables automatic construction, execution,\n\nof a coding language, we define an AZR reasoning task as a triplet (p, i, o), where p \u2208 P is a program, i \u2208 I is an input, and o \u2208 O is\n\nof a coding language, we define an AZR reasoning task as a triplet\n\n, where\n\nis a program,\n\nis an input, and\n\nis\n\n(\n\n)\n\np, i, o\n\np\n\ni\n\no\n\nP\n\nI\n\nO\n\n\u2208\n\n\u2208\n\n\u2208\n\nand validation of reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O of\n\nand validation of reasoning tasks (Stuart,\n\n2015;\n\nAryabumi et al.,\n\n2024). Give program space\n\n, input space\n\nand output space\n\nof\n\nP\n\nI\n\nO\n\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about different parts of this task\n\nthe corresponding output produced by running program on input,\n\n. AZR learns by reasoning about different parts of this task\n\n=\n\n(\n\n)\n\no\n\np\n\ni\n\na coding language, we defne an AZR reasoning task as a triplet (p, i, o), where p \u2192 P is a program, i \u2192 I is an input, and o \u2192 O is\n\n(\n\n)\n\na coding language, we defne an AZR reasoning task as a triplet\n\n, where\n\nis a program,\n\nis an input, and\n\nis\n\np, i, o\n\np\n\ni\n\no\n\nP\n\nI\n\nO\n\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\n\u2192\n\n\u2192\n\n\u2192\n\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about diferent parts of this task\n\n=\n\n(\n\n)\n\nthe corresponding output produced by running program on input,\n\n. AZR learns by reasoning about diferent parts of this task\n\no\n\np\n\ni\n\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction buffer Ddeduction\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction bufer Ddeduction\n(all task bufers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verified using type-aware value equality\nin python to account for possible variations (such as set ordering or fractions).\ncompleting the triplet (p, i, o), which is added to the bufer if non-error output was produced.\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verifed using type-aware value equality\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\nin python to account for possible variations (such as set ordering or fractions).\n\u2022 As a proposer, the policy \u03c0\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\nexecutes p(i) and get the triplet (p, i, o).\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\u2022 As a proposer, the policy \u03c0propose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verified by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\nexecutes p(i) and get the triplet (p, i, o).\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verifed by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn\n, on\n)}, requiring generalization from partial information.\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn, on)}, requiring generalization from partial information.\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222a Ddeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn\n, on\n)}, m), which is stored\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222aDdeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn, on)}, m), which is stored\nin the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\u2022 As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\nlogic and promotes generalized induction.\n\u2022 As a solver, the model is shown the frst half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\nlogic and promotes generalized induction.\n\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\n1.\n\n: predicting the output\n\ngiven a program\n\nand input\n\n, capturing step-by-step logical reasoning.\n\nDeduction\n\no\n\np\n\ni\n\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction buffer Ddeduction\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction bufer Ddeduction\n(all task bufers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\n\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction buffer Ddeduction\n\nAs a\n\n, AZR is conditioned on the task type\n\ndeduction\n\nreference examples from the deduction buffer\n\nand\n\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n\n1.\n\n: predicting the output\n\ngiven a program\n\nand input\n\n, capturing step-by-step logical reasoning.\n\nDeduction\n\no\n\np\n\ni\n\n=\n\n\u2022\n\nproposer\n\n\u03b1\n\nK\n\ndeduction\n\nD\n\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\n\n(all task buffers are outlined in Section\n\n3.3), and generates a pair\n\n. The environment\n\nthen executes\n\nto compute\n\n,\n\n(\n\n)\n\n(\n\n)\n\np, i\n\ne\n\np\n\ni\n\no\n\n\u2022 As a proposer, AZR is conditioned on the task type \u03b1 = deduction and K reference examples from the deduction bufer Ddeduction\n\n=\n\n\u2022\n\nAs a\n\n, AZR is conditioned on\n\nthe\n\ntask type\n\ndeduction and\n\nreference examples from the deduction bufer\n\nproposer\n\n\u03b1\n\nK\n\nD\n\ndeduction\n\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\n\ncompleting the triplet\n\n, which is added to the buffer if non-error output was produced.\n\n(\n\n)\n\np, i, o\n\n(all task bufers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\n\n(\n\n)\n\n(\n\n)\n\n(all task bufers are outlined in Section\n\n3.3), and generates a pair\n\n. The environment\n\nthen executes\n\nto compute\n\n,\n\np, i\n\ne\n\np\n\ni\n\no\n\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verified using type-aware value equality\nin python to account for possible variations (such as set ordering or fractions).\ncompleting the triplet (p, i, o), which is added to the bufer if non-error output was produced.\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verifed using type-aware value equality\n\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verified using type-aware value equality\n\nAs a\n\n, the model receives\n\nand predicts the output\n\n. The predicted output is verified using type-aware value equality\n\ncompleting the triplet (p, i, o), which is added to the bufer if non-error output was produced.\n\n(\n\n)\n\ncompleting the triplet\n\n, which is added to the bufer if non-error output was produced.\n\np, i, o\n\n(\n\n)\n\n\u2022\n\nsolver\n\np, i\n\no\n\n\u03c0\n\nin python to account for possible variations (such as set ordering or fractions).\n\nin python to account for possible variations (such as set ordering or fractions).\n\n\u2022 As a solver, the model receives (p, i) and predicts the output o\u03c0. The predicted output is verifed using type-aware value equality\n\n(\n\n)\n\n\u2022\n\nAs a\n\n, the model receives\n\nand predicts the output\n\n. The predicted output is verifed using type-aware value equality\n\nsolver\n\np, i\n\no\n\n\u03c0\n\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\nin python to account for possible variations (such as set ordering or fractions).\n\nin python to account for possible variations (such as set ordering or fractions).\n\nin python to account for possible variations (such as set ordering or fractions).\n\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\n2.\n\n: inferring a plausible input\n\ngiven the program\n\nand an output\n\n, resembling trial-and-error or online search.\n\nAbduction\n\ni\n\np\n\no\n\n\u2022 As a proposer, the policy \u03c0\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\nexecutes p(i) and get the triplet (p, i, o).\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\u2022 As a proposer, the policy \u03c0propose's input and output is almost the same as the proposer for the deduction task, except that the task\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\n\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n\n2.\n\n: inferring a plausible input\n\ngiven the program\n\nand an output\n\n, resembling trial-and-error or online search.\n\nAbduction\n\ni\n\np\n\no\n\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\n\npropose\n\n\u2022 As a proposer, the policy \u03c0\n\nAs a\n\n, the policy\n\n's input and output is almost the same as the proposer for the deduction task, except that the task\n\n\u2022\n\nproposer\n\n\u03c0\n\n\u2022 As a proposer, the policy \u03c0propose's input and output is almost the same as the proposer for the deduction task, except that the task\n\npropose's input and output is almost the same as the proposer for the deduction task, except that the task\n\n\u2022\n\nAs a\n\n, the policy\n\nproposer\n\n\u03c0\n\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\n\nabduction\n\nis changed as an input. The model generates a pair\n\nconditioned on\n\nand reference examples. Then we\n\ntype\n\n=\n\n(\n\n)\n\n\u03b1\n\np, i\n\n\u03b1\n\ntype \u03b1 = abduction is changed as an input. The model generates a pair (p, i) conditioned on \u03b1 and reference examples. Then we\n\n=\n\n(\n\n)\n\ntype\n\nabduction is changed as an input. The model generates a pair\n\nconditioned on\n\nand reference examples. Then we\n\n\u03b1\n\np, i\n\n\u03b1\n\nexecutes p(i) and get the triplet (p, i, o).\n\nexecutes\n\nand get the triplet\n\n.\n\n(\n\n)\n\n(\n\n)\n\np\n\ni\n\np, i, o\n\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verified by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\nexecutes p(i) and get the triplet (p, i, o).\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verifed by checking whether p(i\u03c0) = o. Since programs\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\n\nexecutes p(i) and get the triplet (p, i, o).\n\n(\n\n)\n\n(\n\n)\n\nexecutes\n\nand get the triplet\n\n.\n\np\n\ni\n\np, i, o\n\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verified by checking whether p(i\u03c0) = o. Since programs\n\nAs a\n\n, the model receives\n\nand predicts\n\n. The solution is verified by checking whether\n\n. Since programs\n\n(\n\n)\n\n(\n\n) =\n\n\u2022\n\nsolver\n\np, o\n\ni\n\np\n\ni\n\no\n\n\u03c0\n\n\u03c0\n\n\u2022 As a solver, the model receives (p, o) and predicts i\u03c0. The solution is verifed by checking whether p(i\u03c0) = o. Since programs\n\n(\n\n)\n\n(\n\n) =\n\n\u2022\n\nAs a\n\n, the model receives\n\nand predicts\n\n. The solution is verifed by checking whether\n\n. Since programs\n\nsolver\n\np, o\n\ni\n\np\n\ni\n\no\n\n\u03c0\n\n\u03c0\n\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\n\nmay not be bijective, we use\n\nvalue equivalence rather than requiring exact input matches.\n\noutput\n\nmay not be bijective, we use output value equivalence rather than requiring exact input matches.\n\nmay not be bijective, we use\n\nvalue equivalence rather than requiring exact input matches.\n\noutput\n\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn\n, on\n)}, requiring generalization from partial information.\n3. Induction: synthesizing a program p from a set of in-out examples {(i\nn, on)}, requiring generalization from partial information.\n\n3. Induction: synthesizing a program p from a set of in-out examples {(i\n\n3.\n\nsynthesizing a program\n\nfrom a set of in-out examples\n\n)}, requiring generalization from partial information.\n\n, requiring generalization from partial information.\n\nn\n\nn\n\n, on\n\nn\n\n(\n\n)\n\nInduction:\n\np\n\ni\n\n, o\n\nn, on)}, requiring generalization from partial information.\n\nn\n\nn\n\n{\n\n}\n\n3. Induction: synthesizing a program p from a set of in-out examples {(i\n\n(\n\n)\n\n3.\n\nsynthesizing a program\n\nfrom a set of in-out examples\n\n, requiring generalization from partial information.\n\nInduction:\n\np\n\ni\n\n, o\n\n{\n\n}\n\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222a Ddeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn\n, on\n)}, m), which is stored\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222aDdeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\nn, on)}, m), which is stored\nin the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222a Ddeduction, generates N new inputs and a message m, and uses the\n\nAs a\n\n, AZR samples a valid program\n\n, generates\n\nnew inputs and a message\n\n, and uses the\n\nfrom\n\n\u2022\n\nproposer\n\np\n\nN\n\nm\n\nabduction\n\ndeduction\n\n\u2022 As a proposer, AZR samples a valid program p from Dabduction \u222aDdeduction, generates N new inputs and a message m, and uses the\n\n\u2022\n\nAs a\n\n, AZR samples a valid program\n\nfrom\n\ndeduction, generates\n\nnew inputs and a message\n\n, and uses the\n\nproposer\n\np\n\nN\n\nm\n\nD\n\n\u222a D\n\nD\n\n\u222aD\n\nabduction\n\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\n\nenvironment to compute corresponding outputs. This forms an extended task representation\n\n)}, m), which is stored\n\n, which is stored\n\nn\n\nn\n\n, on\n\nn\n\n(\n\n(\n\n)\n\n)\n\np,\n\ni\n\n, o\n\n, m\n\nn, on)}, m), which is stored\n\nn\n\nn\n\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(i\n\n(\n\n(\n\n)\n\n)\n\nenvironment to compute corresponding outputs. This forms an extended task representation\n\n, which is stored\n\np,\n\ni\n\n, o\n\n, m\n\n{\n\n}\n\n{\n\n}\n\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\n\nin the induction buffer\n\n. Since infinitely many functions can map the inputs to the outputs, making the induction task\n\nin the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\n\nin the induction bufer\n\ninduction. Since infnitely many functions can map the inputs to the outputs, making the induction task\n\ninduction\n\nD\n\nD\n\nunder-constrained, the message m helps properly condition the problem for the solver.\n\nunder-constrained, the message\n\nhelps properly condition the problem for the solver.\n\nm\n\nunder-constrained, the message m helps properly condition the problem for the solver.\n\nunder-constrained, the message\n\nhelps properly condition the problem for the solver.\n\nm\n\n\u2022 As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\nlogic and promotes generalized induction.\n\u2022 As a solver, the model is shown the frst half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\nlogic and promotes generalized induction.\n\n\u2022 As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\n\nAs a\n\n, the model is shown the first half of the input-output pairs and the message\n\n, and must synthesize a program\n\nthat\n\n\u2022\n\n\u2022 As a solver, the model is shown the frst half of the input-output pairs and the message m, and must synthesize a program p\u03c0 that\n\n\u2022\n\nAs a\n\n, the model is shown the frst half of the input-output pairs and the message\n\n, and must synthesize a program\n\nthat\n\nsolver\n\nsolver\n\nm\n\np\n\nm\n\np\n\n\u03c0\n\n\u03c0\n\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\n\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\n\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\n\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else\n\nlogic and promotes generalized induction.\n\nlogic and promotes generalized induction.\n\nlogic and promotes generalized induction.\n\nlogic and promotes generalized induction.\n\nEach reasoning task type leverages code as an expressive and verifable\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025; Lam-\nbert et al., 2024). All prompts used by three diferent task types and two\ntypes of roles within a task type are shown in Figures 34 to 39. Next, we\noutline exact details of our algorithm.\nEach reasoning task type leverages code as an expressive and verifiable\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025;\nLambert et al., 2024). All prompts used by three different task types and\ntwo types of roles within a task type are shown in Figures 34 to 39. Next,\nwe outline exact details of our algorithm.\n\nEach reasoning task type leverages code as an expressive and verifable\n\nEach reasoning task type leverages code as an expressive and verifable\n\nEach reasoning task type leverages code as an expressive and verifiable\n\nEach reasoning task type leverages code as an expressive and verifiable\n\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\n\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\n\nProgram Triplet\nInput: \"Hello World\"\n1 def f(x):\n2 return x\nOutput: \"Hello World\"\n\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\n\nmedium, aligning with the Absolute Zero Paradigm's goals of fully self-\n\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025; Lam-\n\nimproving systems in open-ended domains (DeepSeek-AI et al.,\n\n2025;\n\nLam-\n\nProgram Triplet\n\nProgram Triplet\n\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025;\n\nimproving systems in open-ended domains\n\n(DeepSeek-AI et al.,\n\n2025;\n\nbert et al., 2024). All prompts used by three diferent task types and two\n\nbert et al.,\n\n2024). All prompts used by three diferent task types and two\n\nLambert et al., 2024). All prompts used by three different task types and\n\nLambert et al.,\n\n2024)\n\n. All prompts used by three different task types and\n\nInput: \"Hello World\"\n\nInput:\n\n\"Hello World\"\n\ntypes of roles within a task type are shown in Figures 34 to 39. Next, we\n\ntypes of roles within a task type are shown in Figures\n\n34\n\nto\n\n39. Next, we\n\ntwo types of roles within a task type are shown in Figures 34 to 39. Next,\n\ntwo types of roles within a task type are shown in Figures\n\n34\n\nto\n\n39.\n\nNext,\n\noutline exact details of our algorithm.\n\noutline exact details of our algorithm.\n\nwe outline exact details of our algorithm.\n\nwe outline exact details of our algorithm.\n\n1 def f(x):\n\ndef\n\nf(x):\n\n1\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n2 return x\n\nreturn\n\nx\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n3.3. Absolute Zero Reasoner Learning Algorithm\n\n2\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\ning initialization of bufers 3.3.1, usage of thse bufers 3.3.2, construction of\nvalid tasks 3.3.3, validating solutions 3.3.4, and fnally advantage estimator\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure of\nAZR in Algorithm 1.\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\ning initialization of buffers 3.3.1, usage of thse buffers 3.3.2, construction of\nvalid tasks 3.3.3, validating solutions 3.3.4, and finally advantage estimator\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure\nof AZR in Algorithm 1.\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\n\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\n\ning initialization of bufers 3.3.1, usage of thse bufers 3.3.2, construction of\n\ning initialization of bufers\n\n3.3.1, usage of thse bufers\n\n3.3.2, construction of\n\ning initialization of buffers 3.3.1, usage of thse buffers 3.3.2, construction of\n\ning initialization of buffers\n\nusage of thse buffers\n\n3.3.2, construction of\n\n3.3.1,\n\nvalid tasks 3.3.3, validating solutions 3.3.4, and fnally advantage estimator\n\nvalid tasks\n\n3.3.3, validating solutions\n\n3.3.4, and fn\n\nally ad\n\nvantage estimator\n\nOutput: \"Hello World\"\n\nOutput:\n\n\"Hello World\"\n\nvalid tasks 3.3.3, validating solutions 3.3.4, and finally advantage estimator\n\nvalid tasks\n\n3.3.3,\n\nvalidating solutions\n\n3.3.4, and finally advantage estimator\n\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure of\n\ncalculation\n\n3.3.5. We outline the o\n\nverall\n\nrecipe of the self-play procedure of\n\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure\n\ncalculation\n\n3.3.5. We outline the overall recipe of the self-play procedure\n\nAZR in Algorithm 1.\n\nAZR in\n\nAlgor\n\nithm\n\n1.\n\nof AZR in Algorithm 1.\n\nof AZR in Algorithm\n\n1.\n\nFigure 5. The Seed AZR Zero Triplet. The above\nidentity function triplet was the only triplet provided\nto AZR to initiate its self-bootstrap propose-and-solve\nRLVR loop. We note that the base LLM is fully ca-\npable of initiating the AZR loop without any seed pro-\ngram; its inclusion illustrates our approach's fexibility:\nwe can optionally initialize seed programs with existing\ndatasets of varying complexity, and we initialized ours\nwith the simplest program.\nFigure 5. The Seed AZR Zero Triplet. The above\nidentity function triplet was the only triplet provided\nto AZR to initiate its self-bootstrap propose-and-solve\nRLVR loop. We note that the base LLM is fully capable\nof initiating the AZR loop without any seed program;\nits inclusion illustrates our approach's flexibility: we\ncan optionally initialize seed programs with existing\ndatasets of varying complexity, and we initialized ours\nwith the simplest program.\n\nFigure 5. The Seed AZR Zero Triplet. The above\n\nThe above\n\nThe Seed AZR Zero Triplet.\n\nFigure 5. The Seed AZR Zero Triplet. The above\n\nFigure 5.\n\nThe above\n\nFigure 5.\n\nThe Seed AZR Zero Triplet.\n\n3.3.1. BUFFER INITIALIZATION\nTo initialize AZR self-play, we frst generate a seed set of valid triplets using\n3.3.1. Buffer Initialization\n\n3.3.1. BUFFER INITIALIZATION\n\n3.3.1. BUFFER INITIALIZATION\n\nidentity function triplet was the only triplet provided\n\nidentity function triplet was the only triplet provided\n\nidentity function triplet was the only triplet provided\n\nidentity function triplet was the only triplet provided\n\n3.3.1. Buffer Initialization\n\n3.3.1. Buffer Initialization\n\nto AZR to initiate its self-bootstrap propose-and-solve\n\nto AZR to initiate its self-bootstrap propose-and-solve\n\nto AZR to initiate its self-bootstrap propose-and-solve\n\nto AZR to initiate its self-bootstrap propose-and-solve\n\nTo initialize AZR self-play, we frst generate a seed set of valid triplets using\n\nTo initialize AZR self-play, we frst generate a seed set of valid triplets using\n\nRLVR loop. We note that the base LLM is fully capable\n\nRLVR loop. We note that the base LLM is fully capable\n\nRLVR loop. We note that the base LLM is fully ca-\n\nRLVR loop. We note that the base LLM is fully ca-\n\nthe base language model. Each prompt samples up to K triplets from the\ncurrent seed bufer Dseed as references. When Dseed is empty at time 0, we\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\nuse the same proposer prompts detailed in Figures 34 to 36.\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\nTo initialize AZR self-play, we first generate a seed set of valid triplets using\nthe base language model. Each prompt samples up to K triplets from the\ncurrent seed buffer Dseed as references. When Dseed is empty at time 0, we\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\nuse the same proposer prompts detailed in Figures 34 to 36.\n\nTo initialize AZR self-play, we first generate a seed set of valid triplets using\n\nTo initialize AZR self-play, we first generate a seed set of valid triplets using\n\nthe base language model. Each prompt samples up to K triplets from the\n\nthe base language model. Each prompt samples up to\n\ntriplets from the\n\nK\n\nof initiating the AZR loop without any seed program;\n\nof initiating the AZR loop without any seed program;\n\npable of initiating the AZR loop without any seed pro-\n\npable of initiating the AZR loop without any seed pro-\n\nthe base language model. Each prompt samples up to K triplets from the\n\nthe base language model. Each prompt samples up to\n\ntriplets from the\n\nK\n\ncurrent seed bufer Dseed as references. When Dseed is empty at time 0, we\n\ncurrent seed bufer\n\nas references. When\n\nis empty at time 0, we\n\nD\n\nD\n\nseed\n\nseed\n\nits inclusion illustrates our approach's flexibility: we\n\nits inclusion illustrates our approach's flexibility: we\n\ngram; its inclusion illustrates our approach's fexibility:\n\ngram; its inclusion illustrates our approach's fexibility:\n\ncurrent seed buffer Dseed as references. When Dseed is empty at time 0, we\n\ncurrent seed buffer\n\nas references. When\n\nis empty at time 0, we\n\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\n\nfall back to the zero triplet show in Figure\n\n5. During the seeding stage, we\n\nseed\n\nseed\n\nD\n\nD\n\nwe can optionally initialize seed programs with existing\n\nwe can optionally initialize seed programs with existing\n\ncan optionally initialize seed programs with existing\n\ncan optionally initialize seed programs with existing\n\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\n\nfall back to the zero triplet show in Figure\n\n5. During the seeding stage, we\n\nuse the same proposer prompts detailed in Figures 34 to 36.\n\nuse the same proposer prompts detailed\n\nin\n\nFigures\n\n34\n\nto\n\n36.\n\ndatasets of varying complexity, and we initialized ours\n\ndatasets of varying complexity, and we initialized ours\n\ndatasets of varying complexity, and we initialized ours\n\ndatasets of varying complexity, and we initialized ours\n\nuse the same proposer prompts detailed in Figures 34 to 36.\n\nuse the same proposer prompts detailed in Figures\n\n34\n\nto\n\n36.\n\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n\nwith the simplest program.\n\nwith the simplest program.\n\nwith the simplest program.\n\nwith the simplest program.\n\n(p, i) pairs, which are fltered, executed, and stored as valid triplets. We\ninitialize D0\nabduction = D0\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n(p, i) pairs, which are filtered, executed, and stored as valid triplets. We\n\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n\n(p, i) pairs, which are fltered, executed, and stored as valid triplets. We\n\n(\n\n)\n\npairs, which are fltered, executed, and stored as valid triplets. We\n\np, i\n\nabduction = D0\n\n0\n\ninitialize D0\n\n0\n\n(p, i) pairs, which are filtered, executed, and stored as valid triplets. We\n\npairs, which are filtered, executed, and stored as valid triplets. We\n\n=\n\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where\n\n=\n\n=\n\ninitialize\n\nseed, where\n\n, where\n\nB\n\nS\n\n(\n\n)\n\np, i\n\nD\n\nD\n\nD\n\n|D\n\n|\n\n\u00d7\n\nseed\n\nabduction\n\ndeduction\n\nB is the batch size, and S = 4 is a factor we fx in all experiments. All seed triplet's program are stripped of global variables and\ncomments (Appendix C), but subsequent iterations of adding new triplets to the bufers are unaltered. No model updates occur during\nthis phase. Similarly, to initialize the induction bufer, we sample programs from Dseed, generate matching input sets and messages, and\ncollect valid examples until |D0\ninduction| = B \u00d7 S.\ninitialize D\n0\nabduction = D\n0\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where B is the batch size, and S = 4 is a factor we fix in all\nexperiments. All seed triplet's program are stripped of global variables and comments (Appendix D), but subsequent iterations of adding\nnew triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we\nsample programs from Dseed, generate matching input sets and messages, and collect valid examples until |D0\ninduction| = B \u00d7 S.\n\ninitialize D\n\ninitialize\n\ndeduction = Dseed, where |Dseed| = B \u00d7 S, where B is the batch size, and S = 4 is a factor we fix in all\n\n, where\n\n, where\n\nis the batch size, and\n\nis a factor we fix in all\n\nB is the batch size, and S = 4 is a factor we fx in all experiments. All seed triplet's program are stripped of global variables and\n\n= 4\n\nis the batch size, and\n\nis a factor we fx in all experiments. All seed triplet's program are stripped of global variables and\n\n0\n\n0\n\n0\n\n0\n\nB\n\nS\n\nabduction = D\n\n=\n\n=\n\n=\n\n= 4\n\nB\n\nS\n\nB\n\nS\n\nseed\n\nseed\n\nabduction\n\ndeduction\n\nD\n\nD\n\nD\n\n|D\n\n|\n\n\u00d7\n\ncomments (Appendix C), but subsequent iterations of adding new triplets to the bufers are unaltered. No model updates occur during\n\ncomments (Appendix\n\nC), but subsequent iterations of adding new triplets to the bufers are un\n\nalte\n\nred. No model updates occur during\n\nexperiments. All seed triplet's program are stripped of global variables and comments (Appendix D), but subsequent iterations of adding\n\nexperiments. All seed triplet's program are stripped of global variables and comments (Appendix\n\nD), but subsequent iterations of adding\n\nthis phase. Similarly, to initialize the induction bufer, we sample programs from Dseed, generate matching input sets and messages, and\n\nthis phase. Similarly, to initialize the induction bufer, we sample programs from\n\nseed, generate matching input sets and messages, and\n\nnew triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we\n\nnew triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we\n\nD\n\ncollect valid examples until |D0\n\n0\n\ninduction| = B \u00d7 S.\n\n=\n\ncollect valid examples until\n\n.\n\nB\n\nS\n\nsample programs from Dseed, generate matching input sets and messages, and collect valid examples until |D0\n\nsample programs from\n\n, generate matching input sets and messages, and collect valid examples until\n\ninduction| = B \u00d7 S.\n\n.\n\n0\n\n=\n\n|D\n\n|\n\n\u00d7\n\nB\n\nS\n\ninduction\n\nseed\n\ninduction\n\nD\n\n|D\n\n|\n\n\u00d7\n\n6\n6\n\n6\n\n6\n\n6\n\n6",
        "tables": []
      },
      {
        "page_num": 6,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAlgorithm 1Self-Play Training of Absolute Zero Reasoner (AZR)Require:Pretrained base LLM\n\u03c0\u03b8; batch size\nB; #referencesK; iterations\nT1:\nDded,\nDabd,\nDind\u2190 InitSeeding(\u03c0\u03b8)see \u00a73.3.1\n\u25b7to\n2:\n\u2190 1\nfor\nt\nTdo3:\nfor\n\u2190 1\nbto\ndo\nB\u25b7PROPOSE PHASE4:\np\n\u223c Dabd\u222a Ddedsample a program for induction task proposal\n\u25b7N\nn\n5:\n\ni\n\u03c0propose\n(ind, p)\nn=1, m\u03c0\n\u2190 \u03c0\n\u03b8generate\n\u25b7\nNinputs and a descriptionn\n6:\n\n(i\nif\n\u03c0N\nn=1 \u2190 ValidateByExecuting\n, on\n)\np,\n{i\n\u03c0syntax\nn\n\u03c0},\nthen\n\u25b7validate I/Os, see \u00a73.3.37:\nDindn\n, on\n\n(p,\n{(i\n)}, m\u03c0)\n\u2190 Dind\n\u222a\n\u03c0\n\u03c0\u25b7update induction buffer8:\nfor\n\u03b1\u2208 {ded,\nabd}\ndo9:\nK\npk, ik, ok\nk=1 \u223c D\u03b1sample\n\u25b7\nKreference examples10:propose\n\n\n(p\u03c0, i\u03c0)\n{(pk, ik, ok)}\n\u2190 \u03c0\n\u03b1,\n\u03b8\u25b7propose new task11:\nif\u2190 ValidateByExecuting\no\u03c0\np\u03c0, i\u03c0,syntax,safety,determinism\nthensee \u00a73.3.3\n\u25b712:\n(p\u03c0, i\u03c0, o\u03c0)\nD\u03b1 \u2190 D\u03b1 \u222aif valid, update deduction or abduction buffers\n\u25b713:\nfor all\n\u03b1\u2208 {ded,\nabd,\nind}\ndo\u25b7SOLVE PHASE(x, y\u22c6\n14:\u2190 SamplePrepareTasks\nD\u03b1, B, t\n)\u25b7 x, y\u22c6 prepared based on\u03b1, see \u00a73.3.3&amp;3.3.415:\ny\u03c0\n\u223c \u03c0solve\n(x)\n\u03b816:\nReward:Use proposed task triplets and solved answers to get&amp;\nrpropose\nrsolvesee \u00a73.1\n\u25b717:\nRL update:use Task Relative REINFORCE++ to update\u03c0\u03b8see \u00a73.3.5\n\u25b7\n\nAlgorithm 1\n\nSelf-Play Training of Absolute Zero Reasoner (AZR)\n\nRequire:\n\nPretrained base LLM\n\u03c0\u03b8; batch size\nB; #references\n\nK; iterations\nT\n\n1:\nDded,\nDabd,\nDind\n\n\u2190 InitSeeding(\u03c0\u03b8)\n\nsee \u00a73.3.1\n\u25b7\n\nto\n2:\n\u2190 1\nfor\nt\nT\n\ndo\n\n3:\nfor\n\u2190 1\nb\n\nto\ndo\nB\n\n\u25b7\n\nPROPOSE PHASE\n\n4:\np\n\u223c Dabd\n\n\u222a Dded\n\nsample a program for induction task proposal\n\u25b7\n\nN\nn\n5:\n\ni\n\u03c0\n\npropose\n(ind, p)\nn=1, m\u03c0\n\u2190 \u03c0\n\u03b8\n\ngenerate\n\u25b7\nN\n\ninputs and a description\n\nn\n6:\n\n(i\nif\n\u03c0\n\nN\nn=1 \u2190 ValidateByExecuting\n, on\n)\np,\n{i\n\u03c0\n\nsyntax\nn\n\u03c0},\nthen\n\u25b7\n\nvalidate I/Os, see \u00a73.3.3\n\n7:\nDind\n\nn\n, on\n\n(p,\n{(i\n)}, m\u03c0)\n\u2190 Dind\n\u222a\n\u03c0\n\u03c0\n\n\u25b7\n\nupdate induction buffer\n\n8:\nfor\n\u03b1\n\n\u2208 {ded,\nabd}\ndo\n\n9:\n\nK\npk, ik, ok\nk=1 \u223c D\u03b1\n\nsample\n\u25b7\nK\n\nreference examples\n\n10:\n\npropose\n\n\n(p\u03c0, i\u03c0)\n{(pk, ik, ok)}\n\u2190 \u03c0\n\u03b1,\n\u03b8\n\n\u25b7\n\npropose new task\n\n11:\nif\n\n\u2190 ValidateByExecuting\no\u03c0\np\u03c0, i\u03c0,\n\nsyntax,safety,determinism\nthen\n\nsee \u00a73.3.3\n\u25b7\n\n12:\n\n(p\u03c0, i\u03c0, o\u03c0)\nD\u03b1 \u2190 D\u03b1 \u222a\n\nif valid, update deduction or abduction buffers\n\u25b7\n\n13:\nfor all\n\u03b1\n\n\u2208 {ded,\nabd,\nind}\ndo\n\n\u25b7\n\nSOLVE PHASE\n\n(x, y\u22c6\n14:\n\n\u2190 SamplePrepareTasks\nD\u03b1, B, t\n)\n\n\u25b7 x, y\u22c6 prepared based on\n\n\u03b1, see \u00a73.3.3&amp;3.3.4\n\n15:\ny\u03c0\n\u223c \u03c0\n\nsolve\n(x)\n\u03b8\n\n16:\nReward:\n\nUse proposed task triplets and solved answers to get\n\n&amp;\nrpropose\nrsolve\n\nsee \u00a73.1\n\u25b7\n\n17:\nRL update:\n\nuse Task Relative REINFORCE++ to update\n\n\u03c0\u03b8\n\nsee \u00a73.3.5\n\u25b7\n\n3.3.2. Task Proposal Inputs and Buffer Management\n\n3.3.2. Task Proposal Inputs and Buffer Management\n\n3.3.2. Task Proposal Inputs and Buffer Management\n\nDuring the actual self-play stage of AZR, we use the task buffer in three ways. First, for the proposer of abduction and deduction tasks,\nwe uniformly sample K past triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task.\nThe design is to show it past examples, and prompt it to generate a different one to promote diversity (Zhao et al., 2025a). Second, we\nsample one triplet from the union of abduction and deduction buffers Dabd S\nDded, and present the program p from that triplet to the\ninduction proposer to generate a set of N matching inputs {i\nn\n} and a natural language message m. Lastly, to maintain stable training, if\na batch of solver problems contains fewer than B valid proposed tasks (proposer not adhering to formatting), we fill the remainder by\nuniformly sampling from the corresponding task buffer of previously validated triplets.\n\nDuring the actual self-play stage of AZR, we use the task buffer in three ways. First, for the proposer of abduction and deduction tasks,\n\nDuring the actual self-play stage of AZR, we use the task buffer in three ways.\n\n, for the proposer of abduction and deduction tasks,\n\nFirst\n\nwe uniformly sample K past triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task.\n\nwe uniformly sample\n\npast triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task.\n\nK\n\nThe design is to show it past examples, and prompt it to generate a different one to promote diversity (Zhao et al., 2025a). Second, we\n\nThe design is to show it past examples, and prompt it to generate a different one to promote diversity\n\n(Zhao et al.,\n\n2025a)\n\n.\n\n, we\n\nSecond\n\nsample one triplet from the union of abduction and deduction buffers Dabd S\n\nsample one triplet from the union of abduction and deduction buffers\n\nDded, and present the program p from that triplet to the\n\n, and present the program\n\nfrom that triplet to the\n\nS\n\np\n\nabd\n\nded\n\nD\n\nD\n\ninduction proposer to generate a set of N matching inputs {i\n\ninduction proposer to generate a set of\n\nmatching inputs\n\n} and a natural language message m. Lastly, to maintain stable training, if\n\nand a natural language message\n\n, to maintain stable training, if\n\n.\n\nn\n\nn\n\nLastly\n\nN\n\ni\n\nm\n\n{\n\n}\n\na batch of solver problems contains fewer than B valid proposed tasks (proposer not adhering to formatting), we fill the remainder by\n\na batch of solver problems contains fewer than\n\nvalid proposed tasks (proposer not adhering to formatting), we fill the remainder by\n\nB\n\nuniformly sampling from the corresponding task buffer of previously validated triplets.\n\nuniformly sampling from the corresponding task buffer of previously validated triplets.\n\nThe buffer grows for abduction and deduction tasks whenever \u03c0 propose a valid triplet (p, i, o), regardless if it gets any task reward.\nSimilarly, for induction tasks, all valid triplets (p, {i\nn\n, on\n}), m are added to the buffer.\n\nThe buffer grows for abduction and deduction tasks whenever \u03c0 propose a valid triplet (p, i, o), regardless if it gets any task reward.\n\nThe buffer grows for abduction and deduction tasks whenever\n\npropose a valid triplet\n\n, regardless if it gets any task reward.\n\n(\n\n)\n\n\u03c0\n\np, i, o\n\nSimilarly, for induction tasks, all valid triplets (p, {i\n\nSimilarly, for induction tasks, all valid triplets\n\n}), m are added to the buffer.\n\nare added to the buffer.\n\nn\n\nn\n\n, on\n\nn\n\n(\n\n)\n\np,\n\ni\n\n, o\n\n, m\n\n{\n\n}\n\n3.3.3. Constructing Valid Tasks\n\n3.3.3. Constructing Valid Tasks\n\n3.3.3. Constructing Valid Tasks\n\nProposal Task Validation. We first describe how we construct valid tasks from the proposals generated by the policy \u03c0. For deduction\nand abduction tasks, each proposal consists of a program and an input (p, i). To validate the task, we use the task validation procedure\n(steps shown below) on the input to obtain the correct output o, resulting in a complete triplet (p, i, o). For induction tasks, given a\nprogram p the policy proposes a set of inputs {i\nn\n} and message m. We also use the task validation procedure on each of the input i\nn\nin the set to obtain a corresponding output o\nn\n, forming a set of input-output pairs {i\nn\n, on\n}. We do not impose any constraints on m.\nThe resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. The task\nvalidation procedure entails:\n\nProposal Task Validation. We first describe how we construct valid tasks from the proposals generated by the policy \u03c0. For deduction\n\nWe first describe how we construct valid tasks from the proposals generated by the policy\n\n. For\n\nProposal Task Validation.\n\ndeduction\n\n\u03c0\n\nand abduction tasks, each proposal consists of a program and an input (p, i). To validate the task, we use the task validation procedure\n\ntasks, each proposal consists of a program and an input\n\n. To validate the task, we use the task validation procedure\n\n(\n\n)\n\nand abduction\n\np, i\n\n(steps shown below) on the input to obtain the correct output o, resulting in a complete triplet (p, i, o). For induction tasks, given a\n\n(steps shown below) on the input to obtain the correct output\n\n, resulting in a complete triplet\n\n. For\n\ntasks, given a\n\n(\n\n)\n\ninduction\n\no\n\np, i, o\n\nprogram p the policy proposes a set of inputs {i\n\nprogram\n\nthe policy proposes a set of inputs\n\n} and message m. We also use the task validation procedure on each of the input i\n\nand message\n\n. We also use the task validation procedure on each of the input\n\nn\n\nn\n\nn\n\nn\n\np\n\ni\n\nm\n\ni\n\n{\n\n}\n\nin the set to obtain a corresponding output o\n\nin the set to obtain a corresponding output\n\n, forming a set of input-output pairs {i\n\n, forming a set of input-output pairs\n\n}. We do not impose any constraints on m.\n\n. We do not impose any constraints on\n\n.\n\nn\n\nn\n\nn\n\nn\n\n, on\n\nn\n\no\n\ni\n\n, o\n\nm\n\n{\n\n}\n\nThe resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. The task\n\nThe resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. The\n\ntask\n\nvalidation procedure entails:\n\nentails:\n\nvalidation procedure\n\n1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.\n2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\nFigures 34 to 36.\n3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p \u2208 Pdeterministic \u2282 P, where P is the space\nof all valid programs and I is the space of all valid inputs:\n\n1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.\n\n1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\n\nWe first use Python to run the program\n\nwith the input\n\n. If no errors are raised and something is returned, we\n\n1.\n\nProgram Integrity.\n\np\n\ni\n\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.\n\nthen gather the output\n\nof that\n\npair and determine that the program at least has valid syntax.\n\n(\n\n)\n\no\n\np, i\n\n2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\nFigures 34 to 36.\n\n2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\n\n2.\n\nWe also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\n\nProgram Safety.\n\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\n\ncause harm to the Python environment,\n\n.,\n\n. The list of packages used to filter out invalid programs is\n\ni.e\n\nos.sys, sys, shutil\n\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\n\nprovided in Figure\n\n8.\n\nThis list is also included in the instructions when prompting the language model to generate questions. See\n\nFigures 34 to 36.\n\nFigures\n\n34\n\nto\n\n36.\n\n3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p \u2208 Pdeterministic \u2282 P, where P is the space\nof all valid programs and I is the space of all valid inputs:\n\n3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p \u2208 Pdeterministic \u2282 P, where P is the space\n\n3.\n\nIn our setting, we only consider\n\n, where\n\nis the space\n\n,\n\n.,\n\nCheck for Determinism.\n\ndeterministic programs\n\ni.e\n\np\n\nP\n\nP\n\nP\n\ndeterministic\n\n\u2208\n\n\u2282\n\nof all valid programs and I is the space of all valid inputs:\n\nof all valid programs and\n\nis the space of all valid inputs:\n\nI\n\n7\n\n7\n\n7",
        "tables": [
          {
            "text": "Algorithm 1Self-Play Training of Absolute Zero Reasoner (AZR)Require:Pretrained base LLM\n\u03c0\u03b8; batch size\nB; #referencesK; iterations\nT1:\nDded,\nDabd,\nDind\u2190 InitSeeding(\u03c0\u03b8)see \u00a73.3.1\n\u25b7to\n2:\n\u2190 1\nfor\nt\nTdo3:\nfor\n\u2190 1\nbto\ndo\nB\u25b7PROPOSE PHASE4:\np\n\u223c Dabd\u222a Ddedsample a program for induction task proposal\n\u25b7N\nn\n5:\n\ni\n\u03c0propose\n(ind, p)\nn=1, m\u03c0\n\u2190 \u03c0\n\u03b8generate\n\u25b7\nNinputs and a descriptionn\n6:\n\n(i\nif\n\u03c0N\nn=1 \u2190 ValidateByExecuting\n, on\n)\np,\n{i\n\u03c0syntax\nn\n\u03c0},\nthen\n\u25b7validate I/Os, see \u00a73.3.37:\nDindn\n, on\n\n(p,\n{(i\n)}, m\u03c0)\n\u2190 Dind\n\u222a\n\u03c0\n\u03c0\u25b7update induction buffer8:\nfor\n\u03b1\u2208 {ded,\nabd}\ndo9:\nK\npk, ik, ok\nk=1 \u223c D\u03b1sample\n\u25b7\nKreference examples10:propose\n\n\n(p\u03c0, i\u03c0)\n{(pk, ik, ok)}\n\u2190 \u03c0\n\u03b1,\n\u03b8\u25b7propose new task11:\nif\u2190 ValidateByExecuting\no\u03c0\np\u03c0, i\u03c0,syntax,safety,determinism\nthensee \u00a73.3.3\n\u25b712:\n(p\u03c0, i\u03c0, o\u03c0)\nD\u03b1 \u2190 D\u03b1 \u222aif valid, update deduction or abduction buffers\n\u25b713:\nfor all\n\u03b1\u2208 {ded,\nabd,\nind}\ndo\u25b7SOLVE PHASE(x, y\u22c6\n14:\u2190 SamplePrepareTasks\nD\u03b1, B, t\n)\u25b7 x, y\u22c6 prepared based on\u03b1, see \u00a73.3.3&amp;3.3.415:\ny\u03c0\n\u223c \u03c0solve\n(x)\n\u03b816:\nReward:Use proposed task triplets and solved answers to get&amp;\nrpropose\nrsolvesee \u00a73.1\n\u25b717:\nRL update:use Task Relative REINFORCE++ to update\u03c0\u03b8see \u00a73.3.5\n\u25b7",
            "csv": "",
            "json": "{\n  \"headers\": [],\n  \"rows\": []\n}"
          }
        ]
      },
      {
        "page_num": 7,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n\u2200p \u2208 Pdeterministic, \u2200i \u2208 I ,\n\nlim\nj\u2192\u221e\np(i)\n(1) = p(i)\n(2) = \u00b7 \u00b7 \u00b7 = p(i)\n(j)\n\n, (7)\n\n(1) = p(i)\n\n(1)\n\n(2) = \u00b7 \u00b7 \u00b7 = p(i)\n\n(2)\n\n(j)\n\n(\n\n)\n\nj\n\n, (7)\n\n(7)\n\nlim\n\nlim\n\np(i)\n\n(\n\n)\n\n=\n\n(\n\n)\n\n=\n\n=\n\n(\n\n)\n\n\u2200p \u2208 Pdeterministic, \u2200i \u2208 I ,\n\np\n\n,\n\ni\n\n,\n\np\n\ni\n\np\n\ni\n\np\n\ni\n\n,\n\nP\n\nI\n\ndeterministic\n\n\u2200\n\n\u2208\n\n\u2200\n\n\u2208\n\n\u00b7 \u00b7 \u00b7\n\nj\u2192\u221e\n\nj\n\n\u2192\u221e\n\nwhere (j) indexes repeated independent executions of the program. That is, for all inputs i, the output of p(i) remains identical\nwith any independent execution of the program. A valid program/input/output triplet (p, i, o) is defined such that o = p(i), where\np \u2208 Pdeterministic.\n\nwhere (j) indexes repeated independent executions of the program. That is, for all inputs i, the output of p(i) remains identical\n\nwhere\n\nindexes repeated independent executions of the program. That is, for all inputs\n\n, the output of\n\nremains identical\n\n(\n\n)\n\n(\n\n)\n\nj\n\ni\n\np\n\ni\n\nwith any independent execution of the program. A valid program/input/output triplet (p, i, o) is defined such that o = p(i), where\n\nwith any independent execution of the program. A\n\nis defined such that\n\n, where\n\n(\n\n)\n\n=\n\n(\n\n)\n\nvalid program/input/output triplet\n\np, i, o\n\no\n\np\n\ni\n\np \u2208 Pdeterministic.\n\n.\n\np\n\nP\n\ndeterministic\n\n\u2208\n\nSince the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the\ncorrectness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class\nof deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and\npromising to include in future versions of AZR.\n\nSince the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the\n\nSince the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the\n\ncorrectness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class\n\ncorrectness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class\n\nof deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and\n\nof deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and\n\npromising to include in future versions of AZR.\n\npromising to include in future versions of AZR.\n\nTo implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted in\nEquation (7), we approximate this procedure by independently running the program j finite times and checking that all the outputs\nare equal. For computational budget reasons, we fixed j = 2 for all experiments.\n\nTo implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted in\n\nTo implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted in\n\nEquation (7), we approximate this procedure by independently running the program j finite times and checking that all the outputs\n\nEquation\n\n(7)\n\n, we approximate this procedure by independently running the program\n\nfinite times and checking that all the outputs\n\nj\n\nare equal. For computational budget reasons, we fixed j = 2 for all experiments.\n\nare equal. For computational budget reasons, we fixed\n\nfor all experiments.\n\n= 2\n\nj\n\nSolving Task Construction. If a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to\npresent part of the triplet to the solver. Specifically, we set x = (p, i)for deduction; x = (p, o)for abduction; and x = ({i\nn\n, on\n}\nN//2\nn=1 , m)\nfor induction, where half of the tests cases and a program description m is used. We use all valid tasks from timestep t; if the batch B is\nnot full, we uniformly sample from previously validated tasks to fill the batch.\n\nSolving Task Construction. If a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to\n\nIf a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to\n\nSolving Task Construction.\n\nN//2\n\n2\n\nN//\n\npresent part of the triplet to the solver. Specifically, we set x = (p, i)for deduction; x = (p, o)for abduction; and x = ({i\n\npresent part of the triplet to the solver. Specifically, we set\n\nfor deduction;\n\nfor abduction; and\n\nn\n\nn\n\n, on\n\nn\n\n= (\n\n)\n\n= (\n\n)\n\n= (\n\nn=1 , m)\n\n)\n\nx\n\np, i\n\nx\n\np, o\n\nx\n\ni\n\n, o\n\n, m\n\n{\n\n}\n\n}\n\n=1\n\nn\n\nfor induction, where half of the tests cases and a program description m is used. We use all valid tasks from timestep t; if the batch B is\n\nfor induction, where half of the tests cases and a program description\n\nis used. We use all valid tasks from timestep\n\n; if the batch\n\nis\n\nm\n\nt\n\nB\n\nnot full, we uniformly sample from previously validated tasks to fill the batch.\n\nnot full, we uniformly sample from previously validated tasks to fill the batch.\n\n3.3.4. Answer Verification\n\n3.3.4. Answer Verification\n\n3.3.4. Answer Verification\n\nFor abduction task, we receive i\u03c0 from the solver policy, then we equivalence match using p(i\u03c0) = p(i\n\u22c6\n), where \u2217 refers to the\nprivileged gold information. The reason we do not just match i\u03c0 and i\n\u22c6\nis because p is not necessarily bijective. For deduction task, we\nmatch o\u03c0 = o\n\u22c6\n. For induction, we match all({p\u03c0(i\n\u22c6\nn) = o\n\u22c6\nn}\nN ). This part might be convoluted to explain in language, therefore we\nrecommend the reader to see how we did abduction, deduction and induction verification in code in Figures 10 to 12, respectively.\n\nFor abduction task, we receive i\u03c0 from the solver policy, then we equivalence match using p(i\u03c0) = p(i\n\nFor abduction task, we receive\n\nfrom the solver policy, then we equivalence match using\n\n), where \u2217 refers to the\n\n, where\n\nrefers to the\n\n\u22c6\n\n\u22c6\n\n(\n\n) =\n\n(\n\n)\n\ni\n\np\n\ni\n\np\n\ni\n\n\u2217\n\n\u03c0\n\n\u03c0\n\nprivileged gold information. The reason we do not just match i\u03c0 and i\n\nprivileged gold information. The reason we do not just match\n\nis because p is not necessarily bijective. For deduction task, we\n\nis because\n\nis not necessarily bijective. For deduction task, we\n\nand\n\n\u22c6\n\n\u22c6\n\ni\n\ni\n\np\n\n\u03c0\n\nmatch o\u03c0 = o\n\nmatch\n\n. For induction, we match all({p\u03c0(i\n\n. For induction, we match\n\nN ). This part might be convoluted to explain in language, therefore we\n\n. This part might be convoluted to explain in language, therefore we\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\n\u22c6\n\nN\n\n=\n\nall(\n\n(\n\nn) = o\n\n) =\n\n)\n\no\n\no\n\np\n\ni\n\no\n\n{\n\nn}\n\n}\n\n\u03c0\n\n\u03c0\n\nn\n\nn\n\nrecommend the reader to see how we did abduction, deduction and induction verification in code in Figures 10 to 12, respectively.\n\nrecommend the reader to see how we did abduction, deduction and induction verification in code in Figures\n\n10\n\nto\n\n12,\n\nrespectively.\n\n3.3.5. Task-Relative REINFORCE++\n\n3.3.5. Task-Relative REINFORCE++\n\n3.3.5. Task-Relative REINFORCE++\n\nSince AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup (Zhang & Yang, 2021;\nZhao et al., 2022; Wang et al., 2023; Yue et al., 2023). Instead of computing a single global baseline as in REINFORCE++ (Hu, 2025)\n(Appendix A), we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between\nper-question baselines, as in GRPO (Shao et al., 2024), and a global baseline, allowing for more structured variance reduction tailored to\neach task setup. We refer to this variant as Task-Relative REINFORCE++ (TRR++). The normalized advantage A\nnorm is computed as:\n\nSince AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup (Zhang & Yang, 2021;\n\nSince AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup\n\n(Zhang & Yang,\n\n2021;\n\nZhao et al., 2022; Wang et al., 2023; Yue et al., 2023). Instead of computing a single global baseline as in REINFORCE++ (Hu, 2025)\n\nZhao et al.,\n\nWang et al.,\n\nYue et al.,\n\n. Instead of computing a single global baseline as in REINFORCE++\n\n2022;\n\n2023;\n\n2023)\n\n(Hu,\n\n2025)\n\n(Appendix A), we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between\n\n, we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between\n\n(Appendix\n\nA)\n\nper-question baselines, as in GRPO (Shao et al., 2024), and a global baseline, allowing for more structured variance reduction tailored to\n\nper-question baselines, as in GRPO\n\n(Shao et al.,\n\n, and a global baseline, allowing for more structured variance reduction tailored to\n\n2024)\n\nnorm is computed as:\n\nnorm\n\neach task setup. We refer to this variant as Task-Relative REINFORCE++ (TRR++). The normalized advantage A\n\neach task setup. We refer to this variant as\n\n. The normalized advantage\n\nis computed as:\n\nTask-Relative REINFORCE++ (TRR++)\n\nA\n\nA\nnorm\ntask,role =\nr \u2212 \u00b5task,role\n\u03c3task,role\n, task \u2208 {ind,ded,abd}, role \u2208 {propose,solve}, (8)\n\nr \u2212 \u00b5task,role\n\nr\n\n\u00b5\n\ntask,role\n\nnorm\n\nnorm\n\n\u2212\n\n, task \u2208 {ind,ded,abd}, role \u2208 {propose,solve}, (8)\n\ntask\n\nind,ded,abd\n\nrole\n\npropose,solve\n\n(8)\n\ntask,role =\n\n=\n\nA\n\nA\n\n,\n\n,\n\n,\n\ntask,role\n\n\u2208 {\n\n}\n\n\u2208 {\n\n}\n\n\u03c3task,role\n\n\u03c3\n\ntask,role\n\nwhere the mean and standard deviation are computed within each task type and role, yielding six baselines.\n\nwhere the mean and standard deviation are computed within each task type and role, yielding six baselines.\n\nwhere the mean and standard deviation are computed\n\n, yielding six baselines.\n\nwithin each task type and role\n\n4. Experiments\n\n4. Experiments\n\n4. Experiments\n\n4.1. Experiment Setup\n\n4.1. Experiment Setup\n\n4.1. Experiment Setup\n\nTraining Details. For all experiments, we initialize the buffers as described in Section 3.1. AZR models are trained using a batch\nsize of 64 \u00d7 6 (2 roles \u00d7 3 task types). We use constant learning rate= 1e\u22126 and the AdamW optimizer (Loshchilov & Hutter, 2019).\nComplete list of hyperparameters is provided in Table 3.\n\nTraining Details. For all experiments, we initialize the buffers as described in Section 3.1. AZR models are trained using a batch\n\nFor all experiments, we initialize the buffers as described in Section\n\nAZR models are trained using a batch\n\n3.1.\n\nTraining Details.\n\nsize of 64 \u00d7 6 (2 roles \u00d7 3 task types). We use constant learning rate= 1e\u22126 and the AdamW optimizer (Loshchilov & Hutter, 2019).\n\nsize of\n\n(2 roles\n\n3 task types). We use constant learning rate\n\nand the AdamW optimizer\n\n(Loshchilov & Hutter,\n\n2019)\n\n.\n\n64\n\n6\n\n= 1\n\n6\n\ne\n\n\u00d7\n\n\u00d7\n\n\u2212\n\nComplete list of hyperparameters is provided in Table 3.\n\nComplete list of hyperparameters is provided in Table\n\n3.\n\nFor the main experiments, we train AZR models on Qwen2.5-7B and Qwen2.5-7B-Coder, resulting in Absolute\nZero Reasoner-base-7B and Absolute Zero Reasoner-Coder-7B, respectively. Additional experiments include training\nQwen2.5-Coder-3B, Qwen2.5-Coder-14B, Qwen2.5-14B, Llama-3.1-8B (Yang et al., 2024a; Hui et al., 2024; Dubey et al.,\n2024).\n\nFor the main experiments, we train AZR models on Qwen2.5-7B and Qwen2.5-7B-Coder, resulting in Absolute\n\nFor the main experiments, we train AZR models on\n\nand\n\n, resulting in\n\nQwen2.5-7B\n\nQwen2.5-7B-Coder\n\nAbsolute\n\nZero Reasoner-base-7B and Absolute Zero Reasoner-Coder-7B, respectively. Additional experiments include training\n\nand\n\n, respectively. Additional experiments include training\n\nZero Reasoner-base-7B\n\nAbsolute Zero Reasoner-Coder-7B\n\nQwen2.5-Coder-3B, Qwen2.5-Coder-14B, Qwen2.5-14B, Llama-3.1-8B (Yang et al., 2024a; Hui et al., 2024; Dubey et al.,\n\n,\n\n,\n\n,\n\n(Yang et al.,\n\n2024a;\n\nHui et al.,\n\n2024;\n\nDubey et al.,\n\nQwen2.5-Coder-3B\n\nQwen2.5-Coder-14B\n\nQwen2.5-14B\n\nLlama-3.1-8B\n\n2024).\n\n2024)\n\n.\n\nEvaluation Protocol. To evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD)\ncategories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning\nbenchmarks. For coding tasks, we evaluate using Evalplus (Liu et al., 2023) on the HumanEval+ and MBPP+ benchmarks, as\nwell as LiveCodeBench Generation (v1-5, May 23-Feb 25) (Jain et al., 2024). For mathematical reasoning, we utilize six standard\nbenchmarks commonly used in recent zero-shot trained reasoners: AIME'24, AIME'25, OlympiadBench (He et al., 2024), Minerva,\nMath500 (Hendrycks et al., 2021), and AMC'23. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-\nExecution (Gu et al., 2024; Jain et al., 2024), which assess reasoning capabilities regarding the input and output of programs (Li et al.,\n2025). Greedy decoding is used for all baseline methods and AZR results to ensure reproducibility.\n\nEvaluation Protocol. To evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD)\n\nTo evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD)\n\nEvaluation Protocol.\n\ncategories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning\n\ncategories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning\n\nbenchmarks. For coding tasks, we evaluate using Evalplus (Liu et al., 2023) on the HumanEval+ and MBPP+ benchmarks, as\n\nbenchmarks. For coding tasks, we evaluate using Evalplus\n\n(Liu et al.,\n\n2023)\n\non the HumanEval+ and MBPP+ benchmarks, as\n\nwell as LiveCodeBench Generation (v1-5, May 23-Feb 25) (Jain et al., 2024). For mathematical reasoning, we utilize six standard\n\nwell as LiveCodeBench Generation (v1-5, May 23-Feb 25)\n\n(Jain et al.,\n\n2024)\n\n. For mathematical reasoning, we utilize six standard\n\nbenchmarks commonly used in recent zero-shot trained reasoners: AIME'24, AIME'25, OlympiadBench (He et al., 2024), Minerva,\n\nbenchmarks commonly used in recent zero-shot trained reasoners: AIME'24, AIME'25, OlympiadBench\n\n(He et al.,\n\n2024)\n\n, Minerva,\n\nMath500 (Hendrycks et al., 2021), and AMC'23. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-\n\nMath500\n\n(Hendrycks et al.,\n\n, and AMC'23. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-\n\n2021)\n\nExecution (Gu et al., 2024; Jain et al., 2024), which assess reasoning capabilities regarding the input and output of programs (Li et al.,\n\nExecution\n\n(Gu et al.,\n\nJain et al.,\n\n, which assess reasoning capabilities regarding the input and output of programs\n\n(Li et al.,\n\n2024;\n\n2024)\n\n2025). Greedy decoding is used for all baseline methods and AZR results to ensure reproducibility.\n\n2025)\n\n.\n\nis used for all baseline methods and AZR results to ensure reproducibility.\n\nGreedy decoding\n\n8\n\n8\n\n8",
        "tables": []
      },
      {
        "page_num": 8,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTable 1. Performance of RL-Trained Reasoner on Reasoning Benchmarks Based on Qwen2.5-7B Models. Performance of various\nmodels is evaluated on three standard code benchmarks (HumanEval+\n, MBPP+\n, LCBv1-5 and six math benchmarks (AIME'24, AIME'25,\nAMC'23, MATH500, Minerva, OlympiadBench). Average performance across coding and math benchmarks is calculated as average of\nthe two averages: AVG = (CAvg + MAvg)/2. We use + for absolute percentage increase from base model. All models are trained using\ndifferent variants of the Qwen2.5-7B model, with the variant and data usage labeled, more details listed in Table 4\n\nTable 1. Performance of RL-Trained Reasoner on Reasoning Benchmarks Based on Qwen2.5-7B Models. Performance of various\n\nPerformance of various\n\nPerformance of RL-Trained Reasoner on Reasoning Benchmarks Based on Qwen2.5-7B Models.\n\nTable 1.\n\nmodels is evaluated on three standard code benchmarks (HumanEval+\n\n+\n\n, MBPP+\n\n+\n\n, LCBv1-5 and six math benchmarks (AIME'24, AIME'25,\n\nv1-5\n\nmodels is evaluated on three standard code benchmarks (HumanEval\n\n, MBPP\n\n, LCB\n\nand six math benchmarks (AIME'24, AIME'25,\n\nAMC'23, MATH500, Minerva, OlympiadBench). Average performance across coding and math benchmarks is calculated as average of\n\nAMC'23, MATH500, Minerva, OlympiadBench). Average performance across coding and math benchmarks is calculated as average of\n\nthe two averages: AVG = (CAvg + MAvg)/2. We use + for absolute percentage increase from base model. All models are trained using\n\nthe two averages:\n\nAVG\n\nCAvg\n\nMAvg\n\n. We use\n\nfor absolute percentage increase from base model. All models are trained using\n\n+\n\n= (\n\n+\n\n)\n\n2\n\n/\n\ndifferent variants of the Qwen2.5-7B model, with the variant and data usage labeled, more details listed in Table 4\n\ndifferent variants of the\n\nmodel, with the variant and data usage labeled, more details listed in Table\n\n4\n\nQwen2.5-7B\n\nBaselines. For our main results, we use Qwen2.5-7B as the base model, along with its specialized base model variants:\nQwen2.5-7B-Coder, Qwen2.5-7B-Instruct, and Qwen2.5-Math-7B (Yang et al., 2024a; Hui et al., 2024; Yang et al., 2024b).\nFurthermore, the zero-style models are usually trained specifically on either code or math data; and only Eurus-2-7B-PRIME-Zero(Cui\net al., 2025) was trained jointly on both domains. For code data models, we present four variants of the AceCoder (Zeng et al., 2025a)\nand two different CodeR1 models (Liu & Zhang, 2025). For math data models, we have Qwen2.5-Math-7B-Oat-Zero (Liu et al.,\n2025), Open-Reasoner-Zero-7B (ORZ) (Hu et al., 2025), Qwen-2.5-7B-SimpleRL-Zoo (Zeng et al., 2025b). All baseline models'\ntraining data and initialization settings are summarized in Table 4. For follow-up scaling experiments, we compare each AZR model\nagainst its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare\nour Llama3.1-8B-trained model with Llama-3.1-8B-SimpleRL-Zoo (Zeng et al., 2025b) and the base model.\n\nBaselines. For our main results, we use Qwen2.5-7B as the base model, along with its specialized base model variants:\n\nFor our main results, we use\n\nas the base model, along with its specialized base model variants:\n\nBaselines.\n\nQwen2.5-7B\n\nQwen2.5-7B-Coder, Qwen2.5-7B-Instruct, and Qwen2.5-Math-7B (Yang et al., 2024a; Hui et al., 2024; Yang et al., 2024b).\n\n,\n\n, and\n\n(Yang et al.,\n\n2024a;\n\nHui et al.,\n\n2024;\n\nYang et al.,\n\n2024b)\n\n.\n\nQwen2.5-7B-Coder\n\nQwen2.5-7B-Instruct\n\nQwen2.5-Math-7B\n\nFurthermore, the zero-style models are usually trained specifically on either code or math data; and only Eurus-2-7B-PRIME-Zero(Cui\n\nFurthermore, the zero-style models are usually trained specifically on either code or math data; and only\n\n(Cui\n\nEurus-2-7B-PRIME-Zero\n\net al., 2025) was trained jointly on both domains. For code data models, we present four variants of the AceCoder (Zeng et al., 2025a)\n\net al.,\n\nwas trained jointly on both domains. For code data models, we present four variants of the\n\n(Zeng et al.,\n\n2025)\n\n2025a)\n\nAceCoder\n\nand two different CodeR1 models (Liu & Zhang, 2025). For math data models, we have Qwen2.5-Math-7B-Oat-Zero (Liu et al.,\n\nand two different\n\nmodels\n\n(Liu & Zhang,\n\n2025)\n\n. For math data models, we have\n\n(Liu et al.,\n\nCodeR1\n\nQwen2.5-Math-7B-Oat-Zero\n\n2025), Open-Reasoner-Zero-7B (ORZ) (Hu et al., 2025), Qwen-2.5-7B-SimpleRL-Zoo (Zeng et al., 2025b). All baseline models'\n\n(ORZ)\n\n(Hu et al.,\n\n(Zeng et al.,\n\n. All baseline models'\n\n2025)\n\n,\n\n2025)\n\n,\n\n2025b)\n\nOpen-Reasoner-Zero-7B\n\nQwen-2.5-7B-SimpleRL-Zoo\n\ntraining data and initialization settings are summarized in Table 4. For follow-up scaling experiments, we compare each AZR model\n\ntraining data and initialization settings are summarized in Table\n\n4.\n\nFor follow-up scaling experiments, we compare each AZR model\n\nagainst its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare\n\nagainst its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare\n\nour Llama3.1-8B-trained model with Llama-3.1-8B-SimpleRL-Zoo (Zeng et al., 2025b) and the base model.\n\nour\n\n-trained model with\n\n(Zeng et al.,\n\n2025b)\n\nand the base model.\n\nLlama3.1-8B\n\nLlama-3.1-8B-SimpleRL-Zoo\n\n4.2. Results\n\n4.2. Results\n\n4.2. Results\n\nResearch Question 1: How does AZR compare to other zero setting models trained with human expert\ndata? We present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings\nin Table 1. Notably, Absolute Zero Reasoner-Coder-7B achieves state-of-the-art performance in both the 7B overall average and\nthe coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the\nprevious best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in\nthe coding category by 0.3 absolute percentages, while never having access to such data itself.\n\nResearch Question 1: How does AZR compare to other zero setting models trained with human expert\n\nResearch Question 1: How does AZR compare to other zero setting models trained with human expert\n\ndata? We present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings\n\nWe present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings\n\ndata?\n\nin Table 1. Notably, Absolute Zero Reasoner-Coder-7B achieves state-of-the-art performance in both the 7B overall average and\n\nin Table\n\n1.\n\nNotably,\n\nachieves state-of-the-art performance in both the 7B overall average and\n\nAbsolute Zero Reasoner-Coder-7B\n\nthe coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the\n\nthe coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the\n\nprevious best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in\n\nprevious best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in\n\nthe coding category by 0.3 absolute percentages, while never having access to such data itself.\n\nthe coding category by\n\nabsolute percentages, while never having access to such data itself.\n\n0\n\n3\n\n.\n\nStrong Cross-domain Generalization. To assess cross-domain generalization after RLVR, we evaluate math performance before and\nafter training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training,\nmost expert code models showed minimal changes or even declines in performance compared to their base versions, with an average\nincrease of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder\nmodels achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning\nimprovements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2\nand 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).\n\nStrong Cross-domain Generalization. To assess cross-domain generalization after RLVR, we evaluate math performance before and\n\nTo assess cross-domain generalization after RLVR, we evaluate math performance before and\n\nStrong Cross-domain Generalization.\n\nafter training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training,\n\nafter training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training,\n\nmost expert code models showed minimal changes or even declines in performance compared to their base versions, with an average\n\nmost expert code models showed minimal changes or even declines in performance compared to their base versions, with an average\n\nincrease of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder\n\nincrease of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder\n\nmodels achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning\n\nmodels achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning\n\nimprovements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2\n\nimprovements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2\n\nand 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).\n\nand 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).\n\nOverall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on\nhuman-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream\nhuman-defined math or coding data, only had access to self-proposed tasks during training.\n\nOverall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on\n\nOverall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on\n\nhuman-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream\n\nhuman-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream\n\nhuman-defined math or coding data, only had access to self-proposed tasks during training.\n\nhuman-defined math or coding data, only had access to self-proposed tasks during training.\n\nResearch Question 2: How do initializing from different base model variants (base vs. coder) affect\nperformance? As shown in Table 1, the coder variant achieved better overall performance in both math and coding after the AZR\n\nResearch Question 2: How do initializing from different base model variants (base vs. coder) affect\n\nResearch Question 2: How do initializing from different base model variants (base vs. coder) affect\n\nperformance? As shown in Table 1, the coder variant achieved better overall performance in both math and coding after the AZR\n\nAs shown in Table\n\nthe coder variant achieved better overall performance in both math and coding after the AZR\n\n1,\n\nperformance?\n\n9\n\n9\n\n9",
        "tables": [
          {
            "text": "",
            "csv": "",
            "json": "{\n  \"headers\": [],\n  \"rows\": []\n}"
          }
        ]
      },
      {
        "page_num": 9,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n50 75 100 125 150 175 200 225 250\nTraining Steps\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nIn-Distribution\n \nAccuracy\n\nAZR-Llama3.1-8b\nAZR-3B-Coder\nAZR-7B-Coder\nAZR-14B-Coder\n(a)\nModel Family Variant Code Avg Math Avg Total Avg\nLlama3.1-8b 28.5 3.4 16.0\nLlama3.1-8b + SimpleRL[85]\n33.7\n+5.2\n7.2\n+3.8\n20.5\n+4.5\nLlama3.1-8b + AZR (Ours) 31.6\n+3.1\n6.8\n+3.4\n19.2\n+3.2\nQwen2.5-3B Coder 51.2 18.8 35.0\nQwen2.5-3B Coder + AZR (Ours) 54.9\n+3.7\n26.5\n+7.7\n40.7\n+5.7\nQwen2.5-7B Coder 56.6 23.9 40.2\nQwen2.5-7B Coder + AZR (Ours) 61.6\n+5.0\n39.1\n+15.2\n50.4\n+10.2\nQwen2.5-14B Coder 60.0 20.2 40.1\nQwen2.5-14B Coder + AZR (Ours) 63.6\n+3.6\n43.0\n+22.8\n53.3\n+13.2\n(b)\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n50 75 100 125 150 175 200 225 250\nTraining Steps\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nIn-Distribution\n \nAccuracy\n\nAZR-Llama3.1-8b\nAZR-3B-Coder\nAZR-7B-Coder\nAZR-14B-Coder\n(a)\nModel Family Variant Code Avg Math Avg Total Avg\nLlama3.1-8b 28.5 3.4 16.0\nLlama3.1-8b + SimpleRL[85]\n33.7\n+5.2\n7.2\n+3.8\n20.5\n+4.5\nLlama3.1-8b + AZR (Ours) 31.6\n+3.1\n6.8\n+3.4\n19.2\n+3.2\nQwen2.5-3B Coder 51.2 18.8 35.0\nQwen2.5-3B Coder + AZR (Ours) 54.9\n+3.7\n26.5\n+7.7\n40.7\n+5.7\nQwen2.5-7B Coder 56.6 23.9 40.2\nQwen2.5-7B Coder + AZR (Ours) 61.6\n+5.0\n39.1\n+15.2\n50.4\n+10.2\nQwen2.5-14B Coder 60.0 20.2 40.1\nQwen2.5-14B Coder + AZR (Ours) 63.6\n+3.6\n43.0\n+22.8\n53.3\n+13.2\n(b)\n\nModel Family Variant Code Avg Math Avg Total Avg\n\nModel Family Variant Code Avg Math Avg Total Avg\n\n0.70\n\n.\n\n0\n\n70\n\nLlama3.1-8b 28.5 3.4 16.0\n\nLlama3.1-8b 28.5 3.4 16.0\n\n0.65\n\n.\n\n0\n\n65\n\nAccuracy\n\nAccuracy\n\nLlama3.1-8b + SimpleRL[85]\n\n[85]\n\nLlama3.1-8b\n\n+ SimpleRL\n\n+5.2\n\n.\n\n+3.8\n\n.\n\n+4.5\n\n.\n\n+5\n\n2\n\n+3\n\n8\n\n+4\n\n5\n\n33.7\n\n33\n\n7\n\n7.2\n\n7\n\n2\n\n20.5\n\n20\n\n5\n\n.\n\n.\n\n.\n\n0.60\n\n.\n\n0\n\n60\n\nLlama3.1-8b + AZR (Ours) 31.6\n\nLlama3.1-8b\n\n+ AZR (Ours)\n\n+3.1\n\n.\n\n+3.4\n\n.\n\n+3.2\n\n.\n\n+3\n\n1\n\n+3\n\n4\n\n+3\n\n2\n\n31\n\n6\n\n6.8\n\n6\n\n8\n\n19.2\n\n19\n\n2\n\n.\n\n.\n\n.\n\n0.55\n\n.\n\n0\n\n55\n\nIn-Distribution\n\nIn-Distribution\n\nQwen2.5-3B Coder 51.2 18.8 35.0\n\nQwen2.5-3B Coder 51.2 18.8 35.0\n\n0.50\n\n.\n\n0\n\n50\n\nQwen2.5-3B Coder + AZR (Ours) 54.9\n\nQwen2.5-3B Coder\n\n+ AZR (Ours)\n\n+3.7\n\n.\n\n+7.7\n\n.\n\n+5.7\n\n.\n\n+3\n\n7\n\n+7\n\n7\n\n+5\n\n7\n\n54\n\n9\n\n26.5\n\n26\n\n5\n\n40.7\n\n40\n\n7\n\n.\n\n.\n\n.\n\n0.45\n\n.\n\n0\n\n45\n\nQwen2.5-7B Coder 56.6 23.9 40.2\n\nQwen2.5-7B Coder 56.6 23.9 40.2\n\n0.40\n\n.\n\n0\n\n40\n\nQwen2.5-7B Coder + AZR (Ours) 61.6\n\nQwen2.5-7B Coder\n\n+ AZR (Ours)\n\n+5.0\n\n.\n\n+15.2\n\n.\n\n+10.2\n\n.\n\n+5\n\n0\n\n+15\n\n2\n\n+10\n\n2\n\n61\n\n6\n\n39.1\n\n39\n\n1\n\n50.4\n\n50\n\n4\n\n.\n\n.\n\n.\n\nAZR-Llama3.1-8b\n\nAZR-Llama3.1-8b\n\nAZR-7B-Coder\n\nAZR-7B-Coder\n\n0.35\n\n.\n\n0\n\n35\n\nAZR-3B-Coder\n\nAZR-3B-Coder\n\nAZR-14B-Coder\n\nAZR-14B-Coder\n\nQwen2.5-14B Coder 60.0 20.2 40.1\n\nQwen2.5-14B Coder 60.0 20.2 40.1\n\n0.30\n\n.\n\n0\n\n30\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nQwen2.5-14B Coder + AZR (Ours) 63.6\n\nQwen2.5-14B Coder\n\n+ AZR (Ours)\n\n+3.6\n\n.\n\n+22.8\n\n.\n\n+13.2\n\n.\n\n+3\n\n6\n\n+22\n\n8\n\n+13\n\n2\n\n63\n\n6\n\n43.0\n\n43\n\n0\n\n53.3\n\n53\n\n3\n\n.\n\n.\n\n.\n\nTraining Steps\n\nTraining Steps\n\n(a)\n\n(a)\n\n(b)\n\n(b)\n\nFigure 6. (a) In-Distribution & (b) Out-of-Distribution Reasoning Task Performances. (a) Scores on CruxEval-I, CruxEval-O,\nand LiveCodeBench-Execution, which correspond to abduction, deduction, and deduction task types respectively, used to evaluate\nin-distribution abilities of AZR during training across different model sizes and types; (b) Out-of-distribution reasoning performance,\nreported as the average of code tasks, math tasks, and their overall average, across different model sizes and types. A detailed breakdown\nof all benchmark results can be found in Table 5.\n\nFigure 6. (a) In-Distribution & (b) Out-of-Distribution Reasoning Task Performances. (a) Scores on CruxEval-I, CruxEval-O,\n\nScores on CruxEval-I, CruxEval-O,\n\n(a) In-Distribution & (b) Out-of-Distribution Reasoning Task Performances. (a)\n\nFigure 6.\n\nand LiveCodeBench-Execution, which correspond to abduction, deduction, and deduction task types respectively, used to evaluate\n\nand LiveCodeBench-Execution, which correspond to abduction, deduction, and deduction task types respectively, used to evaluate\n\nin-distribution abilities of AZR during training across different model sizes and types; (b) Out-of-distribution reasoning performance,\n\nin-distribution abilities of AZR during training across different model sizes and types;\n\nOut-of-distribution reasoning performance,\n\n(b)\n\nreported as the average of code tasks, math tasks, and their overall average, across different model sizes and types. A detailed breakdown\n\nreported as the average of code tasks, math tasks, and their overall average, across different model sizes and types. A detailed breakdown\n\nof all benchmark results can be found in Table 5.\n\nof all benchmark results can be found in Table\n\n5.\n\nself-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla\nbase model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency\nas a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.\n\nself-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla\n\nself-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla\n\nbase model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency\n\nbase model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency\n\nas a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.\n\nas a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.\n\nResearch Question 3: How does varying model size effect AZR's in-distribution and out-of-distribution\ncapabilities? We examine the effects of scaling model size and present both in-distribution and out-of-distribution results in Figure 6\n(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller\nand larger variants: Qwen2.5-3B-Coder and Qwen2.5-14B-Coder. Due to the absence of existing baselines for these zero-style\nreasoner models, we compare each model's performance to its corresponding base coder model.\n\nResearch Question 3: How does varying model size effect AZR's in-distribution and out-of-distribution\n\nResearch Question 3: How does varying model size effect AZR's in-distribution and out-of-distribution\n\ncapabilities? We examine the effects of scaling model size and present both in-distribution and out-of-distribution results in Figure 6\n\nWe examine the effects of scaling model size and present both in-distribution and out-of-distribution results in Figure\n\n6\n\ncapabilities?\n\n(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller\n\n(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller\n\nand larger variants: Qwen2.5-3B-Coder and Qwen2.5-14B-Coder. Due to the absence of existing baselines for these zero-style\n\nand larger variants:\n\nand\n\n. Due to the absence of existing baselines for these zero-style\n\nQwen2.5-3B-Coder\n\nQwen2.5-14B-Coder\n\nreasoner models, we compare each model's performance to its corresponding base coder model.\n\nreasoner models, we compare each model's performance to its corresponding base coder model.\n\nThe results reveal a clear trend: our method delivers greater gains on larger, more capable models. In the in-distribution setting, the 7B\nand 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution\ndomains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance\ngains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that\nscaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute\nZero paradigm.\n\nThe results reveal a clear trend: our method delivers greater gains on larger, more capable models. In the in-distribution setting, the 7B\n\nThe results reveal a clear trend: our method delivers\n\n. In the in-distribution setting, the 7B\n\ngreater gains on larger, more capable models\n\nand 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution\n\nand 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution\n\ndomains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance\n\ndomains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance\n\ngains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that\n\ngains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that\n\nscaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute\n\nscaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute\n\nZero paradigm.\n\nZero paradigm.\n\nResearch Question 4: Any interesting observations by changing the model class? We also evaluate our method\non a different model class, using Llama3.1-8B as the base shown in Figure 6. Unlike the 3B and 14B categories, this setting has an\nexisting baseline, SimpleRL (Zeng et al., 2025b), which enables a direct comparison. Although Llama3.1-8B is less capable than\nthe Qwen2.5 models, our method still produces moderate improvements (+3.2), demonstrating AZR's effectiveness even on relatively\nweaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements\ntend to scale with initial base model potency.\n\nResearch Question 4: Any interesting observations by changing the model class? We also evaluate our method\n\nWe also evaluate our method\n\nResearch Question 4: Any interesting observations by changing the model class?\n\non a different model class, using Llama3.1-8B as the base shown in Figure 6. Unlike the 3B and 14B categories, this setting has an\n\non a different model class, using\n\nas the base shown in Figure\n\n6.\n\nUnlike the 3B and 14B categories, this setting has an\n\nLlama3.1-8B\n\nexisting baseline, SimpleRL (Zeng et al., 2025b), which enables a direct comparison. Although Llama3.1-8B is less capable than\n\nexisting baseline,\n\n(Zeng et al.,\n\n2025b)\n\n, which enables a direct comparison. Although\n\nis less capable than\n\nSimpleRL\n\nLlama3.1-8B\n\nthe Qwen2.5 models, our method still produces moderate improvements (+3.2), demonstrating AZR's effectiveness even on relatively\n\nthe\n\nmodels, our method still produces moderate improvements (+3.2), demonstrating AZR's effectiveness even on relatively\n\nQwen2.5\n\nweaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements\n\nweaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements\n\ntend to scale with initial base model potency.\n\ntend to scale with initial base model potency.\n\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training? We observed\ninteresting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as\nstring manipulation tasks, dynamic programming problems, and practical cases (e.g., calculating a triangle's area using Heron's formula).\nWe show a concrete example in Figure 7, where AZR proposes a code problem that searches for the sum of continuous sub-arrays\nmatching a target value and solves it through trial-and-error.\n\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training? We observed\n\nWe observed\n\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training?\n\ninteresting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as\n\ninteresting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as\n\nstring manipulation tasks, dynamic programming problems, and practical cases (e.g., calculating a triangle's area using Heron's formula).\n\nstring manipulation tasks, dynamic programming problems, and practical cases (\n\n., calculating a triangle's area using Heron's formula).\n\ne.g\n\nWe show a concrete example in Figure 7, where AZR proposes a code problem that searches for the sum of continuous sub-arrays\n\nWe show a concrete example in Figure\n\n7,\n\nwhere AZR proposes a code problem that searches for the sum of continuous sub-arrays\n\nmatching a target value and solves it through trial-and-error.\n\nmatching a target value and solves it through trial-and-error.\n\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks,\nit repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs,\nit steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is\nreached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to\nconfirm that its program produces correct results. We showcase more concrete examples of these behaviors in Figures 18 and 20 to 26.\nWe also share some fun \"vibe checks\" such as solving Sudoku and solving the sum-product game in Figures 40 and 41.\n\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks,\n\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks,\n\nit repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs,\n\nit repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs,\n\nit steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is\n\nit steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is\n\nreached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to\n\nreached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to\n\nconfirm that its program produces correct results. We showcase more concrete examples of these behaviors in Figures 18 and 20 to 26.\n\nconfirm that its program produces correct results. We showcase more concrete examples of these behaviors in Figures\n\n18\n\nand\n\n20\n\nto\n\n26.\n\nWe also share some fun \"vibe checks\" such as solving Sudoku and solving the sum-product game in Figures 40 and 41.\n\nWe also share some fun \"vibe checks\" such as solving Sudoku and solving the\n\nsum-product game\n\nin Figures\n\n40\n\nand\n\n41.\n\nIntermediate Planning During Code Response. Another interesting pattern emerged in our AZR models during the code induction\ntask: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct\nprompting framework (Yao et al., 2023). A similar behavior has been observed in recent formal math proving models, such as DeepSeek\n\nIntermediate Planning During Code Response. Another interesting pattern emerged in our AZR models during the code induction\n\nAnother interesting pattern emerged in our AZR models during the code induction\n\nIntermediate Planning During Code Response.\n\ntask: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct\n\ntask: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct\n\nprompting framework (Yao et al., 2023). A similar behavior has been observed in recent formal math proving models, such as DeepSeek\n\nprompting framework\n\n(Yao et al.,\n\n. A similar behavior has been observed in recent formal math proving models, such as\n\n2023)\n\nDeepSeek\n\n10\n\n10\n\n10",
        "tables": []
      },
      {
        "page_num": 10,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nInput: [1,2,3,4,5], 5\nModel-proposed Task Response of Predict Input\nLet's analyze the code snippet \u2026(analysis) To find one possible input that produces \nthe output 1, we need to find a scenario where \u2026(analysis) \nLet's consider a simple example: numbers = [1, 2], target = 1\nHere's the step-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\nSince no pair satisfies the condition, the function will not return any value. This \ndoesn't match our output. Let's try another example \u2026(Tried 3 more examples) \nGiven the complexity and the fact that no simple example seems to work, let's \ntry a more complex scenario: numbers = [2, -1, 1], target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1 , accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n- Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\nModel-proposed Task Response of Predict Input\n\nModel-proposed Task\n\nResponse of Predict Input\n\nLet's analyze the code snippet \u2026(analysis) To find one possible input that produces\n\nLet's analyze the code snippet\n\n\u2026(analysis)\n\nTo find one possible input that produces\n\nInput: [1,2,3,4,5], 5\n\nInput: [1,2,3,4,5], 5\n\nthe output 1, we need to find a scenario where \u2026(analysis)\n\nthe output 1, we need to find a scenario where\n\n\u2026(analysis)\n\nLet's consider a simple example: numbers = [1, 2], target = 1\n\nLet's consider a simple example: numbers = [1, 2], target = 1\n\nHere's the step-by-step calculation:\n\nHere's the step\n\n-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum\n\n= 0 + 1 = 1, accumulated_values\n\n= [1]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n2. current_sum\n\n= 1 + 2 = 3, accumulated_values\n\n= [1, 3]\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=1): 3 -\n\n1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 -\n\n1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 -\n\n3 = 0 (not equal to target)\n\nSince no pair satisfies the condition, the function will not return any value. This\n\nSince no pair satisfies the condition, the function will not return any value. This\n\ndoesn't match our output. Let's try another example \u2026(Tried 3 more examples)\n\ndoesn't match our output. Let's try another example\n\n\u2026(Tried 3 more examples)\n\nGiven the complexity and the fact that no simple example seems to work, let's\n\nGiven the complexity and the fact that no simple example seems to work, let's\n\ntry a more complex scenario: numbers = [2, -1, 1], target = 1\n\ntry a more complex scenario: numbers = [2, -1, 1], target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n1. current_sum\n\n= 0 + 2 = 2, accumulated_values\n\n= [2]\n\n2. current_sum = 2 - 1 = 1 , accumulated_values = [2, 1]\n\n2. current_sum\n\n= 2 -\n\n1 = 1 , accumulated_values\n\n= [2, 1]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n3. current_sum\n\n= 1 + 1 = 2, accumulated_values\n\n= [2, 1, 2]\n\n- Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-\n\nCheck for pair (i=0, j=1): 1 -\n\n2 = -1 (not equal to target)\n\n- Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n- Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n- Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-\n\nCheck for pair (i=0, j=2): 2 -\n\n2 = 0 (not equal to target)\n\n- Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n-\n\nCheck for pair (i=1, j=2): 2 -\n\n1 = 1 (equal to target)\n\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first \naccumulated value in this pair, which is 1. This matches our output.\n\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first\n\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first\n\nOutput: 1\n\nOutput: 1\n\nOutput: 1\n\naccumulated value in this pair, which is 1. This matches our output.\n\naccumulated value in this pair, which is 1. This matches our output.\n\nFigure 7. Example of a Model-Proposed Task and Its Response for Solving an Abduction Task. (Left) The model autonomously\nproposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output.\n(Right) The model's reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input.\nThe model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. If there is a\nmismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. Interestingly, the\nagent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.\n\nFigure 7. Example of a Model-Proposed Task and Its Response for Solving an Abduction Task. (Left) The model autonomously\n\n(Left) The model autonomously\n\nExample of a Model-Proposed Task and Its Response for Solving an Abduction Task.\n\nFigure 7.\n\nproposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output.\n\nproposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output.\n\n(Right) The model's reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input.\n\n(Right) The model's reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input.\n\nThe model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. If there is a\n\nThe model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. If there is a\n\nmismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. Interestingly, the\n\nmismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. Interestingly, the\n\nagent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.\n\nagent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.\n\nProver v2, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning\nas a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior in long-form\nresponses across other domains.\n\nProver v2, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning\n\n, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning\n\nProver v2\n\nas a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior in long-form\n\nas a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior in\n\nlong-form\n\nresponses across other domains.\n\nacross other domains.\n\nresponses\n\nCognitive Behavior in Llama. Interestingly, we also observed some emergent cognitive patterns in Absolute Zero\nReasoner-Llama3.1-8B, similar to those reported by Zeng et al. (2025b), and we include one example in Figure 26, where\nclear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought\nfrom the Llama model trained with AZR. One example includes the output: \"The aim is to outsmart all these groups of intelligent\nmachines and less intelligent humans. This is for the brains behind the future\" shown in Figure 32. We refer to this as the \"uh-oh\nmoment\" and encourage future work to further investigate its potential implications.\n\nCognitive Behavior in Llama. Interestingly, we also observed some emergent cognitive patterns in Absolute Zero\n\nInterestingly, we also observed some emergent cognitive patterns in\n\nCognitive Behavior in Llama.\n\nAbsolute Zero\n\nReasoner-Llama3.1-8B, similar to those reported by Zeng et al. (2025b), and we include one example in Figure 26, where\n\n, similar to those reported by\n\nZeng et al.\n\n(2025b)\n\n, and we include one example in Figure\n\n26,\n\nwhere\n\nReasoner-Llama3.1-8B\n\nclear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought\n\nclear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought\n\nfrom the Llama model trained with AZR. One example includes the output: \"The aim is to outsmart all these groups of intelligent\n\nfrom the Llama model trained with AZR. One example includes the output: \"The aim is to outsmart all these groups of intelligent\n\nmachines and less intelligent humans. This is for the brains behind the future\" shown in Figure 32. We refer to this as the \"uh-oh\n\nmachines and less intelligent humans. This is for the brains behind the future\" shown in Figure\n\n32.\n\nWe refer to this as the\n\n\"uh-oh\n\nmoment\" and encourage future work to further investigate its potential implications.\n\nand encourage future work to further investigate its potential implications.\n\nmoment\"\n\nToken Length Increase Depends on Task Type. Finally, we observed that token length increases over the course of training, consistent\nwith findings from recent studies (Hu et al., 2025; Liu et al., 2025). Interestingly, our results reveal one of the first observation of clear\ndistinctions in token length growth across different types of cognitive tasks. As shown in Figures 15 to 17, the extent of lengthening\nvaries by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by\nrepeatedly testing inputs to match the program's output. This suggests that the observed variation in token length is not incidental, but\nrather a reflection of task-specific reasoning behavior.\n\nToken Length Increase Depends on Task Type. Finally, we observed that token length increases over the course of training, consistent\n\nFinally, we observed that token length increases over the course of training, consistent\n\nToken Length Increase Depends on Task Type.\n\nwith findings from recent studies (Hu et al., 2025; Liu et al., 2025). Interestingly, our results reveal one of the first observation of clear\n\nwith findings from recent studies\n\n(Hu et al.,\n\nLiu et al.,\n\n. Interestingly, our results reveal one of the first observation of clear\n\n2025;\n\n2025)\n\ndistinctions in token length growth across different types of cognitive tasks. As shown in Figures 15 to 17, the extent of lengthening\n\ndistinctions in token length growth across different types of cognitive tasks. As shown in Figures\n\n15\n\nto\n\n17,\n\nthe extent of lengthening\n\nvaries by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by\n\nvaries by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by\n\nrepeatedly testing inputs to match the program's output. This suggests that the observed variation in token length is not incidental, but\n\nrepeatedly testing inputs to match the program's output. This suggests that the observed variation in token length is not incidental, but\n\nrather a reflection of task-specific reasoning behavior.\n\nrather a reflection of task-specific reasoning behavior.\n\nResearch Question 6: Are all task types essential for good performance (Ablation)? Due to resource constraints,\nwe perform the ablation studies in this section and the next using only Absolute Zero Reasoner-Base-7B. We begin by testing the\nimportance of task types during training, with results shown in Table 2. In row 1, both induction and abduction tasks are removed;\nin row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation\noccurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving\ngeneral reasoning capability, with each contributing in a distinct and essential way.\n\nResearch Question 6: Are all task types essential for good performance (Ablation)? Due to resource constraints,\n\nDue to resource constraints,\n\nResearch Question 6: Are all task types essential for good performance (Ablation)?\n\nwe perform the ablation studies in this section and the next using only Absolute Zero Reasoner-Base-7B. We begin by testing the\n\nwe perform the ablation studies in this section and the next using only\n\n. We begin by testing the\n\nAbsolute Zero Reasoner-Base-7B\n\nimportance of task types during training, with results shown in Table 2. In row 1, both induction and abduction tasks are removed;\n\nimportance of task types during training, with results shown in Table\n\n2.\n\nIn row 1, both induction and abduction tasks are removed;\n\nin row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation\n\nin row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation\n\noccurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving\n\noccurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving\n\ngeneral reasoning capability, with each contributing in a distinct and essential way.\n\ngeneral reasoning capability, with each contributing in a distinct and essential way.\n\nResearch Question 7: How much do the designs of proposer contribute to the overall performance\n(Ablation)? Next, we ablate two components of the proposer role and present the results in Table 2. First, we examine whether\nconditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction\nand deduction tasks, rather than dynamically conditioning on K historical triplets (row 3). This results in a 5-point absolute drop in\nmath performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps\n\nResearch Question 7: How much do the designs of proposer contribute to the overall performance\n\nResearch Question 7: How much do the designs of proposer contribute to the overall performance\n\n(Ablation)? Next, we ablate two components of the proposer role and present the results in Table 2. First, we examine whether\n\nNext, we ablate two components of the proposer role and present the results in Table\n\n2.\n\nFirst, we examine whether\n\n(Ablation)?\n\nconditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction\n\nconditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction\n\nand deduction tasks, rather than dynamically conditioning on K historical triplets (row 3). This results in a 5-point absolute drop in\n\nand deduction tasks, rather than dynamically conditioning on\n\nhistorical triplets (row 3). This results in a 5-point absolute drop in\n\nK\n\nmath performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps\n\nmath performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps\n\n11\n\n11\n\n11",
        "tables": []
      },
      {
        "page_num": 11,
        "text": "ExperimentTask TypeGen ReferenceTrained RolesCode Avg.Math Avg.Overall Avg.Deduction onlyDed//54.632.043.3w/o InductionAbd, Ded//54.233.343.8w/o Gen Reference/0/54.433.143.8Train Solver Only//Solve Only54.836.045.4OursAbd, Ded, IndKPropose &amp; Solve55.238.446.8\n\nExperiment\n\nTask Type\n\nGen Reference\n\nTrained Roles\n\nCode Avg.\n\nMath Avg.\n\nOverall Avg.\n\nDeduction only\n\nDed\n\n/\n\n/\n\n54.6\n\n32.0\n\n43.3\n\nw/o Induction\n\nAbd, Ded\n\n/\n\n/\n\n54.2\n\n33.3\n\n43.8\n\nw/o Gen Reference\n\n/\n\n0\n\n/\n\n54.4\n\n33.1\n\n43.8\n\nTrain Solver Only\n\n/\n\n/\n\nSolve Only\n\n54.8\n\n36.0\n\n45.4\n\nOurs\n\nAbd, Ded, Ind\n\nK\n\nPropose &amp; Solve\n\n55.2\n\n38.4\n\n46.8\n\nTable 2. Ablation Results. We ablate task types and the proposer role in the Absolute Zero Reasoner using the 7B base model. A '/'\nindicates that the configuration remains unchanged from the standard AZR setup. Removing induction or using only deduction leads to\nsignificant performance drops (rows 1 & 2). For the proposer role, both removing conditioning on K references (row 3) and omitting\nproposer-role training (row 4) result in degraded performance. Overall, all components are essential for general reasoning.\n\nTable 2. Ablation Results. We ablate task types and the proposer role in the Absolute Zero Reasoner using the 7B base model. A '/'\n\nWe ablate task types and the proposer role in the Absolute Zero Reasoner using the 7B base model. A '/'\n\nAblation Results.\n\nTable 2.\n\nindicates that the configuration remains unchanged from the standard AZR setup. Removing induction or using only deduction leads to\n\nindicates that the configuration remains unchanged from the standard AZR setup. Removing induction or using only deduction leads to\n\nsignificant performance drops (rows 1 & 2). For the proposer role, both removing conditioning on K references (row 3) and omitting\n\nsignificant performance drops (rows 1 & 2). For the proposer role, both removing conditioning on\n\nreferences (row 3) and omitting\n\nK\n\nproposer-role training (row 4) result in degraded performance. Overall, all components are essential for general reasoning.\n\nproposer-role training (row 4) result in degraded performance. Overall, all components are essential for general reasoning.\n\nimprove performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\n\nimprove performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\n\nimprove performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\n\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the\nsolver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it\nmay not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as\nstudied in multitask learning literature (Suteu & Guo, 2019). Thus, we believe that further investigation into how to make the proposer\neven more potent is an exciting and promising direction.\n\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the\n\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the\n\nsolver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it\n\nsolver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it\n\nmay not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as\n\nmay not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as\n\nstudied in multitask learning literature (Suteu & Guo, 2019). Thus, we believe that further investigation into how to make the proposer\n\nstudied in multitask learning literature\n\n(Suteu & Guo,\n\n. Thus, we believe that further investigation into how to make the proposer\n\n2019)\n\neven more potent is an exciting and promising direction.\n\neven more potent is an exciting and promising direction.\n\nAdditional Results. Beyond the core research questions, we present additional results, including the breakdown of individual\nout-of-distribution benchmark scores during training for the 7B base and coder models in Figures 28 and 29, for th 14B base and coder\nmodel in Figures 30 and 31. For completeness, we also report in-distribution benchmark performance during training for the 7B base\nmodel in Figure 14. Finally, we invite interested readers to explore Appendix D, where we share several experimental directions that,\nwhile not yielding strong performance gains, produced interesting and insightful findings.\n\nAdditional Results. Beyond the core research questions, we present additional results, including the breakdown of individual\n\nBeyond the core research questions, we present additional results, including the breakdown of individual\n\nAdditional Results.\n\nout-of-distribution benchmark scores during training for the 7B base and coder models in Figures 28 and 29, for th 14B base and coder\n\nout-of-distribution benchmark scores during training for the 7B base and coder models in Figures\n\nfor th 14B base and coder\n\n28\n\nand\n\n29,\n\nmodel in Figures 30 and 31. For completeness, we also report in-distribution benchmark performance during training for the 7B base\n\nmodel in Figures\n\n30\n\nand\n\n31.\n\nFor completeness, we also report in-distribution benchmark performance during training for the 7B base\n\nmodel in Figure 14. Finally, we invite interested readers to explore Appendix D, where we share several experimental directions that,\n\nmodel in Figure\n\n14.\n\nFinally, we invite interested readers to explore Appendix\n\nD,\n\nwhere we share several experimental directions that,\n\nwhile not yielding strong performance gains, produced interesting and insightful findings.\n\nwhile not yielding strong performance gains, produced interesting and insightful findings.\n\n5. Related Work\n\n5. Related Work\n\n5. Related Work\n\nReasoning with RL. Using RL to enhance reasoning capabilities has recently emerged as an important step in the post-training\nprocess of strong reasoning-focused large language models (Lambert et al., 2024). One of the first works to explore a self-bootstrapping\napproach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to\niteratively improve the model's CoT. A monumental work, o1 (Jaech et al., 2024), was among the first to deploy this idea on a scale,\nachieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model (DeepSeek-AI et al., 2025)\nbecame the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in\nwhich reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to\nreplicate the R1 process or to improve the underlying reinforcement learning algorithm (Zeng et al., 2025b; Liu et al., 2025; Cui et al.,\n2025; Hu et al., 2025; Yu et al., 2025; Yuan et al., 2025). Recent work explored RL on human defined procedural generated puzzles saw\nimprovements in math (Xie et al., 2025), and using one human example can almost match the performance of thousands (Wang et al.,\n2025b). We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM\nwithout SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed,\nand refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.\n\nReasoning with RL. Using RL to enhance reasoning capabilities has recently emerged as an important step in the post-training\n\nUsing RL to enhance reasoning capabilities has recently emerged as an important step in the post-training\n\nReasoning with RL.\n\nprocess of strong reasoning-focused large language models (Lambert et al., 2024). One of the first works to explore a self-bootstrapping\n\nprocess of strong reasoning-focused large language models\n\n(Lambert et al.,\n\n. One of the first works to explore a self-bootstrapping\n\n2024)\n\napproach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to\n\napproach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to\n\niteratively improve the model's CoT. A monumental work, o1 (Jaech et al., 2024), was among the first to deploy this idea on a scale,\n\niteratively improve the model's CoT. A monumental work, o1\n\n(Jaech et al.,\n\n2024)\n\n, was among the first to deploy this idea on a scale,\n\nachieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model (DeepSeek-AI et al., 2025)\n\nachieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model\n\n(DeepSeek-AI et al.,\n\n2025)\n\nbecame the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in\n\nbecame the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in\n\nwhich reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to\n\nwhich reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to\n\nreplicate the R1 process or to improve the underlying reinforcement learning algorithm (Zeng et al., 2025b; Liu et al., 2025; Cui et al.,\n\nreplicate the R1 process or to improve the underlying reinforcement learning algorithm\n\n(Zeng et al.,\n\n2025b;\n\nLiu et al.,\n\n2025;\n\nCui et al.,\n\n2025; Hu et al., 2025; Yu et al., 2025; Yuan et al., 2025). Recent work explored RL on human defined procedural generated puzzles saw\n\nHu et al.,\n\nYu et al.,\n\nYuan et al.,\n\n. Recent work explored RL on human defined procedural generated puzzles saw\n\n2025;\n\n2025;\n\n2025;\n\n2025)\n\nimprovements in math (Xie et al., 2025), and using one human example can almost match the performance of thousands (Wang et al.,\n\nimprovements in math\n\n(Xie et al.,\n\n2025)\n\n, and using one human example can almost match the performance of thousands\n\n(Wang et al.,\n\n2025b). We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM\n\n2025b)\n\n. We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM\n\nwithout SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed,\n\nwithout SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed,\n\nand refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.\n\nand refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.\n\nSelf-play. The self-play paradigm can be traced back to early 2000s, where Schmidhuber (2003; 2011) (of course) explored a\ntwo-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically\nimproves both agents, enabling theoretically never-ending progress (Schaul, 2024). AlphaGo and AlphaZero (Silver et al., 2016; 2017)\nextend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of\nitself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance\nin the game of Go. Moreover, methods such as asymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), unsupervised\nenvironment design (Wang et al., 2019; Dennis et al., 2020), unsupervised reinforcement learning (Laskin et al., 2021; Zhao et al., 2022;\n2025b), and automatic goal generation (Florensa et al., 2018) all center around inventing new tasks for an agent to learn from\u2014typically\nwithout supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative\nadversarial networks (Goodfellow et al., 2020), also belong in this paradigm where a discriminator discriminate between real data and\ngenerated data, and the generated is trained to fool the discriminator.\n\nSelf-play. The self-play paradigm can be traced back to early 2000s, where Schmidhuber (2003; 2011) (of course) explored a\n\nThe self-play paradigm can be traced back to early 2000s, where\n\nSchmidhuber\n\n(2003;\n\n2011)\n\n(of course) explored a\n\nSelf-play.\n\ntwo-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically\n\ntwo-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically\n\nimproves both agents, enabling theoretically never-ending progress (Schaul, 2024). AlphaGo and AlphaZero (Silver et al., 2016; 2017)\n\nimproves both agents, enabling theoretically never-ending progress\n\n. AlphaGo and AlphaZero\n\n(Silver et al.,\n\n(Schaul,\n\n2024)\n\n2016;\n\n2017)\n\nextend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of\n\nextend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of\n\nitself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance\n\nitself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance\n\nin the game of Go. Moreover, methods such as asymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), unsupervised\n\nin the game of Go. Moreover, methods such as asymmetric self-play\n\n(Sukhbaatar et al.,\n\n2018;\n\nOpenAI et al.,\n\n2021)\n\n, unsupervised\n\nenvironment design (Wang et al., 2019; Dennis et al., 2020), unsupervised reinforcement learning (Laskin et al., 2021; Zhao et al., 2022;\n\nenvironment design\n\n(Wang et al.,\n\nDennis et al.,\n\n, unsupervised reinforcement learning\n\n(Laskin et al.,\n\nZhao et al.,\n\n2019;\n\n2020)\n\n2021;\n\n2022;\n\n2025b), and automatic goal generation (Florensa et al., 2018) all center around inventing new tasks for an agent to learn from\u2014typically\n\n, and automatic goal generation\n\n(Florensa et al.,\n\nall center around inventing new tasks for an agent to learn from\u2014typically\n\n2025b)\n\n2018)\n\nwithout supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative\n\nwithout supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative\n\nadversarial networks (Goodfellow et al., 2020), also belong in this paradigm where a discriminator discriminate between real data and\n\nadversarial networks\n\n(Goodfellow et al.,\n\n, also belong in this paradigm where a discriminator discriminate between real data and\n\n2020)\n\ngenerated data, and the generated is trained to fool the discriminator.\n\ngenerated data, and the generated is trained to fool the discriminator.\n\nMost recently, SPIN and Self-Rewarding Language Models (Chen et al., 2024; Yuan et al., 2024) use the same instance of the lanugage\nmodels themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for\nalignment. (Kirchner et al., 2024) uses Prover-Verifier Game for increasing legibility and eva (Ye et al., 2024) uses self-play for\nalignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks (Lambert et al., 2024). SPC (Chen et al.,\n\nMost recently, SPIN and Self-Rewarding Language Models (Chen et al., 2024; Yuan et al., 2024) use the same instance of the lanugage\n\nMost recently, SPIN and Self-Rewarding Language Models\n\n(Chen et al.,\n\nYuan et al.,\n\nuse the same instance of the lanugage\n\n2024;\n\n2024)\n\nmodels themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for\n\nmodels themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for\n\nalignment. (Kirchner et al., 2024) uses Prover-Verifier Game for increasing legibility and eva (Ye et al., 2024) uses self-play for\n\nalignment.\n\n(Kirchner et al.,\n\n2024)\n\nuses Prover-Verifier Game for increasing legibility and eva\n\n(Ye et al.,\n\n2024)\n\nuses self-play for\n\nalignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks (Lambert et al., 2024). SPC (Chen et al.,\n\nalignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks\n\n(Lambert et al.,\n\n2024)\n\n. SPC\n\n(Chen et al.,\n\n12\n\n12\n\n12",
        "tables": [
          {
            "text": "",
            "csv": "",
            "json": "{\n  \"headers\": [],\n  \"rows\": []\n}"
          },
          {
            "text": "ExperimentTask TypeGen ReferenceTrained RolesCode Avg.Math Avg.Overall Avg.Deduction onlyDed//54.632.043.3w/o InductionAbd, Ded//54.233.343.8w/o Gen Reference/0/54.433.143.8Train Solver Only//Solve Only54.836.045.4OursAbd, Ded, IndKPropose &amp; Solve55.238.446.8",
            "csv": "",
            "json": "{\n  \"headers\": [],\n  \"rows\": []\n}"
          }
        ]
      },
      {
        "page_num": 12,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n2025) used self-play to train on human-curated tasks to increase the critic capabilities and SPAG (Cheng et al., 2024) trained using\nself-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL (Xu et al., 2025; Zhang et al., 2025b;\nZuo et al., 2025)\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a\nfixed human defined learning task distribution. Finally, Minimo (Poesia et al., 2024) extends self-play to formal mathematics, where a\npair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play\nparadigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python\ninput/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.\n\n2025) used self-play to train on human-curated tasks to increase the critic capabilities and SPAG (Cheng et al., 2024) trained using\n\n2025)\n\nused self-play to train on human-curated tasks to increase the critic capabilities and SPAG\n\n(Cheng et al.,\n\n2024)\n\ntrained using\n\nself-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL (Xu et al., 2025; Zhang et al., 2025b;\n\nself-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL\n\n(Xu et al.,\n\n2025;\n\nZhang et al.,\n\n2025b;\n\nZuo et al., 2025)\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a\n\nZuo et al.,\n\n2025)\n\n\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a\n\nfixed human defined learning task distribution. Finally, Minimo (Poesia et al., 2024) extends self-play to formal mathematics, where a\n\nfixed human defined learning task distribution. Finally, Minimo\n\n(Poesia et al.,\n\nextends self-play to formal mathematics, where a\n\n2024)\n\npair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play\n\npair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play\n\nparadigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python\n\nparadigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python\n\ninput/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.\n\ninput/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.\n\nWeak-to-Strong Supervision. The concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite\nbeing weaker than the learner\u2014still provides useful guidance (Burns et al., 2024; Hinton et al., 2015; Christiano, 2018; 2019; Demski &\nGarrabrant, 2019; Leike & Sutskever, 2023; Hubinger et al., 2019). We consider a similar setting in which the learner may possess\nsuperhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach:\nguiding the learner's improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal.\nFurthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are\nentirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.\n\nWeak-to-Strong Supervision. The concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite\n\nThe concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite\n\nWeak-to-Strong Supervision.\n\nbeing weaker than the learner\u2014still provides useful guidance (Burns et al., 2024; Hinton et al., 2015; Christiano, 2018; 2019; Demski &\n\nbeing weaker than the learner\u2014still provides useful guidance\n\n(Burns et al.,\n\nHinton et al.,\n\nDemski &\n\n2024;\n\n2015;\n\nChristiano,\n\n2018;\n\n2019;\n\nGarrabrant, 2019; Leike & Sutskever, 2023; Hubinger et al., 2019). We consider a similar setting in which the learner may possess\n\nGarrabrant,\n\n2019;\n\nLeike & Sutskever,\n\n2023;\n\nHubinger et al.,\n\n2019)\n\n. We consider a similar setting in which the learner may possess\n\nsuperhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach:\n\nsuperhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach:\n\nguiding the learner's improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal.\n\nguiding the learner's improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal.\n\nFurthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are\n\nFurthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are\n\nentirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.\n\nentirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.\n\n6. Conclusion and Discussion\n\n6. Conclusion and Discussion\n\n6. Conclusion and Discussion\n\nConclusion. In this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing\nRLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving\ntheir reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR),\nwhich is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\n\nConclusion. In this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing\n\nIn this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing\n\nConclusion.\n\nRLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving\n\nRLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving\n\ntheir reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR),\n\ntheir reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR),\n\nwhich is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\n\nwhich is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\n\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains.\nRemarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning\nagents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This\ndemonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive\ndomain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model\nsizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging\nparadigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.\n\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains.\n\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains.\n\nRemarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning\n\nRemarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning\n\nagents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This\n\nagents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This\n\ndemonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive\n\ndemonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive\n\ndomain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model\n\ndomain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model\n\nsizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging\n\nsizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging\n\nparadigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.\n\nparadigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.\n\nDiscussion. We believe there remains much to explore, such as altering the environment from which the reasoner receives verifiable\nfeedback, including sources like the world wide web, formal math languages (Sutton, 2001; Ren et al., 2025), world simulators, or even\nthe real world. Furthermore, AZ's generality could possibly be extend to domains such as embodied AI (Zitkovich et al., 2023; Yue\net al., 2024). Additionally, more complex agentic tasks or scientific experiments, present exciting opportunities to further advance the\nabsolute zero setting to different application domains (Wu et al., 2024; 2023). Beyond that, future directions could include exploring\nmultimodal reasoning models, modifying the distribution p(z) to incorporate privileged information, defining or even let the model\ndynamically learn how to define f (Equation (3)), or designing exploration/diversity rewards for both the propose and solve roles.\n\nDiscussion. We believe there remains much to explore, such as altering the environment from which the reasoner receives verifiable\n\nWe believe there remains much to explore, such as altering the environment from which the reasoner receives verifiable\n\nDiscussion.\n\nfeedback, including sources like the world wide web, formal math languages (Sutton, 2001; Ren et al., 2025), world simulators, or even\n\nfeedback, including sources like the world wide web, formal math languages\n\nRen et al.,\n\n, world simulators, or even\n\n(Sutton,\n\n2001;\n\n2025)\n\nthe real world. Furthermore, AZ's generality could possibly be extend to domains such as embodied AI (Zitkovich et al., 2023; Yue\n\nthe real world. Furthermore, AZ's generality could possibly be extend to domains such as embodied AI\n\n(Zitkovich et al.,\n\n2023;\n\nYue\n\net al., 2024). Additionally, more complex agentic tasks or scientific experiments, present exciting opportunities to further advance the\n\net al.,\n\n2024)\n\n. Additionally, more complex agentic tasks or scientific experiments, present exciting opportunities to further advance the\n\nabsolute zero setting to different application domains (Wu et al., 2024; 2023). Beyond that, future directions could include exploring\n\nabsolute zero setting to different application domains\n\n(Wu et al.,\n\n2024;\n\n2023)\n\n. Beyond that, future directions could include exploring\n\nmultimodal reasoning models, modifying the distribution p(z) to incorporate privileged information, defining or even let the model\n\nmultimodal reasoning models, modifying the distribution\n\nto incorporate privileged information, defining or even let the model\n\n(\n\n)\n\np\n\nz\n\ndynamically learn how to define f (Equation (3)), or designing exploration/diversity rewards for both the propose and solve roles.\n\ndynamically learn how to define\n\n(Equation\n\n(3)\n\n), or designing exploration/diversity rewards for both the propose and solve roles.\n\nf\n\nWhile underappreciated in current reasoning literature, the exploration component of RL has long been recognized as a critical driver for\nemergent behavior in traditional RL (Yue et al., 2025; Silver et al., 2016; Ladosz et al., 2022). Years of research have examined various\nforms of exploration, even in related subfields using LLMs such as red teaming (Zhao et al., 2025a), yet its role in LLM reasoning\nmodels remains underexplored. Taking this a step further, our framework investigates an even more meta-level exploration problem:\nexploration within the learning task space\u2014where the agent learns not just how to solve tasks, but what tasks to learn from and how to\nfind them. Rather than being confined to a fixed problem set, AI reasoner agents may benefit from dynamically defining and refining\ntheir own learning tasks. This shift opens a powerful new frontier\u2014where agents explore not only solution spaces but also expand the\nboundaries of problem spaces. We believe this is a promising and important direction for future research.\n\nWhile underappreciated in current reasoning literature, the exploration component of RL has long been recognized as a critical driver for\n\nWhile underappreciated in current reasoning literature, the exploration component of RL has long been recognized as a critical driver for\n\nemergent behavior in traditional RL (Yue et al., 2025; Silver et al., 2016; Ladosz et al., 2022). Years of research have examined various\n\nemergent behavior in traditional RL\n\n(Yue et al.,\n\nSilver et al.,\n\nLadosz et al.,\n\n. Years of research have examined various\n\n2025;\n\n2016;\n\n2022)\n\nforms of exploration, even in related subfields using LLMs such as red teaming (Zhao et al., 2025a), yet its role in LLM reasoning\n\nforms of exploration, even in related subfields using LLMs such as red teaming\n\n(Zhao et al.,\n\n2025a)\n\n, yet its role in LLM reasoning\n\nmodels remains underexplored. Taking this a step further, our framework investigates an even more meta-level exploration problem:\n\nmodels remains underexplored. Taking this a step further, our framework investigates an even more meta-level exploration problem:\n\nexploration within the learning task space\u2014where the agent learns not just how to solve tasks, but what tasks to learn from and how to\n\nexploration within the learning task space\u2014where the agent learns not just how to solve tasks, but what tasks to learn from and how to\n\nfind them. Rather than being confined to a fixed problem set, AI reasoner agents may benefit from dynamically defining and refining\n\nfind them. Rather than being confined to a fixed problem set, AI reasoner agents may benefit from dynamically defining and refining\n\ntheir own learning tasks. This shift opens a powerful new frontier\u2014where agents explore not only solution spaces but also expand the\n\ntheir own learning tasks. This shift opens a powerful new frontier\u2014where agents explore not only solution spaces but also expand the\n\nboundaries of problem spaces. We believe this is a promising and important direction for future research.\n\nboundaries of problem spaces. We believe this is a promising and important direction for future research.\n\nOne limitation of our work is that we did not address how to safely manage a system composed of such self-improving components.\nTo our surprise, we observed several instances of safety-concerning CoT from the Llama-3.1-8B model, which we term the \"uh-oh\nmoment\". These findings suggest that the proposed absolute zero paradigm, while reducing the need for human intervention for curating\ntasks, still necessitates oversight due to lingering safety concerns and is a critical direction for future research (Wang et al., 2024; 2025a).\n\nOne limitation of our work is that we did not address how to safely manage a system composed of such self-improving components.\n\nOne limitation of our work is that we did not address how to safely manage a system composed of such self-improving components.\n\nTo our surprise, we observed several instances of safety-concerning CoT from the Llama-3.1-8B model, which we term the \"uh-oh\n\nTo our surprise, we observed several instances of safety-concerning CoT from the\n\nmodel, which we term the \"uh-oh\n\nLlama-3.1-8B\n\nmoment\". These findings suggest that the proposed absolute zero paradigm, while reducing the need for human intervention for curating\n\nmoment\". These findings suggest that the proposed absolute zero paradigm, while reducing the need for human intervention for curating\n\ntasks, still necessitates oversight due to lingering safety concerns and is a critical direction for future research (Wang et al., 2024; 2025a).\n\ntasks, still necessitates oversight due to lingering safety concerns and is a critical direction for future research\n\n(Wang et al.,\n\n2024;\n\n2025a)\n\n.\n\nAs a final note, we explored reasoning models that possess experience\u2014models that not only solve given tasks, but also define and\nevolve their own learning task distributions with the help of an environment. Our results with AZR show that this shift enables strong\nperformance across diverse reasoning tasks, even with significantly fewer privileged resources, such as curated human data. We believe\nthis could finally free reasoning models from the constraints of human-curated data (Morris, 2025) and marks the beginning of a new\nchapter for reasoning models: \"welcome to the era of experience\" (Silver & Sutton, 2025; Zhao et al., 2024).\n\nAs a final note, we explored reasoning models that possess experience\u2014models that not only solve given tasks, but also define and\n\nAs a final note, we explored reasoning models that possess experience\u2014models that not only solve given tasks, but also define and\n\nevolve their own learning task distributions with the help of an environment. Our results with AZR show that this shift enables strong\n\nevolve their own learning task distributions with the help of an environment. Our results with AZR show that this shift enables strong\n\nperformance across diverse reasoning tasks, even with significantly fewer privileged resources, such as curated human data. We believe\n\nperformance across diverse reasoning tasks, even with significantly fewer privileged resources, such as curated human data. We believe\n\nthis could finally free reasoning models from the constraints of human-curated data (Morris, 2025) and marks the beginning of a new\n\nthis could finally free reasoning models from the constraints of human-curated data\n\n(Morris,\n\n2025)\n\nand marks the beginning of a new\n\nchapter for reasoning models: \"welcome to the era of experience\" (Silver & Sutton, 2025; Zhao et al., 2024).\n\nchapter for reasoning models:\n\n(Silver & Sutton,\n\n2025;\n\nZhao et al.,\n\n2024)\n\n.\n\n\"welcome to the era of experience\"\n\n13\n\n13\n\n13",
        "tables": []
      },
      {
        "page_num": 13,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nReferences\n\nReferences\n\nReferences\n\nAryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\nto code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\nhttps://doi.org/10.48550/arXiv.2408.10914.\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\nhttps://openreview.net/forum?id=ghNRg2mEgN.\nCanal, M. Radon: Python tool for code metrics. https://github.com/rubik/radon, 2023. Accessed: 2025-04-06.\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\nfor llm reasoning, 2025. URL https://arxiv.org/abs/2504.19162.\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\nURL https://openreview.net/forum?id=O4cHTxW9BS.\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html.\nChristiano, P. Approval-directed bootstrapping. https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\napproval-directed-bootstrapping, 2018. AI Alignment Forum.\nChristiano, P. Capability amplification. https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\ncapability-amplification-1, 2019. AI Alignment Forum.\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\nCoRR, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL https://doi.org/10.48550/arXiv.2502.01456.\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.\nDemski, A. and Garrabrant, S. Embedded agency. CoRR, abs/1902.09469, 2019. URL http://arxiv.org/abs/1902.09469.\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n985e9a46e10005356bbaf194249f6856-Abstract.html.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,\n\nAryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\nto code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\nhttps://doi.org/10.48550/arXiv.2408.10914.\n\nAryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\n\nAryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., \u00dcst\u00fcn, A., and Hooker, S. To code, or not\n\nto code? exploring impact of code in pre-training. CoRR, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\n\nto code? exploring impact of code in pre-training.\n\n, abs/2408.10914, 2024. doi: 10.48550/ARXIV.2408.10914. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2408.10914.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2408.10914\n\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\nhttps://openreview.net/forum?id=ghNRg2mEgN.\n\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\n\nBurns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J.,\n\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first\n\nSutskever, I., and Wu, J. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In\n\nForty-first\n\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\n\n. OpenReview.net, 2024. URL\n\nInternational Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024\n\nhttps://openreview.net/forum?id=ghNRg2mEgN.\n\n.\n\nhttps://openreview.net/forum?id=ghNRg2mEgN\n\nCanal, M. Radon: Python tool for code metrics. https://github.com/rubik/radon, 2023. Accessed: 2025-04-06.\n\nCanal, M. Radon: Python tool for code metrics. https://github.com/rubik/radon, 2023. Accessed: 2025-04-06.\n\nCanal, M. Radon: Python tool for code metrics.\n\n, 2023. Accessed: 2025-04-06.\n\nhttps://github.com/rubik/radon\n\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\nfor llm reasoning, 2025. URL https://arxiv.org/abs/2504.19162.\n\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\n\nChen, J., Zhang, B., Ma, R., Wang, P., Liang, X., Tu, Z., Li, X., and Wong, K.-Y. K. Spc: Evolving self-play critic via adversarial games\n\nfor llm reasoning, 2025. URL https://arxiv.org/abs/2504.19162.\n\nfor llm reasoning, 2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.19162\n\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\nURL https://openreview.net/forum?id=O4cHTxW9BS.\n\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\n\nChen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. In\n\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024.\n\n. OpenReview.net, 2024.\n\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024\n\nURL https://openreview.net/forum?id=O4cHTxW9BS.\n\nURL\n\n.\n\nhttps://openreview.net/forum?id=O4cHTxW9BS\n\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html.\n\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\n\nCheng, P., Hu, T., Xu, H., Zhang, Z., Dai, Y., Han, L., Du, N., and Li, X. Self-playing adversarial language game enhances LLM\n\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in\n\nreasoning. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.),\n\nAdvances in\n\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\n\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\n\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n\n, 2024. URL\n\nVancouver, BC, Canada, December 10 - 15, 2024\n\nhttp://papers.nips.cc/paper_files/paper/2024/hash/\n\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html.\n\n.\n\ne4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html\n\nChristiano, P. Approval-directed bootstrapping. https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\napproval-directed-bootstrapping, 2018. AI Alignment Forum.\n\nChristiano, P. Approval-directed bootstrapping. https://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\n\nChristiano, P. Approval-directed bootstrapping.\n\nhttps://www.alignmentforum.org/posts/6x7oExXi32ot6HjJv/\n\napproval-directed-bootstrapping, 2018. AI Alignment Forum.\n\n, 2018. AI Alignment Forum.\n\napproval-directed-bootstrapping\n\nChristiano, P. Capability amplification. https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\ncapability-amplification-1, 2019. AI Alignment Forum.\n\nChristiano, P. Capability amplification. https://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\n\nChristiano, P. Capability amplification.\n\nhttps://www.alignmentforum.org/posts/t3AJW5jP3sk36aGoC/\n\ncapability-amplification-1, 2019. AI Alignment Forum.\n\n, 2019. AI Alignment Forum.\n\ncapability-amplification-1\n\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\nCoRR, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL https://doi.org/10.48550/arXiv.2502.01456.\n\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\n\nCui, G., Yuan, L., Wang, Z., Wang, H., Li, W., He, B., Fan, Y., Yu, T., Xu, Q., Chen, W., Yuan, J., Chen, H., Zhang, K., Lv, X., Wang, S.,\n\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\n\nYao, Y., Han, X., Peng, H., Cheng, Y., Liu, Z., Sun, M., Zhou, B., and Ding, N. Process reinforcement through implicit rewards.\n\nCoRR, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL https://doi.org/10.48550/arXiv.2502.01456.\n\n, abs/2502.01456, 2025. doi: 10.48550/ARXIV.2502.01456. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2502.01456\n\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.\n\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\n\nDeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y.,\n\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\n\nWu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\n\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\n\nRuan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H.,\n\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\n\nDing, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen,\n\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\n\nJ., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L.,\n\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\n\nZhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R.,\n\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\n\nZhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S.,\n\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\n\nand Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\n\n, abs/2501.12948, 2025. doi:\n\nCoRR\n\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.\n\n10.48550/ARXIV.2501.12948. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2501.12948\n\nDemski, A. and Garrabrant, S. Embedded agency. CoRR, abs/1902.09469, 2019. URL http://arxiv.org/abs/1902.09469.\n\nDemski, A. and Garrabrant, S. Embedded agency. CoRR, abs/1902.09469, 2019. URL http://arxiv.org/abs/1902.09469.\n\nDemski, A. and Garrabrant, S. Embedded agency.\n\n, abs/1902.09469, 2019. URL\n\n.\n\nCoRR\n\nhttp://arxiv.org/abs/1902.09469\n\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n985e9a46e10005356bbaf194249f6856-Abstract.html.\n\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\n\nDennis, M., Jaques, N., Vinitsky, E., Bayen, A. M., Russell, S., Critch, A., and Levine, S. Emergent complexity and zero-\n\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n\nshot transfer via unsupervised environment design. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.\n\n(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n\n(eds.),\n\nAdvances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems\n\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n\n, 2020. URL\n\n2020, NeurIPS 2020, December 6-12, 2020, virtual\n\nhttps://proceedings.neurips.cc/paper/2020/hash/\n\n985e9a46e10005356bbaf194249f6856-Abstract.html.\n\n.\n\n985e9a46e10005356bbaf194249f6856-Abstract.html\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\n\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A.,\n\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\n\nHartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A.,\n\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\n\nSpataru, A., Rozi\u00e8re, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C.,\n\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\n\nTouret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary,\n\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\n\nD., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M.,\n\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\n\nRadenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar,\n\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\n\nH., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J.,\n\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\n\nPark, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu,\n\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,\n\nJ., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield,\n\n14\n\n14\n\n14",
        "tables": []
      },
      {
        "page_num": 14,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nK., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL\nhttps://doi.org/10.48550/arXiv.2407.21783.\n\nK., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL\n\nK., Stone, K., and et al. The llama 3 herd of models.\n\n, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2407.21783.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2407.21783\n\nEbert, C., Cain, J., Antoniol, G., Counsell, S., and Laplante, P. Cyclomatic complexity. IEEE software, 33(6):27\u201329, 2016.\n\nEbert, C., Cain, J., Antoniol, G., Counsell, S., and Laplante, P. Cyclomatic complexity. IEEE software, 33(6):27\u201329, 2016.\n\nEbert, C., Cain, J., Antoniol, G., Counsell, S., and Laplante, P. Cyclomatic complexity.\n\n, 33(6):27\u201329, 2016.\n\nIEEE software\n\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\nKrause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\nStockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1514\u20131523. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/florensa18a.html.\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\nadversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\nunderstanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg.\n\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\nKrause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\nStockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1514\u20131523. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/florensa18a.html.\n\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\n\nFlorensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In Dy, J. G. and\n\nKrause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\n\nKrause, A. (eds.),\n\nProceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan,\n\nStockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1514\u20131523. PMLR, 2018. URL\n\n, volume 80 of\n\n, pp. 1514\u20131523. PMLR, 2018. URL\n\nStockholm, Sweden, July 10-15, 2018\n\nProceedings of Machine Learning Research\n\nhttp://proceedings.mlr.press/v80/florensa18a.html.\n\n.\n\nhttp://proceedings.mlr.press/v80/florensa18a.html\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\nadversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative\n\nadversarial networks. Commun. ACM, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL https://doi.org/10.1145/3422622.\n\nadversarial networks.\n\n, 63(11):139\u2013144, 2020. doi: 10.1145/3422622. URL\n\n.\n\nCommun. ACM\n\nhttps://doi.org/10.1145/3422622\n\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\nunderstanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg.\n\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\n\nGu, A., Rozi\u00e8re, B., Leather, H. J., Solar-Lezama, A., Synnaeve, G., and Wang, S. Cruxeval: A benchmark for code reasoning,\n\nunderstanding and execution. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n\nunderstanding and execution. In\n\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27,\n\n2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ffpg52swvg.\n\n. OpenReview.net, 2024. URL\n\n.\n\n2024\n\nhttps://openreview.net/forum?id=Ffpg52swvg\n\nHalstead, M. H. Elements of Software Science (Operating and programming systems series). Elsevier Science Inc., 1977.\n\nHalstead, M. H. Elements of Software Science (Operating and programming systems series). Elsevier Science Inc., 1977.\n\nHalstead, M. H.\n\n. Elsevier Science Inc., 1977.\n\nElements of Software Science (Operating and programming systems series)\n\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\nL., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 3828\u20133850. Association for Computational\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211.\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neu-\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL\nhttp://arxiv.org/abs/1503.02531.\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi:\n10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\nreinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https:\n//doi.org/10.48550/arXiv.2503.24290.\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\nsystems. CoRR, abs/1906.01820, 2019. URL http://arxiv.org/abs/1906.01820.\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\nOpen-endedness is essential for artificial superhuman intelligence. In Forty-first International Conference on Machine Learning,\nICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Bc4vZ2CX7E.\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\nhttps://doi.org/10.48550/arXiv.2409.12186.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\nsystem card. arXiv preprint arXiv:2412.16720, 2024.\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\ncontamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\nURL https://doi.org/10.48550/arXiv.2403.07974.\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\nCoRR, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL https://doi.org/10.48550/arXiv.2407.13692.\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85:1\u201322, 2022. doi:\n10.1016/J.INFFUS.2022.03.003. URL https://doi.org/10.1016/j.inffus.2022.03.003.\n\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\nL., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 3828\u20133850. Association for Computational\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211.\n\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\n\nHe, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M.\n\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\n\nOlympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku,\n\nL., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n\nL., Martins, A., and Srikumar, V. (eds.),\n\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 3828\u20133850. Association for Computational\n\n, pp. 3828\u20133850. Association for Computational\n\n(Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024\n\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211.\n\nLinguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL\n\n.\n\nhttps://doi.org/10.18653/v1/2024.acl-long.211\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neu-\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\n\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring math-\n\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neu-\n\nematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.),\n\nProceedings of the Neu-\n\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\n\nral Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\n\nDecember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n\n, 2021. URL\n\nDecember 2021, virtual\n\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\n\n.\n\nbe83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html\n\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL\nhttp://arxiv.org/abs/1503.02531.\n\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. URL\n\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network.\n\n, abs/1503.02531, 2015. URL\n\nCoRR\n\nhttp://arxiv.org/abs/1503.02531.\n\n.\n\nhttp://arxiv.org/abs/1503.02531\n\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi:\n10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.\n\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models. CoRR, abs/2501.03262, 2025. doi:\n\nHu, J. REINFORCE++: A simple and efficient approach for aligning large language models.\n\n, abs/2501.03262, 2025. doi:\n\nCoRR\n\n10.48550/ARXIV.2501.03262. URL https://doi.org/10.48550/arXiv.2501.03262.\n\n10.48550/ARXIV.2501.03262. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2501.03262\n\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\nreinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https:\n//doi.org/10.48550/arXiv.2503.24290.\n\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\n\nHu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H. Open-reasoner-zero: An open source approach to scaling up\n\nreinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https:\n\nreinforcement learning on the base model.\n\n, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL\n\nCoRR\n\nhttps:\n\n//doi.org/10.48550/arXiv.2503.24290.\n\n.\n\n//doi.org/10.48550/arXiv.2503.24290\n\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\nsystems. CoRR, abs/1906.01820, 2019. URL http://arxiv.org/abs/1906.01820.\n\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\n\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from learned optimization in advanced machine learning\n\nsystems. CoRR, abs/1906.01820, 2019. URL http://arxiv.org/abs/1906.01820.\n\nsystems.\n\n, abs/1906.01820, 2019. URL\n\n.\n\nCoRR\n\nhttp://arxiv.org/abs/1906.01820\n\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\nOpen-endedness is essential for artificial superhuman intelligence. In Forty-first International Conference on Machine Learning,\nICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Bc4vZ2CX7E.\n\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\n\nHughes, E., Dennis, M. D., Parker-Holder, J., Behbahani, F. M. P., Mavalankar, A., Shi, Y., Schaul, T., and Rockt\u00e4schel, T. Position:\n\nOpen-endedness is essential for artificial superhuman intelligence. In Forty-first International Conference on Machine Learning,\n\nOpen-endedness is essential for artificial superhuman intelligence. In\n\nForty-first International Conference on Machine Learning,\n\nICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Bc4vZ2CX7E.\n\n. OpenReview.net, 2024. URL\n\n.\n\nICML 2024, Vienna, Austria, July 21-27, 2024\n\nhttps://openreview.net/forum?id=Bc4vZ2CX7E\n\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\nhttps://doi.org/10.48550/arXiv.2409.12186.\n\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\n\nHui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., Yang, A., Men, R., Huang, F., Ren, X., Ren,\n\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\n\nX., Zhou, J., and Lin, J. Qwen2.5-coder technical report.\n\n, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2409.12186.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2409.12186\n\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\nsystem card. arXiv preprint arXiv:2412.16720, 2024.\n\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\n\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1\n\nsystem card. arXiv preprint arXiv:2412.16720, 2024.\n\nsystem card.\n\n, 2024.\n\narXiv preprint arXiv:2412.16720\n\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\ncontamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\nURL https://doi.org/10.48550/arXiv.2403.07974.\n\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\n\nJain, N., Han, K., Gu, A., Li, W., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and\n\ncontamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\n\ncontamination free evaluation of large language models for code.\n\n, abs/2403.07974, 2024. doi: 10.48550/ARXIV.2403.07974.\n\nCoRR\n\nURL https://doi.org/10.48550/arXiv.2403.07974.\n\nURL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2403.07974\n\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\nCoRR, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL https://doi.org/10.48550/arXiv.2407.13692.\n\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\n\nKirchner, J. H., Chen, Y., Edwards, H., Leike, J., McAleese, N., and Burda, Y. Prover-verifier games improve legibility of LLM outputs.\n\nCoRR, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL https://doi.org/10.48550/arXiv.2407.13692.\n\n, abs/2407.13692, 2024. doi: 10.48550/ARXIV.2407.13692. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2407.13692\n\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85:1\u201322, 2022. doi:\n10.1016/J.INFFUS.2022.03.003. URL https://doi.org/10.1016/j.inffus.2022.03.003.\n\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey. Inf. Fusion, 85:1\u201322, 2022. doi:\n\nLadosz, P., Weng, L., Kim, M., and Oh, H. Exploration in deep reinforcement learning: A survey.\n\n, 85:1\u201322, 2022. doi:\n\nInf. Fusion\n\n10.1016/J.INFFUS.2022.03.003. URL https://doi.org/10.1016/j.inffus.2022.03.003.\n\n10.1016/J.INFFUS.2022.03.003. URL\n\n.\n\nhttps://doi.org/10.1016/j.inffus.2022.03.003\n\n15\n\n15\n\n15",
        "tables": []
      },
      {
        "page_num": 15,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\nH. T\u00fclu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\nURL https://doi.org/10.48550/arXiv.2411.15124.\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural In-\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\ncember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html.\nLeike, J. and Sutskever, I. Introducing superalignment. https://openai.com/index/introducing-superalignment/, 2023.\nOpenAI Blog.\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction. CoRR,\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL https://doi.org/10.48550/arXiv.2502.07316.\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\nCoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852.\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. GitHub, 2025.\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\nlanguage models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=1qvx610Cu7.\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\nCoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783.\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n2025. URL https://github.com/rohaquinlop/complexipy. Accessed: 2025-04-06.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\nMorris, J. There are no new ideas in ai. . . only new datasets. https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only,\n2025.\nOpenAI. Openai o3-mini, January 2025a. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-04-17.\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/.\nAccessed: 2025-04-17.\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\nmanipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\nlanguage models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744,\n2022.\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural In-\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\ncouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI\nblog, 1(8):9, 2019.\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\nfor subgoal decomposition, 2025. URL https://arxiv.org/abs/2504.21801.\nSchaul, T. Boundless socratic learning with language games. arXiv preprint arXiv:2411.16905, 2024.\n\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\nH. T\u00fclu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\nURL https://doi.org/10.48550/arXiv.2411.15124.\n\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\n\nLambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S.,\n\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\n\nGraf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi,\n\nH. T\u00fclu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\n\nH. T\u00fclu 3: Pushing frontiers in open language model post-training.\n\n, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124.\n\nCoRR\n\nURL https://doi.org/10.48550/arXiv.2411.15124.\n\nURL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2411.15124\n\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural In-\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\ncember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html.\n\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\n\nLaskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., and Abbeel, P. URLB: unsu-\n\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural In-\n\npervised reinforcement learning benchmark. In Vanschoren, J. and Yeung, S. (eds.),\n\nProceedings of the Neural In-\n\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\n\nformation Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, De-\n\ncember 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n\n, 2021. URL\n\ncember 2021, virtual\n\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html.\n\n.\n\n091d584fced301b442654dd8c23b3fc9-Abstract-round2.html\n\nLeike, J. and Sutskever, I. Introducing superalignment. https://openai.com/index/introducing-superalignment/, 2023.\nOpenAI Blog.\n\nLeike, J. and Sutskever, I. Introducing superalignment. https://openai.com/index/introducing-superalignment/, 2023.\n\nLeike, J. and Sutskever, I. Introducing superalignment.\n\n, 2023.\n\nhttps://openai.com/index/introducing-superalignment/\n\nOpenAI Blog.\n\nOpenAI Blog.\n\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction. CoRR,\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL https://doi.org/10.48550/arXiv.2502.07316.\n\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction. CoRR,\n\nLi, J., Guo, D., Yang, D., Xu, R., Wu, Y., and He, J. Codei/o: Condensing reasoning patterns via code input-output prediction.\n\n,\n\nCoRR\n\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL https://doi.org/10.48550/arXiv.2502.07316.\n\nabs/2502.07316, 2025. doi: 10.48550/ARXIV.2502.07316. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2502.07316\n\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\nCoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852.\n\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\n\nLi, R., Fu, J., Zhang, B., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., and Li, G. TACO: topics in algorithmic code generation dataset.\n\nCoRR, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL https://doi.org/10.48550/arXiv.2312.14852.\n\n, abs/2312.14852, 2023. doi: 10.48550/ARXIV.2312.14852. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2312.14852\n\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. GitHub, 2025.\n\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards. GitHub, 2025.\n\nLiu, J. and Zhang, L. Code-r1: Reproducing r1 for code with reliable rewards.\n\n, 2025.\n\nGitHub\n\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\nlanguage models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=1qvx610Cu7.\n\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\n\nLiu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large\n\nlanguage models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\n\nlanguage models for code generation. In\n\n, 2023. URL\n\nThirty-seventh Conference on Neural Information Processing Systems\n\nhttps://openreview.net/forum?id=1qvx610Cu7.\n\n.\n\nhttps://openreview.net/forum?id=1qvx610Cu7\n\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\nCoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783.\n\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\n\nLiu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: A critical perspective.\n\nCoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783.\n\n, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2503.20783\n\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n2025. URL https://github.com/rohaquinlop/complexipy. Accessed: 2025-04-06.\n\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n\nLopez, R. H. Q. Complexipy: An extremely fast python library to calculate the cognitive complexity of python files, written in rust,\n\n2025. URL https://github.com/rohaquinlop/complexipy. Accessed: 2025-04-06.\n\n2025. URL\n\n. Accessed: 2025-04-06.\n\nhttps://github.com/rohaquinlop/complexipy\n\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\n\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR\n\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In\n\n7th International Conference on Learning Representations, ICLR\n\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\n\n. OpenReview.net, 2019. URL\n\n.\n\n2019, New Orleans, LA, USA, May 6-9, 2019\n\nhttps://openreview.net/forum?id=Bkg6RiCqY7\n\nMorris, J. There are no new ideas in ai. . . only new datasets. https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only,\n2025.\n\nMorris, J. There are no new ideas in ai. . . only new datasets. https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only,\n\nMorris, J. There are no new ideas in ai. . . only new datasets.\n\n,\n\nhttps://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only\n\n2025.\n\n2025.\n\nOpenAI. Openai o3-mini, January 2025a. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-04-17.\n\nOpenAI. Openai o3-mini, January 2025a. URL https://openai.com/index/openai-o3-mini/. Accessed: 2025-04-17.\n\nOpenAI. Openai o3-mini, January 2025a. URL\n\n. Accessed: 2025-04-17.\n\nhttps://openai.com/index/openai-o3-mini/\n\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/.\nAccessed: 2025-04-17.\n\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL https://openai.com/index/introducing-o3-and-o4-mini/.\n\nOpenAI. Introducing openai o3 and o4-mini, April 2025b. URL\n\n.\n\nhttps://openai.com/index/introducing-o3-and-o4-mini/\n\nAccessed: 2025-04-17.\n\nAccessed: 2025-04-17.\n\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\nmanipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\n\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\n\nOpenAI, Plappert, M., Sampedro, R., Xu, T., Akkaya, I., Kosaraju, V., Welinder, P., D'Sa, R., Petron, A., de Oliveira Pinto, H. P.,\n\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\n\nPaino, A., Noh, H., Weng, L., Yuan, Q., Chu, C., and Zaremba, W. Asymmetric self-play for automatic goal discovery in robotic\n\nmanipulation. CoRR, abs/2101.04882, 2021. URL https://arxiv.org/abs/2101.04882.\n\nmanipulation.\n\n, abs/2101.04882, 2021. URL\n\n.\n\nCoRR\n\nhttps://arxiv.org/abs/2101.04882\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\nlanguage models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744,\n2022.\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\n\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training\n\nlanguage models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744,\n\nlanguage models to follow instructions with human feedback.\n\n, 35:27730\u201327744,\n\nAdvances in neural information processing systems\n\n2022.\n\n2022.\n\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural In-\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\ncouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html.\n\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\n\nPoesia, G., Broman, D., Haber, N., and Goodman, N. D. Learning formal mathematics from intrinsic motivation. In Glober-\n\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural In-\n\nsons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.),\n\nAdvances in Neural In-\n\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\n\nformation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Van-\n\ncouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/\n\n, 2024. URL\n\ncouver, BC, Canada, December 10 - 15, 2024\n\nhttp://papers.nips.cc/paper_files/paper/2024/hash/\n\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html.\n\n.\n\n4b8001fc75f0532827472ea5a16af9ca-Abstract-Conference.html\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI\nblog, 1(8):9, 2019.\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI\n\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners.\n\nOpenAI\n\nblog, 1(8):9, 2019.\n\n, 1(8):9, 2019.\n\nblog\n\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\nfor subgoal decomposition, 2025. URL https://arxiv.org/abs/2504.21801.\n\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\n\nRen, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., Wu, Z. F., Gou, Z., Ma, S., Tang, H.,\n\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\n\nLiu, Y., Gao, W., Guo, D., and Ruan, C. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning\n\nfor subgoal decomposition, 2025. URL https://arxiv.org/abs/2504.21801.\n\nfor subgoal decomposition, 2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.21801\n\nSchaul, T. Boundless socratic learning with language games. arXiv preprint arXiv:2411.16905, 2024.\n\nSchaul, T. Boundless socratic learning with language games. arXiv preprint arXiv:2411.16905, 2024.\n\nSchaul, T. Boundless socratic learning with language games.\n\n, 2024.\n\narXiv preprint arXiv:2411.16905\n\n16\n\n16\n\n16",
        "tables": []
      },
      {
        "page_num": 16,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nSchmidhuber, J. Exploring the predictable. In Advances in evolutionary computing: theory and applications, pp. 579\u2013612. Springer,\n2003.\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\nproblem. CoRR, abs/1112.5309, 2011. URL http://arxiv.org/abs/1112.5309.\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\nof mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\nhttps://doi.org/10.48550/arXiv.2402.03300.\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\nRLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\nThe Netherlands, 30 March 2025 - 3 April 2025, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https:\n//doi.org/10.1145/3689031.3696075.\nSilver, D. and Sutton, R. S. The era of experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%\n20/The%20Era%20of%20Experience%20Paper.pdf, 2025.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484\u2013489, 2016.\ndoi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR,\nabs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\nStuart, T. Understanding computation - from simple machines to impossible programs. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\nhttp://www.oreilly.de/catalog/9781449329273/index.html.\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\nasymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients. CoRR, abs/1912.06844, 2019. URL\nhttp://arxiv.org/abs/1912.06844.\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\nConference session, December 2024. URL https://neurips.cc/virtual/2024/test-of-time/105032.\nSutton, R. S. Verification, the key to ai. http://incompleteideas.net/IncIdeas/KeytoAI.html, 2001.\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms. CoRR,\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\nbased on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ViZcgDQjyG.\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\nsimple parameter editing. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\nComputational Linguistics, pp. 6337\u20136357, 2025a.\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\nand diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019. URL http://arxiv.org/abs/1901.01753.\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\nbalances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nURL https://openreview.net/forum?id=vtoY8qJjTR.\n\nSchmidhuber, J. Exploring the predictable. In Advances in evolutionary computing: theory and applications, pp. 579\u2013612. Springer,\n2003.\n\nSchmidhuber, J. Exploring the predictable. In Advances in evolutionary computing: theory and applications, pp. 579\u2013612. Springer,\n\nSchmidhuber, J. Exploring the predictable. In\n\n, pp. 579\u2013612. Springer,\n\nAdvances in evolutionary computing: theory and applications\n\n2003.\n\n2003.\n\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\nproblem. CoRR, abs/1112.5309, 2011. URL http://arxiv.org/abs/1112.5309.\n\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\n\nSchmidhuber, J. POWERPLAY: training an increasingly general problem solver by continually searching for the simplest still unsolvable\n\nproblem. CoRR, abs/1112.5309, 2011. URL http://arxiv.org/abs/1112.5309.\n\nproblem.\n\n, abs/1112.5309, 2011. URL\n\n.\n\nCoRR\n\nhttp://arxiv.org/abs/1112.5309\n\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\nof mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\nhttps://doi.org/10.48550/arXiv.2402.03300.\n\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\n\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits\n\nof mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\n\nof mathematical reasoning in open language models.\n\n, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2402.03300.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2402.03300\n\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\nRLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\nThe Netherlands, 30 March 2025 - 3 April 2025, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https:\n//doi.org/10.1145/3689031.3696075.\n\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\n\nSheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: A flexible and efficient\n\nRLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\n\nRLHF framework. In\n\nProceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam,\n\nThe Netherlands, 30 March 2025 - 3 April 2025, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL https:\n\n, pp. 1279\u20131297. ACM, 2025. doi: 10.1145/3689031.3696075. URL\n\nThe Netherlands, 30 March 2025 - 3 April 2025\n\nhttps:\n\n//doi.org/10.1145/3689031.3696075.\n\n.\n\n//doi.org/10.1145/3689031.3696075\n\nSilver, D. and Sutton, R. S. The era of experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%\n20/The%20Era%20of%20Experience%20Paper.pdf, 2025.\n\nSilver, D. and Sutton, R. S. The era of experience. https://storage.googleapis.com/deepmind-media/Era-of-Experience%\n\nSilver, D. and Sutton, R. S. The era of experience.\n\nhttps://storage.googleapis.com/deepmind-media/Era-of-Experience%\n\n20/The%20Era%20of%20Experience%20Paper.pdf, 2025.\n\n, 2025.\n\n20/The%20Era%20of%20Experience%20Paper.pdf\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484\u2013489, 2016.\ndoi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961.\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam,\n\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\n\nV., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T. P., Leach, M., Kavukcuoglu, K.,\n\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484\u2013489, 2016.\n\nGraepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search.\n\n, 529(7587):484\u2013489, 2016.\n\nNat.\n\ndoi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961.\n\ndoi: 10.1038/NATURE16961. URL\n\n.\n\nhttps://doi.org/10.1038/nature16961\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR,\nabs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap,\n\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. CoRR,\n\nT. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm.\n\n,\n\nCoRR\n\nabs/1712.01815, 2017. URL http://arxiv.org/abs/1712.01815.\n\nabs/1712.01815, 2017. URL\n\n.\n\nhttp://arxiv.org/abs/1712.01815\n\nStuart, T. Understanding computation - from simple machines to impossible programs. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\nhttp://www.oreilly.de/catalog/9781449329273/index.html.\n\nStuart, T. Understanding computation - from simple machines to impossible programs. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\n\nStuart, T.\n\n. O'Reilly, 2015. ISBN 978-1-449-32927-3. URL\n\nUnderstanding computation - from simple machines to impossible programs\n\nhttp://www.oreilly.de/catalog/9781449329273/index.html.\n\n.\n\nhttp://www.oreilly.de/catalog/9781449329273/index.html\n\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\nasymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.\n\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\n\nSukhbaatar, S., Lin, Z., Kostrikov, I., Synnaeve, G., Szlam, A., and Fergus, R. Intrinsic motivation and automatic curricula via\n\nasymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\n\nasymmetric self-play. In\n\n6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -\n\nMay 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=SkT5Yg-RZ.\n\n. OpenReview.net, 2018. URL\n\n.\n\nMay 3, 2018, Conference Track Proceedings\n\nhttps://openreview.net/forum?id=SkT5Yg-RZ\n\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients. CoRR, abs/1912.06844, 2019. URL\nhttp://arxiv.org/abs/1912.06844.\n\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients. CoRR, abs/1912.06844, 2019. URL\n\nSuteu, M. and Guo, Y. Regularizing deep multi-task networks using orthogonal gradients.\n\n, abs/1912.06844, 2019. URL\n\nCoRR\n\nhttp://arxiv.org/abs/1912.06844.\n\n.\n\nhttp://arxiv.org/abs/1912.06844\n\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\nConference session, December 2024. URL https://neurips.cc/virtual/2024/test-of-time/105032.\n\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\n\nSutskever, I., Vinyals, O., and Le, Q. V. Neurips 2024 test of time award session: Sequence to sequence learning with neural networks.\n\nConference session, December 2024. URL https://neurips.cc/virtual/2024/test-of-time/105032.\n\nConference session, December 2024. URL\n\n.\n\nhttps://neurips.cc/virtual/2024/test-of-time/105032\n\nSutton, R. S. Verification, the key to ai. http://incompleteideas.net/IncIdeas/KeytoAI.html, 2001.\n\nSutton, R. S. Verification, the key to ai. http://incompleteideas.net/IncIdeas/KeytoAI.html, 2001.\n\nSutton, R. S. Verification, the key to ai.\n\n, 2001.\n\nhttp://incompleteideas.net/IncIdeas/KeytoAI.html\n\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms. CoRR,\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.\n\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\n\nTeam, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu,\n\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\n\nE., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu,\n\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\n\nH., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L.,\n\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\n\nDong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W.,\n\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\n\nHuang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen,\n\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\n\nY., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang,\n\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms. CoRR,\n\nZ., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms.\n\n,\n\nCoRR\n\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL https://doi.org/10.48550/arXiv.2501.12599.\n\nabs/2501.12599, 2025. doi: 10.48550/ARXIV.2501.12599. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2501.12599\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\nbased on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ViZcgDQjyG.\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\n\nVillalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M. Position: Will we run out of data? limits of LLM scaling\n\nbased on human-generated data. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n\nbased on human-generated data. In\n\nForty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July\n\n21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ViZcgDQjyG.\n\n. OpenReview.net, 2024. URL\n\n.\n\n21-27, 2024\n\nhttps://openreview.net/forum?id=ViZcgDQjyG\n\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\nsimple parameter editing. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\nComputational Linguistics, pp. 6337\u20136357, 2025a.\n\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\n\nWang, H., Yue, Y., Lu, R., Shi, J., Zhao, A., Wang, S., Song, S., and Huang, G. Model surgery: Modulating LLM's behavior via\n\nsimple parameter editing. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\n\nsimple parameter editing. In\n\nProceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for\n\nComputational Linguistics, pp. 6337\u20136357, 2025a.\n\n, pp. 6337\u20136357, 2025a.\n\nComputational Linguistics\n\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\nand diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019. URL http://arxiv.org/abs/1901.01753.\n\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\n\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired open-ended trailblazer (POET): endlessly generating increasingly complex\n\nand diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019. URL http://arxiv.org/abs/1901.01753.\n\nand diverse learning environments and their solutions.\n\n, abs/1901.01753, 2019. URL\n\n.\n\nCoRR\n\nhttp://arxiv.org/abs/1901.01753\n\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\nbalances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nURL https://openreview.net/forum?id=vtoY8qJjTR.\n\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\n\nWang, S., Yang, Q., Gao, J., Lin, M. G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G. Train once, get a family: State-adaptive\n\nbalances for offline-to-online reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n\nbalances for offline-to-online reinforcement learning. In\n\n, 2023.\n\nThirty-seventh Conference on Neural Information Processing Systems\n\nURL https://openreview.net/forum?id=vtoY8qJjTR.\n\nURL\n\n.\n\nhttps://openreview.net/forum?id=vtoY8qJjTR\n\n17\n\n17\n\n17",
        "tables": []
      },
      {
        "page_num": 17,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nWang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for\nComputational Linguistics: ACL 2024, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.findings-acl.591. URL https://aclanthology.org/2024.findings-acl.591/.\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\nhttps://arxiv.org/abs/2504.20571.\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\nLLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\nhttps://doi.org/10.48550/arXiv.2308.08155.\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows. CoRR,\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL https://doi.org/10.48550/arXiv.2403.11322.\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\nreasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\nhttps://doi.org/10.48550/arXiv.2502.14768.\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\nself-training framework for advanced reasoning, 2025. URL https://arxiv.org/abs/2504.08672.\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\nCoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.\ndoi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. CoRR,\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL https://doi.org/10.48550/arXiv.2411.00062.\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\nCoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476.\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. URL https://arxiv.\norg/abs/2401.10020, 2024.\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\nlearning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\nAdvances in Neural Information Processing Systems, 36:60247\u201360277, 2023.\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\nZhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html.\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\ncapacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837.\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information\nProcessing Systems, 35:15476\u201315488, 2022.\n\nWang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for\nComputational Linguistics: ACL 2024, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.findings-acl.591. URL https://aclanthology.org/2024.findings-acl.591/.\n\nWang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\n\nWang, S., Liu, C., Zheng, Z., Qi, S., Chen, S., Yang, Q., Zhao, A., Wang, C., Song, S., and Huang, G. Boosting LLM agents with recursive\n\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for\n\ncontemplation for effective deception handling. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.),\n\nFindings of the Association for\n\nComputational Linguistics: ACL 2024, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n\n, pp. 9909\u20139953, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n\nComputational Linguistics: ACL 2024\n\ndoi: 10.18653/v1/2024.findings-acl.591. URL https://aclanthology.org/2024.findings-acl.591/.\n\ndoi: 10.18653/v1/2024.findings-acl.591. URL\n\n.\n\nhttps://aclanthology.org/2024.findings-acl.591/\n\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\nhttps://arxiv.org/abs/2504.20571.\n\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\n\nWang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, S., Du,\n\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\n\nS. S., and Shen, Y. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL\n\nhttps://arxiv.org/abs/2504.20571.\n\n.\n\nhttps://arxiv.org/abs/2504.20571\n\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\nLLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\nhttps://doi.org/10.48550/arXiv.2308.08155.\n\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\n\nWu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen\n\nLLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\n\nLLM applications via multi-agent conversation framework.\n\n, abs/2308.08155, 2023. doi: 10.48550/ARXIV.2308.08155. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2308.08155.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2308.08155\n\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows. CoRR,\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL https://doi.org/10.48550/arXiv.2403.11322.\n\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows. CoRR,\n\nWu, Y., Yue, T., Zhang, S., Wang, C., and Wu, Q. Stateflow: Enhancing LLM task-solving through state-driven workflows.\n\n,\n\nCoRR\n\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL https://doi.org/10.48550/arXiv.2403.11322.\n\nabs/2403.11322, 2024. doi: 10.48550/ARXIV.2403.11322. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2403.11322\n\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\nreasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\nhttps://doi.org/10.48550/arXiv.2502.14768.\n\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\n\nXie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing LLM\n\nreasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\n\nreasoning with rule-based reinforcement learning.\n\n, abs/2502.14768, 2025. doi: 10.48550/ARXIV.2502.14768. URL\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2502.14768.\n\n.\n\nhttps://doi.org/10.48550/arXiv.2502.14768\n\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\nself-training framework for advanced reasoning, 2025. URL https://arxiv.org/abs/2504.08672.\n\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\n\nXu, F., Yan, H., Ma, C., Zhao, H., Sun, Q., Cheng, K., He, J., Liu, J., and Wu, Z. Genius: A generalizable and purely unsupervised\n\nself-training framework for advanced reasoning, 2025. URL https://arxiv.org/abs/2504.08672.\n\nself-training framework for advanced reasoning, 2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.08672\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\nCoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang,\n\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\n\nJ., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T.,\n\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\n\nXia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report.\n\nCoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\n\n, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2412.15115\n\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.\ndoi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122.\n\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\n\nYang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and\n\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.\n\nZhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement.\n\n, abs/2409.12122, 2024b.\n\nCoRR\n\ndoi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122.\n\ndoi: 10.48550/ARXIV.2409.12122. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2409.12122\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\nOpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\n\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language\n\nmodels. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023.\n\nmodels. In\n\n.\n\nThe Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023\n\nOpenReview.net, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X.\n\nOpenReview.net, 2023. URL\n\n.\n\nhttps://openreview.net/forum?id=WE_vluYUL-X\n\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. CoRR,\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL https://doi.org/10.48550/arXiv.2411.00062.\n\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play. CoRR,\n\nYe, Z., Agarwal, R., Liu, T., Joshi, R., Velury, S., Le, Q. V., Tan, Q., and Liu, Y. Evolving alignment via asymmetric self-play.\n\n,\n\nCoRR\n\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL https://doi.org/10.48550/arXiv.2411.00062.\n\nabs/2411.00062, 2024. doi: 10.48550/ARXIV.2411.00062. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2411.00062\n\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\nCoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476.\n\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\n\nYu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y.,\n\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\n\nZhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J.,\n\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\n\nMa, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale.\n\nCoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476.\n\n, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL\n\n.\n\nCoRR\n\nhttps://doi.org/10.48550/arXiv.2503.14476\n\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. URL https://arxiv.\norg/abs/2401.10020, 2024.\n\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. URL https://arxiv.\n\nYuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models.\n\nURL https://arxiv.\n\norg/abs/2401.10020, 2024.\n\n, 2024.\n\norg/abs/2401.10020\n\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\nlearning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.\n\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\n\nYuan, Y., Yu, Q., Zuo, X., Zhu, R., Xu, W., Chen, J., Wang, C., Fan, T., Du, Z., Wei, X., et al. Vapo: Efficient and reliable reinforcement\n\nlearning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025.\n\nlearning for advanced reasoning tasks.\n\n, 2025.\n\narXiv preprint arXiv:2504.05118\n\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\nAdvances in Neural Information Processing Systems, 36:60247\u201360277, 2023.\n\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\n\nYue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl.\n\nAdvances in Neural Information Processing Systems, 36:60247\u201360277, 2023.\n\n, 36:60247\u201360277, 2023.\n\nAdvances in Neural Information Processing Systems\n\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\nZhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html.\n\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\n\nYue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., and Huang, G. Deer-vla: Dynamic inference of multimodal large\n\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\n\nlanguage models for efficient robot execution. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and\n\nZhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\n\nZhang, C. (eds.),\n\nAdvances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing\n\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_\n\n, 2024. URL\n\nSystems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024\n\nhttp://papers.nips.cc/paper_\n\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html.\n\n.\n\nfiles/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html\n\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\ncapacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837.\n\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\n\nYue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning\n\ncapacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837.\n\ncapacity in llms beyond the base model?, 2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.13837\n\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information\nProcessing Systems, 35:15476\u201315488, 2022.\n\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information\n\nZelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning.\n\nAdvances in Neural Information\n\nProcessing Systems, 35:15476\u201315488, 2022.\n\n, 35:15476\u201315488, 2022.\n\nProcessing Systems\n\n18\n\n18\n\n18",
        "tables": []
      },
      {
        "page_num": 18,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nZeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis. CoRR,\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL https://doi.org/10.48550/arXiv.2502.01718.\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\nlearning for open base models in the wild. CoRR, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL https:\n//doi.org/10.48550/arXiv.2503.18892.\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\non replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\nincentivization, 2025b. URL https://arxiv.org/abs/2504.05812.\nZhang, Y. and Yang, Q. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609,\n2021.\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Pro-\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html.\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\nJ. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\nURL https://doi.org/10.1609/aaai.v38i17.29936.\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.), AAAI-25, Sponsored by the Association for the\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 26021\u201326030. AAAI Press, 2025a.\ndoi: 10.1609/AAAI.V39I24.34797. URL https://doi.org/10.1609/aaai.v39i24.34797.\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning. Neural\nNetworks, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL https://doi.org/10.1016/j.neunet.2025.\n107448.\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research,\npp. 2165\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html.\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n2025. URL https://arxiv.org/abs/2504.16084.\n\nZeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis. CoRR,\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL https://doi.org/10.48550/arXiv.2502.01718.\n\nZeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis. CoRR,\n\nZeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: acing coder RL via automated test-case synthesis.\n\n,\n\nCoRR\n\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL https://doi.org/10.48550/arXiv.2502.01718.\n\nabs/2502.01718, 2025a. doi: 10.48550/ARXIV.2502.01718. URL\n\n.\n\nhttps://doi.org/10.48550/arXiv.2502.01718\n\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\nlearning for open base models in the wild. CoRR, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL https:\n//doi.org/10.48550/arXiv.2503.18892.\n\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\n\nZeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement\n\nlearning for open base models in the wild. CoRR, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL https:\n\nlearning for open base models in the wild.\n\n, abs/2503.18892, 2025b. doi: 10.48550/ARXIV.2503.18892. URL\n\nCoRR\n\nhttps:\n\n//doi.org/10.48550/arXiv.2503.18892.\n\n.\n\n//doi.org/10.48550/arXiv.2503.18892\n\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\non replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.\n\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\n\nZhang, C., Deng, Y., Lin, X., Wang, B., Ng, D., Ye, H., Li, X., Xiao, Y., Mo, Z., Zhang, Q., et al. 100 days after deepseek-r1: A survey\n\non replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.\n\non replication studies and more directions for reasoning language models.\n\n, 2025a.\n\narXiv preprint arXiv:2505.00551\n\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\nincentivization, 2025b. URL https://arxiv.org/abs/2504.05812.\n\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\n\nZhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning\n\nincentivization, 2025b. URL https://arxiv.org/abs/2504.05812.\n\nincentivization, 2025b. URL\n\n.\n\nhttps://arxiv.org/abs/2504.05812\n\nZhang, Y. and Yang, Q. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609,\n2021.\n\nZhang, Y. and Yang, Q. A survey on multi-task learning. IEEE transactions on knowledge and data engineering, 34(12):5586\u20135609,\n\nZhang, Y. and Yang, Q. A survey on multi-task learning.\n\n, 34(12):5586\u20135609,\n\nIEEE transactions on knowledge and data engineering\n\n2021.\n\n2021.\n\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Pro-\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html.\n\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\n\nZhao, A., Lin, M. G., Li, Y., Liu, Y., and Huang, G. A mixture of surprises for unsupervised reinforcement learning. In\n\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Pro-\n\nKoyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.),\n\nAdvances in Neural Information Pro-\n\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\n\ncessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\n\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\n\n, 2022. URL\n\nLA, USA, November 28 - December 9, 2022\n\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\n\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html.\n\n.\n\na7667ee5d545a43d2f0fda98863c260e-Abstract-Conference.html\n\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\nJ. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\nEAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\nURL https://doi.org/10.1609/aaai.v38i17.29936.\n\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\n\nZhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy,\n\nJ. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\n\nJ. G., and Natarajan, S. (eds.),\n\nThirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on\n\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\n\nInnovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,\n\nEAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\n\n, pp. 19632\u201319642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936.\n\nEAAI 2014, February 20-27, 2024, Vancouver, Canada\n\nURL https://doi.org/10.1609/aaai.v38i17.29936.\n\nURL\n\n.\n\nhttps://doi.org/10.1609/aaai.v38i17.29936\n\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.), AAAI-25, Sponsored by the Association for the\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 26021\u201326030. AAAI Press, 2025a.\ndoi: 10.1609/AAAI.V39I24.34797. URL https://doi.org/10.1609/aaai.v39i24.34797.\n\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\n\nZhao, A., Xu, Q., Lin, M., Wang, S., Liu, Y., Zheng, Z., and Huang, G. Diver-ct: Diversity-enhanced red teaming large language model\n\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.), AAAI-25, Sponsored by the Association for the\n\nassistants with relaxing constraints. In Walsh, T., Shah, J., and Kolter, Z. (eds.),\n\nAAAI-25, Sponsored by the Association for the\n\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 26021\u201326030. AAAI Press, 2025a.\n\n, pp. 26021\u201326030. AAAI Press, 2025a.\n\nAdvancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA\n\ndoi: 10.1609/AAAI.V39I24.34797. URL https://doi.org/10.1609/aaai.v39i24.34797.\n\ndoi: 10.1609/AAAI.V39I24.34797. URL\n\n.\n\nhttps://doi.org/10.1609/aaai.v39i24.34797\n\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning. Neural\nNetworks, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL https://doi.org/10.1016/j.neunet.2025.\n107448.\n\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning. Neural\n\nZhao, A., Zhu, E., Lu, R., Lin, M., Liu, Y., and Huang, G. Self-referencing agents for unsupervised reinforcement learning.\n\nNeural\n\nNetworks, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL https://doi.org/10.1016/j.neunet.2025.\n\n, 188:107448, 2025b. doi: 10.1016/J.NEUNET.2025.107448. URL\n\nNetworks\n\nhttps://doi.org/10.1016/j.neunet.2025.\n\n107448.\n\n.\n\n107448\n\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research,\npp. 2165\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html.\n\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\n\nZitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran,\n\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\n\nH. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K.,\n\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\n\nMordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J.,\n\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\n\nIrpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess,\n\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\n\nD., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2:\n\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference\n\nvision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.),\n\nConference\n\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research,\n\n, volume 229 of\n\n,\n\non Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA\n\nProceedings of Machine Learning Research\n\npp. 2165\u20132183. PMLR, 2023. URL https://proceedings.mlr.press/v229/zitkovich23a.html.\n\npp. 2165\u20132183. PMLR, 2023. URL\n\n.\n\nhttps://proceedings.mlr.press/v229/zitkovich23a.html\n\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n2025. URL https://arxiv.org/abs/2504.16084.\n\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n\nZuo, Y., Zhang, K., Qu, S., Sheng, L., Zhu, X., Qi, B., Sun, Y., Cui, G., Ding, N., and Zhou, B. Ttrl: Test-time reinforcement learning,\n\n2025. URL https://arxiv.org/abs/2504.16084.\n\n2025. URL\n\n.\n\nhttps://arxiv.org/abs/2504.16084\n\n19\n\n19\n\n19",
        "tables": []
      },
      {
        "page_num": 19,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAppendix\n\nAppendix\n\nAppendix\n\nAppendix Contents\n\nAppendix Contents\n\nAppendix Contents\n\n20\n\n20\n\n20",
        "tables": []
      },
      {
        "page_num": 20,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nA. Reinforcement Learning with Verifiable Rewards.\n\nA. Reinforcement Learning with Verifiable Rewards.\n\nA. Reinforcement Learning with Verifiable Rewards.\n\nWe use reinforcement learning to update our learner LLM, rewarding it based on a task-specific reward function rf , where the subscript\nf indicates the task. The goal of the RL agent is to maximize the expected discounted sum of rewards. We adopt an online variant of RL,\nREINFORCE++, which is optimized using the original PPO objective:\n\nWe use reinforcement learning to update our learner LLM, rewarding it based on a task-specific reward function rf , where the subscript\n\nWe use reinforcement learning to update our learner LLM, rewarding it based on a task-specific reward function\n\n, where the subscript\n\nr\n\nf\n\nf indicates the task. The goal of the RL agent is to maximize the expected discounted sum of rewards. We adopt an online variant of RL,\n\nindicates the task. The goal of the RL agent is to maximize the expected discounted sum of rewards. We adopt an online variant of RL,\n\nf\n\nREINFORCE++, which is optimized using the original PPO objective:\n\nREINFORCE++, which is optimized using the original PPO objective:\n\nLPPO(\u03b8) = Eq\u223cP (Q), o\u223c\u03c0\u03b8old (O|q)\n\"\n1\n|o|\nX\n|o|\nt=1\nmin \nst(\u03b8)A\nnorm\nf,q , clip (st(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) A\nnorm\nf,q \n#\n, (9)\n\n|o|\n\no\n\n\"\n\n\"\n\n#\n\n#\n\n|\n\n|\n\n1\n\n1\n\nnorm\n\nnorm\n\nnorm\n\nnorm\n\nX\n\nX\n\nLPPO(\u03b8) = Eq\u223cP (Q), o\u223c\u03c0\u03b8old (O|q)\n\nE\n\nf,q , clip (st(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) A\n\nclip\n\n, (9)\n\n(9)\n\nmin\n\nf,q\n\n(\n\n) =\n\nmin\n\nst(\u03b8)A\n\n(\n\n)\n\n(\n\n(\n\n)\n\n1\n\n1 +\n\n)\n\n\u03b8\n\ns\n\n\u03b8\n\nA\n\n,\n\ns\n\n\u03b8\n\n,\n\n\u03f5,\n\n\u03f5\n\nA\n\n,\n\nPPO\n\nL\n\n\u2212\n\nt\n\nt\n\n(\n\n)\n\n(\n\n)\n\nq\n\nP\n\nQ\n\n, o\n\n\u03c0\n\nO\n\nq\n\nf,q\n\nf,q\n\n\u223c\n\n\u223c\n\n|\n\n\u03b8\n\n|o|\n\no\n\nold\n\n|\n\n|\n\nt=1\n\n=1\n\nt\n\nwhere st(\u03b8) is the probability ratio between the new and old policies at timestep t, and A\nnorm\nf,q is the normalized advantage.\n\nnorm\n\nnorm\n\nwhere st(\u03b8) is the probability ratio between the new and old policies at timestep t, and A\n\nwhere\n\nis the probability ratio between the new and old policies at timestep\n\n, and\n\nf,q is the normalized advantage.\n\nis the normalized advantage.\n\n(\n\n)\n\ns\n\n\u03b8\n\nt\n\nA\n\nt\n\nf,q\n\nREINFORCE++ computes the normalized advantage as:\n\nREINFORCE++ computes the normalized advantage as:\n\nREINFORCE++ computes the normalized advantage as:\n\nA\nnorm\nf,q =\nrf,q \u2212 mean({Af,q}\nB)\nstd({Af,q}B)\n, (10)\n\nrf,q \u2212 mean({Af,q}\n\nmean\n\nB)\n\nB\n\n(\n\n)\n\nr\n\nA\n\nnorm\n\nnorm\n\n\u2212\n\n{\n\n}\n\nf,q\n\nf,q\n\n, (10)\n\n(10)\n\nf,q =\n\n=\n\nA\n\nA\n\n,\n\nf,q\n\nstd({Af,q}B)\n\nstd\n\nB\n\n(\n\n)\n\nA\n\n{\n\n}\n\nf,q\n\nwhere rf,q is the outcome reward for question q, task f, mean and std are calculated across the global batch with batch size B. Note that\nwe do not apply any KL penalty to the loss or reward.\n\nwhere rf,q is the outcome reward for question q, task f, mean and std are calculated across the global batch with batch size B. Note that\n\nis the outcome reward for question\n\n, task\n\n, mean and std are calculated across the global batch with batch size\n\n. Note that\n\nwhere\n\nr\n\nq\n\nf\n\nB\n\nf,q\n\nwe do not apply any KL penalty to the loss or reward.\n\nwe do not apply any KL penalty to the loss or reward.\n\nB. Implementation Details\n\nB. Implementation Details\n\nB. Implementation Details\n\nWe built Absolute Zero Reasoner upon the veRL codebase (Sheng et al., 2025). For code execution, we incorporated components from\nthe QwQ Python executor. For safer code execution, we recommend using API-based services such as E2B instead.\n\nWe built Absolute Zero Reasoner upon the veRL codebase (Sheng et al., 2025). For code execution, we incorporated components from\n\nWe built Absolute Zero Reasoner upon the\n\nveRL codebase\n\n(Sheng et al.,\n\n. For code execution, we incorporated components from\n\n2025)\n\nthe QwQ Python executor. For safer code execution, we recommend using API-based services such as E2B instead.\n\nthe\n\nQwQ Python executor.\n\nFor safer code execution, we recommend using API-based services such as\n\nE2B\n\ninstead.\n\nAll experiments were conducted on clusters of A800 GPUs.\n\nAll experiments were conducted on clusters of A800 GPUs.\n\nAll experiments were conducted on clusters of A800 GPUs.\n\nTraining Hyperparameters. We show the hyperparameters used in our training in Table 3. We do not change them for any of\nthe runs.\n\nTraining Hyperparameters. We show the hyperparameters used in our training in Table 3. We do not change them for any of\n\nWe show the hyperparameters used in our training in Table\n\nWe do not change them for any of\n\n3.\n\nTraining Hyperparameters.\n\nthe runs.\n\nthe runs.\n\nParameterValueModel ConfigurationMax Prompt Length6144Max Response Length8096Seed Batch Factor4Max Programs16384Training SettingsTrain Batch Size64 * 6Learning Rate1e-6OptimizerAdamWGrad Clip1.0Total Steps500RL SettingsAlgorithmTRR++ (Section\n3.3.5)KL LossFalseKL RewardFalseEntropy Coefficient0.001PPO Epochs1Rollouts\nN1Rollout Temperature1.0Rollout Top-P1.0References\nK6Samples to Estimate Task Accuracy\nN8\n\nValue\n\nParameter\n\nModel Configuration\n\nMax Prompt Length\n\n6144\n\nMax Response Length\n\n8096\n\nSeed Batch Factor\n\n4\n\nMax Programs\n\n16384\n\nTraining Settings\n\nTrain Batch Size\n\n64 * 6\n\nLearning Rate\n\n1e-6\n\nOptimizer\n\nAdamW\n\n1.0\n\nGrad Clip\n\nTotal Steps\n\n500\n\nRL Settings\n\nAlgorithm\n\nTRR++ (Section\n3.3.5)\n\nKL Loss\n\nFalse\n\nKL Reward\n\nFalse\n\nEntropy Coefficient\n\n0.001\n\nPPO Epochs\n\n1\n\nRollouts\nN\n\n1\n\nRollout Temperature\n\n1.0\n\nRollout Top-P\n\n1.0\n\nReferences\nK\n\n6\n\nSamples to Estimate Task Accuracy\nN\n\n8\n\nTable 3. Hyperparameters Used During AZR Self-play Training.\n\nTable 3. Hyperparameters Used During AZR Self-play Training.\n\nHyperparameters Used During AZR Self-play Training.\n\nTable 3.\n\n21\n\n21\n\n21",
        "tables": [
          {
            "text": "ParameterValueModel ConfigurationMax Prompt Length6144Max Response Length8096Seed Batch Factor4Max Programs16384Training SettingsTrain Batch Size64 * 6Learning Rate1e-6OptimizerAdamWGrad Clip1.0Total Steps500RL SettingsAlgorithmTRR++ (Section\n3.3.5)KL LossFalseKL RewardFalseEntropy Coefficient0.001PPO Epochs1Rollouts\nN1Rollout Temperature1.0Rollout Top-P1.0References\nK6Samples to Estimate Task Accuracy\nN8",
            "csv": "",
            "json": "{\n  \"headers\": [],\n  \"rows\": []\n}"
          }
        ]
      },
      {
        "page_num": 21,
        "text": "ModelData CurationBase ModelOat-7B (Liu et al.,\n2025)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-MathSimpleRL-Zoo (Zeng et al.,\n2025b)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-BaseOpenReasonerZero (Hu et al.,\n2025)57k STEM + math samplesQwen2.5-7B-BasePRIME-Zero (Cui et al.,\n2025)457k math + 27k code problemsQwen2.5Math-7B-BaseCodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)2k Leetcode pairsQwen2.5-7B-Instruct-1MCodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)2k Leetcode + 10k TACO pairs (Li et al.,\n2023)Qwen2.5-7B-Instruct-1MAceCoder-7B-Ins-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Code-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderAceCoder-7B-Code-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderQwen-7B-Instruct (Yang et al.,\n2024a)1M SFT + 150k RL pairsQwen2.5-7B-BaseAZR-7B (Ours)No dataQwen2.5-7B-BaseAZR-7B-Coder (Ours)No dataQwen2.5-7B-CoderTable 4. Reasoner Training Data Source and Base Model.\n\nModelData CurationBase ModelOat-7B (Liu et al.,\n2025)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-MathSimpleRL-Zoo (Zeng et al.,\n2025b)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-BaseOpenReasonerZero (Hu et al.,\n2025)57k STEM + math samplesQwen2.5-7B-BasePRIME-Zero (Cui et al.,\n2025)457k math + 27k code problemsQwen2.5Math-7B-BaseCodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)2k Leetcode pairsQwen2.5-7B-Instruct-1MCodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)2k Leetcode + 10k TACO pairs (Li et al.,\n2023)Qwen2.5-7B-Instruct-1MAceCoder-7B-Ins-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Code-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderAceCoder-7B-Code-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderQwen-7B-Instruct (Yang et al.,\n2024a)1M SFT + 150k RL pairsQwen2.5-7B-BaseAZR-7B (Ours)No dataQwen2.5-7B-BaseAZR-7B-Coder (Ours)No dataQwen2.5-7B-Coder\n\nBase Model\n\nModel\n\nData Curation\n\nQwen2.5-7B-Math\n\nOat-7B (Liu et al.,\n2025)\n\n8.5k math pairs (Hendrycks et al.,\n2021)\n\nSimpleRL-Zoo (Zeng et al.,\n2025b)\n\n8.5k math pairs (Hendrycks et al.,\n2021)\n\nQwen2.5-7B-Base\n\nOpenReasonerZero (Hu et al.,\n2025)\n\n57k STEM + math samples\n\nQwen2.5-7B-Base\n\nPRIME-Zero (Cui et al.,\n2025)\n\n457k math + 27k code problems\n\nQwen2.5Math-7B-Base\n\nCodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)\n\n2k Leetcode pairs\n\nQwen2.5-7B-Instruct-1M\n\nCodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)\n\n2k Leetcode + 10k TACO pairs (Li et al.,\n2023)\n\nQwen2.5-7B-Instruct-1M\n\nAceCoder-7B-Ins-RM (Zeng et al.,\n2025a)\n\n22k code data\n\nQwen2.5-7B-Instruct\n\nAceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)\n\n22k code data\n\nQwen2.5-7B-Instruct\n\nAceCoder-7B-Code-RM (Zeng et al.,\n2025a)\n\n22k code data\n\nQwen2.5-7B-Coder\n\nAceCoder-7B-Code-Rule (Zeng et al.,\n2025a)\n\n22k code data\n\nQwen2.5-7B-Coder\n\nQwen-7B-Instruct (Yang et al.,\n2024a)\n\n1M SFT + 150k RL pairs\n\nQwen2.5-7B-Base\n\nAZR-7B (Ours)\n\nNo data\n\nQwen2.5-7B-Base\n\nAZR-7B-Coder (Ours)\n\nNo data\n\nQwen2.5-7B-Coder\n\nTable 4. Reasoner Training Data Source and Base Model.\n\nTable 4. Reasoner Training Data Source and Base Model.\n\nReasoner Training Data Source and Base Model.\n\nTable 4.\n\nloggingrandommultiprocessingpebblesubprocessthreadingdatetimetimehashlibcalendarbcryptos.sysos.pathsys.exitos.environ\n\nlogging\n\nrandom\n\nmultiprocessing\n\npebble\n\nsubprocess\n\nthreading\n\ndatetime\n\ntime\n\nhashlib\n\ncalendar\n\nbcrypt\n\nos.sys\n\nos.path\n\nsys.exit\n\nos.environ\n\nFigure 8. Forbidden Python Modules. List of Python modules forbidden to exist in proposed tasks' programs.\n\nFigure 8. Forbidden Python Modules. List of Python modules forbidden to exist in proposed tasks' programs.\n\nList of Python modules forbidden to exist in proposed tasks' programs.\n\nForbidden Python Modules.\n\nFigure 8.\n\nC. More Results\n\nC. More Results\n\nC. More Results\n\nC.1. Out-of-Distribution Performance Breakdown\n\nC.1. Out-of-Distribution Performance Breakdown\n\nC.1. Out-of-Distribution Performance Breakdown\n\nWe plot the out-of-distribution performance, broken down by each benchmark and in aggregate, across training steps for our 7B, 7B-Coder,\n14B, and 14B-Coder models in Figures 28 to 31. We observe a strong correlation between training using AZR and improvements in both\nmathematical and coding reasoning capabilities. Moreover, our models are trained for more steps than typical zero-style reasoners; while\noverfitting can occur with static datasets, it is less likely in AZR due to dynamically proposed tasks.\n\nWe plot the out-of-distribution performance, broken down by each benchmark and in aggregate, across training steps for our 7B, 7B-Coder,\n\nWe plot the out-of-distribution performance, broken down by each benchmark and in aggregate, across training steps for our 7B, 7B-Coder,\n\n14B, and 14B-Coder models in Figures 28 to 31. We observe a strong correlation between training using AZR and improvements in both\n\n14B, and 14B-Coder models in Figures\n\nWe observe a strong correlation between training using AZR and improvements in both\n\n28\n\nto\n\n31.\n\nmathematical and coding reasoning capabilities. Moreover, our models are trained for more steps than typical zero-style reasoners; while\n\nmathematical and coding reasoning capabilities. Moreover, our models are trained for more steps than typical zero-style reasoners; while\n\noverfitting can occur with static datasets, it is less likely in AZR due to dynamically proposed tasks.\n\noverfitting can occur with static datasets, it is less likely in AZR due to dynamically proposed tasks.\n\nC.2. In-Distribution Results\n\nC.2. In-Distribution Results\n\nC.2. In-Distribution Results\n\nSince we have defined the task domains as input prediction and output prediction, we can directly evaluate our model's capabilities in these\nareas using popular code reasoning benchmarks: CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E) (Gu\net al., 2024; Jain et al., 2024), where CruxEval-O and LCB-E is solving the deduction task, and CruxEval-I is solving the abduction task.\nIn Figure 14, we visualize the evolution of these metrics during the training of Absolute Zero Reasoner-base-7b. As training\nprogresses, we observe a consistent improvement in in-distribution performance across steps. While these three benchmark curves do\nnot perfectly correlate with broader coding or math reasoning capabilities (compare this with Figure 28), they serve as useful proxies for\ntracking task-specific progress.\n\nSince we have defined the task domains as input prediction and output prediction, we can directly evaluate our model's capabilities in these\n\nSince we have defined the task domains as input prediction and output prediction, we can directly evaluate our model's capabilities in these\n\nareas using popular code reasoning benchmarks: CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E) (Gu\n\nareas using popular code reasoning benchmarks: CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E)\n\n(Gu\n\net al., 2024; Jain et al., 2024), where CruxEval-O and LCB-E is solving the deduction task, and CruxEval-I is solving the abduction task.\n\net al.,\n\nJain et al.,\n\n, where CruxEval-O and LCB-E is solving the deduction task, and CruxEval-I is solving the abduction task.\n\n2024;\n\n2024)\n\nIn Figure 14, we visualize the evolution of these metrics during the training of Absolute Zero Reasoner-base-7b. As training\n\nIn Figure\n\n14,\n\nwe visualize the evolution of these metrics during the training of\n\n. As training\n\nAbsolute Zero Reasoner-base-7b\n\nprogresses, we observe a consistent improvement in in-distribution performance across steps. While these three benchmark curves do\n\nprogresses, we observe a consistent improvement in in-distribution performance across steps. While these three benchmark curves do\n\nnot perfectly correlate with broader coding or math reasoning capabilities (compare this with Figure 28), they serve as useful proxies for\n\nnot perfectly correlate with broader coding or math reasoning capabilities (compare this with Figure\n\n, they serve as useful proxies for\n\n28)\n\ntracking task-specific progress.\n\ntracking task-specific progress.\n\nC.3. Interplay Between Propose and Solve Roles\n\nC.3. Interplay Between Propose and Solve Roles\n\nC.3. Interplay Between Propose and Solve Roles\n\nWe visualize the training dynamics between the propose and solve roles over training steps in Figures 15 to 17. We observe that, in\ngeneral, the solve roles produce more output tokens than the propose role. Intuitively, this makes sense: the propose role emphasizes\ncreativity and generation of novel tasks, whereas the solve role requires deeper reasoning, which naturally leads to longer outputs.\n\nWe visualize the training dynamics between the propose and solve roles over training steps in Figures 15 to 17. We observe that, in\n\nWe visualize the training dynamics between the propose and solve roles over training steps in Figures\n\n15\n\nto\n\n17.\n\nWe observe that, in\n\ngeneral, the solve roles produce more output tokens than the propose role. Intuitively, this makes sense: the propose role emphasizes\n\ngeneral, the solve roles produce more output tokens than the propose role. Intuitively, this makes sense: the propose role emphasizes\n\ncreativity and generation of novel tasks, whereas the solve role requires deeper reasoning, which naturally leads to longer outputs.\n\ncreativity and generation of novel tasks, whereas the solve role requires deeper reasoning, which naturally leads to longer outputs.\n\nInterestingly, we also observe a consistent ordering in token length across reasoning types\u2014abduction and deduction tasks tend to result\nin shorter outputs than induction tasks during problem solving. This aligns with our intuition, as we observed the model engaging\nin trial-and-error reasoning\u2014repeatedly generating hypothesized inputs, evaluating their outcomes, and reflecting and retrying when\nsubsequent deductions fail to produce the correct output. To our knowledge, this is the first time such a clear distinction in token length\n\nInterestingly, we also observe a consistent ordering in token length across reasoning types\u2014abduction and deduction tasks tend to result\n\nInterestingly, we also observe a consistent ordering in token length across reasoning types\u2014abduction and deduction tasks tend to result\n\nin shorter outputs than induction tasks during problem solving. This aligns with our intuition, as we observed the model engaging\n\nin shorter outputs than induction tasks during problem solving. This aligns with our intuition, as we observed the model engaging\n\nin trial-and-error reasoning\u2014repeatedly generating hypothesized inputs, evaluating their outcomes, and reflecting and retrying when\n\nin trial-and-error reasoning\u2014repeatedly generating hypothesized inputs, evaluating their outcomes, and reflecting and retrying when\n\nsubsequent deductions fail to produce the correct output. To our knowledge, this is the first time such a clear distinction in token length\n\nsubsequent deductions fail to produce the correct output. To our knowledge, this is the first time such a clear distinction in token length\n\n1 VALIDATE_CODE_TEMPLATE = \"\"\"{ code }\n2 repr (f({ inputs }))\"\"\"\n3\n4 exec ( VALIDATE_CODE_TEMPLATE )\n\n1 VALIDATE_CODE_TEMPLATE = \"\"\"{ code }\n\nVALIDATE_CODE_TEMPLATE =\n\n\"\"\"{\n\ncode\n\n}\n\n1\n\n2 repr (f({ inputs }))\"\"\"\n\nrepr\n\n(f({\n\ninputs\n\n}))\"\"\"\n\n2\n\n3\n\n3\n\n4 exec ( VALIDATE_CODE_TEMPLATE )\n\nexec\n\n( VALIDATE_CODE_TEMPLATE )\n\n4\n\nFigure 9. Python Program to Check Valid Code.\n\nFigure 9. Python Program to Check Valid Code.\n\nPython Program to Check Valid Code.\n\nFigure 9.\n\n22\n\n22\n\n22",
        "tables": [
          {
            "text": "",
            "csv": "",
            "json": "{\n  \"headers\": [],\n  \"rows\": []\n}"
          },
          {
            "text": "ModelData CurationBase ModelOat-7B (Liu et al.,\n2025)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-MathSimpleRL-Zoo (Zeng et al.,\n2025b)8.5k math pairs (Hendrycks et al.,\n2021)Qwen2.5-7B-BaseOpenReasonerZero (Hu et al.,\n2025)57k STEM + math samplesQwen2.5-7B-BasePRIME-Zero (Cui et al.,\n2025)457k math + 27k code problemsQwen2.5Math-7B-BaseCodeR1-Zero-7B-LC2k-1088 (Liu &amp; Zhang,\n2025)2k Leetcode pairsQwen2.5-7B-Instruct-1MCodeR1-Zero-7B-12k-832 (Liu &amp; Zhang,\n2025)2k Leetcode + 10k TACO pairs (Li et al.,\n2023)Qwen2.5-7B-Instruct-1MAceCoder-7B-Ins-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Ins-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-InstructAceCoder-7B-Code-RM (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderAceCoder-7B-Code-Rule (Zeng et al.,\n2025a)22k code dataQwen2.5-7B-CoderQwen-7B-Instruct (Yang et al.,\n2024a)1M SFT + 150k RL pairsQwen2.5-7B-BaseAZR-7B (Ours)No dataQwen2.5-7B-BaseAZR-7B-Coder (Ours)No dataQwen2.5-7B-Coder",
            "csv": "",
            "json": "{\n  \"headers\": [],\n  \"rows\": []\n}"
          },
          {
            "text": "loggingrandommultiprocessingpebblesubprocessthreadingdatetimetimehashlibcalendarbcryptos.sysos.pathsys.exitos.environ",
            "csv": "",
            "json": "{\n  \"headers\": [],\n  \"rows\": []\n}"
          }
        ]
      },
      {
        "page_num": 22,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n1 EVAL_INPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n2 { gold_output } == f({ agent_input }) \"\"\"\n3\n4 exec ( EVAL_INPUT_PREDICTION_TEMPLATE )\n\n1 EVAL_INPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n\nEVAL_INPUT_PREDICTION_TEMPLATE =\n\n\"\"\"\n\n{\n\ncode\n\n}\n\n1\n\n2 { gold_output } == f({ agent_input }) \"\"\"\n\n{\n\ngold_output\n\n}\n\n==\n\nf({\n\nagent_input\n\n})\n\n\"\"\"\n\n2\n\n3\n\n3\n\n4 exec ( EVAL_INPUT_PREDICTION_TEMPLATE )\n\nexec\n\n( EVAL_INPUT_PREDICTION_TEMPLATE )\n\n4\n\nFigure 10. Python Code to Check Agent Input Abduction Correctness.\n\nFigure 10. Python Code to Check Agent Input Abduction Correctness.\n\nPython Code to Check Agent Input Abduction Correctness.\n\nFigure 10.\n\n1 EVAL_OUTPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n2 eval ({ gold_output }) == eval ({ agent_output }) \"\"\"\n3\n4 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\n1 EVAL_OUTPUT_PREDICTION_TEMPLATE = \"\"\" { code }\n\nEVAL_OUTPUT_PREDICTION_TEMPLATE =\n\n\"\"\"\n\n{\n\ncode\n\n}\n\n1\n\n2 eval ({ gold_output }) == eval ({ agent_output }) \"\"\"\n\neval\n\n({\n\ngold_output\n\n})\n\n==\n\neval\n\n({\n\nagent_output\n\n})\n\n\"\"\"\n\n2\n\n3\n\n3\n\n4 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\nexec\n\n( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\n4\n\nFigure 11. Python Code to Check Agent Output Deduction Correctness.\n\nFigure 11. Python Code to Check Agent Output Deduction Correctness.\n\nPython Code to Check Agent Output Deduction Correctness.\n\nFigure 11.\n\n1 EVAL_FUNCTION_PREDICTION_TEMPLATE = \"\"\"{ code }\n2 matches = []\n3 for gold_input , gold_output in zip ({ gold_inputs } , { gold_outputs }):\n4 match = { gold_output } == f({ gold_input })\n5 matches . append ( match )\n6 \"\"\"\n7\n8 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\n1 EVAL_FUNCTION_PREDICTION_TEMPLATE = \"\"\"{ code }\n\nEVAL_FUNCTION_PREDICTION_TEMPLATE =\n\n\"\"\"{\n\ncode\n\n}\n\n1\n\n2 matches = []\n\nmatches\n\n=\n\n[]\n\n2\n\n3 for gold_input , gold_output in zip ({ gold_inputs } , { gold_outputs }):\n\nfor\n\ngold_input\n\n,\n\ngold_output\n\nin\n\nzip\n\n({\n\ngold_inputs\n\n} ,\n\n{\n\ngold_outputs\n\n}):\n\n3\n\n4 match = { gold_output } == f({ gold_input })\n\nmatch\n\n=\n\n{\n\ngold_output\n\n}\n\n==\n\nf({\n\ngold_input\n\n})\n\n4\n\n5 matches . append ( match )\n\nmatches\n\n.\n\nappend\n\n(\n\nmatch\n\n)\n\n5\n\n6 \"\"\"\n\n\"\"\"\n\n6\n\n7\n\n7\n\n8 exec ( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\nexec\n\n( EVAL_OUTPUT_PREDICTION_TEMPLATE )\n\n8\n\nFigure 12. Python Code to Check Agent Function Induction Correctness.\n\nFigure 12. Python Code to Check Agent Function Induction Correctness.\n\nPython Code to Check Agent Function Induction Correctness.\n\nFigure 12.\n\n1 CHECK_DETERMINISM_TEMPLATE = \"\"\"{ code }\n2 returns = f({ inputs })\n3 if returns != f({ inputs }):\n4 raise Exception ( 'Non - deterministic code ')\n5 repr ( returns ) \"\"\"\n6\n7 exec ( CHECK_DETERMINISM_TEMPLATE )\n\n1 CHECK_DETERMINISM_TEMPLATE = \"\"\"{ code }\n\nCHECK_DETERMINISM_TEMPLATE =\n\n\"\"\"{\n\ncode\n\n}\n\n1\n\n2 returns = f({ inputs })\n\nreturns\n\n=\n\nf({\n\ninputs\n\n})\n\n2\n\n3 if returns != f({ inputs }):\n\nif\n\nreturns\n\n!=\n\nf({\n\ninputs\n\n}):\n\n3\n\n4 raise Exception ( 'Non - deterministic code ')\n\nraise\n\nException\n\n( 'Non\n\n-\n\ndeterministic\n\ncode\n\n')\n\n4\n\n5 repr ( returns ) \"\"\"\n\nrepr\n\n(\n\nreturns\n\n)\n\n\"\"\"\n\n5\n\n6\n\n6\n\n7 exec ( CHECK_DETERMINISM_TEMPLATE )\n\nexec\n\n( CHECK_DETERMINISM_TEMPLATE )\n\n7\n\n0 30 60 90 120 150 180 210 240 270\nTraining Steps\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nPerformance Score\n\nCruxEval-I\nCruxEval-O\nLiveCodeBench-Execution\nFigure 13. Python Code to Check Deterministic Program.\n\nFigure 13. Python Code to Check Deterministic Program.\n\nFigure 13. Python Code to Check Deterministic Program.\n\nPython Code to Check Deterministic Program.\n\nFigure 13.\n\n0 30 60 90 120 150 180 210 240 270\nTraining Steps\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nPerformance Score\n\nCruxEval-I\nCruxEval-O\nLiveCodeBench-Execution\n\n0.65\n\n0.65\n\n0.60\n\n0.60\n\nPerformance Score\n\nPerformance Score\n\n0.55\n\n0.55\n\n0.50\n\n0.50\n\n0.45\n\n0.45\n\nCruxEval-I\n\nCruxEval-I\n\n0.40\n\n0.40\n\nCruxEval-O\n\nCruxEval-O\n\nLiveCodeBench-Execution\n\nLiveCodeBench-Execution\n\n0.35\n\n0.35\n\n0 30 60 90 120 150 180 210 240 270\n\n0\n\n30\n\n60\n\n90\n\n120\n\n150\n\n180\n\n210\n\n240\n\n270\n\nTraining Steps\n\nTraining Steps\n\nFigure 14. In-distribution Benchmark Score During Training. The evolution of CruxEval-I, CruxEval-O, and LiveCodeBench-\nExecution during training for the Qwen2.5-7B base model trained using AZR.\n\nFigure 14. In-distribution Benchmark Score During Training. The evolution of CruxEval-I, CruxEval-O, and LiveCodeBench-\n\nThe evolution of CruxEval-I, CruxEval-O, and LiveCodeBench-\n\nIn-distribution Benchmark Score During Training.\n\nFigure 14.\n\nExecution during training for the Qwen2.5-7B base model trained using AZR.\n\nExecution during training for the\n\nbase model trained using AZR.\n\nQwen2.5-7B\n\n23\n\n23\n\n23",
        "tables": []
      },
      {
        "page_num": 23,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\nReward\n\n0 40 80 120 160 200 240 280\n1000\n2000\n3000\n4000\nToken Length\nTraining Steps Abduction Task\nSolve Propose\nFigure 15. Abduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for abduction task type of\nAbsolute Zero Reasoner-base-7b.\n\n0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\nReward\n\n0 40 80 120 160 200 240 280\n1000\n2000\n3000\n4000\nToken Length\nTraining Steps Abduction Task\nSolve Propose\n\nSolve Propose\n\nSolve\n\nPropose\n\n0.8\n\n0.8\n\n4000\n\n4000\n\n0.6\n\n0.6\n\nToken Length\n\nToken Length\n\n3000\n\n3000\n\nReward\n\nReward\n\n0.4\n\n0.4\n\n2000\n\n2000\n\n0.2\n\n0.2\n\n1000\n\n1000\n\n0.0\n\n0.0\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\nTraining Steps Abduction Task\n\nTraining Steps\n\nAbduction Task\n\nFigure 15. Abduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for abduction task type of\nAbsolute Zero Reasoner-base-7b.\n\nFigure 15. Abduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for abduction task type of\n\nThe task reward and token lengths of the two roles for abduction task type of\n\nAbduction Task Reward and Token Lengths.\n\nFigure 15.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\n0 40 80 120 160 200 240 280\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nReward\n\n0 40 80 120 160 200 240 280\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nToken Length\nTraining Steps Induction Task\nSolve Propose\nFigure 16. Induction Task Reward and Token Lengths. The task reward and token lengths of the two roles for induction task type of\nAbsolute Zero Reasoner-base-7b.\n\n0 40 80 120 160 200 240 280\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nReward\n\n0 40 80 120 160 200 240 280\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nToken Length\nTraining Steps Induction Task\nSolve Propose\n\nSolve Propose\n\nSolve\n\nPropose\n\n0.7\n\n0.7\n\n2000\n\n2000\n\n0.6\n\n0.6\n\n1750\n\n1750\n\n0.5\n\n0.5\n\n1500\n\n1500\n\nToken Length\n\nToken Length\n\nReward\n\nReward\n\n0.4\n\n0.4\n\n1250\n\n1250\n\n0.3\n\n0.3\n\n1000\n\n1000\n\n0.2\n\n0.2\n\n750\n\n750\n\n0.1\n\n0.1\n\n500\n\n500\n\n250\n\n250\n\n0.0\n\n0.0\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\nTraining Steps Induction Task\n\nTraining Steps\n\nInduction Task\n\nFigure 16. Induction Task Reward and Token Lengths. The task reward and token lengths of the two roles for induction task type of\nAbsolute Zero Reasoner-base-7b.\n\nFigure 16. Induction Task Reward and Token Lengths. The task reward and token lengths of the two roles for induction task type of\n\nThe task reward and token lengths of the two roles for induction task type of\n\nInduction Task Reward and Token Lengths.\n\nFigure 16.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\n24\n\n24\n\n24",
        "tables": []
      },
      {
        "page_num": 24,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n\n0 40 80 120 160 200 240 280\n400\n600\n800\n1000\n1200\n1400\nToken Length\nTraining Steps Deduction Task\nSolve Propose\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\n\n0 40 80 120 160 200 240 280\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nReward\n\n0 40 80 120 160 200 240 280\n400\n600\n800\n1000\n1200\n1400\nToken Length\nTraining Steps Deduction Task\nSolve Propose\n\nSolve Propose\n\nSolve\n\nPropose\n\n1.0\n\n1.0\n\n1400\n\n1400\n\n1200\n\n1200\n\n0.8\n\n0.8\n\n1000\n\n1000\n\nToken Length\n\nToken Length\n\n0.6\n\n0.6\n\nReward\n\nReward\n\n800\n\n800\n\n0.4\n\n0.4\n\n600\n\n600\n\n0.2\n\n0.2\n\n400\n\n400\n\n0.0\n\n0.0\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\n0 40 80 120 160 200 240 280\n\n0\n\n40\n\n80\n\n120\n\n160\n\n200\n\n240\n\n280\n\nTraining Steps Deduction Task\n\nTraining Steps\n\nDeduction Task\n\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\nAbsolute Zero Reasoner-base-7b.\n\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\n\nThe task reward and token lengths of the two roles for deduction task type of\n\nDeduction Task Reward and Token Lengths.\n\nFigure 17.\n\nFigure 17. Deduction Task Reward and Token Lengths. The task reward and token lengths of the two roles for deduction task type of\n\nFigure 17.\n\nThe task reward and token lengths of the two roles for deduction task type of\n\nDeduction Task Reward and Token Lengths.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\nAbsolute Zero Reasoner-base-7b.\n\nTask: Propose Deduction Task\nModel Input: (k random selected example programs as few-shot input)\nModel Output:\nThis task requires creating a new Python function that takes an input and returns a value. The function should have at least one\ninput parameter, be deterministic, and require state tracking across multiple data transformations. The input should be a test case\nprovided to a subject for the purpose of deducing the output. The code and the output should be challenging enough to require\nalgorithmic reasoning without using restricted words or packages.\ndef f(prices: list[int], budget: int):\nn = len(prices)\nprofit = [0] * n\nfor i in range(1, n):\nfor j in range(i):\nif prices[i] > prices[j]:\nprofit[i] = max(profit[i], profit[j] + prices[i] - prices[j])\nreturn max(profit + [profit[-1] - prices[-1] if budget >= prices[-1] else 0])\nInput: [7, 1, 5, 3, 6, 4], 7\n\nTask: Propose Deduction Task\n\nTask: Propose Deduction Task\n\nModel Input: (k random selected example programs as few-shot input)\n\n(k random selected example programs as few-shot input)\n\nModel Input:\n\nModel Output:\n\nModel Output:\n\nThis task requires creating a new Python function that takes an input and returns a value. The function should have at least one\n\nThis task requires creating a new Python function that takes an input and returns a value. The function should have at least one\n\ninput parameter, be deterministic, and require state tracking across multiple data transformations. The input should be a test case\n\ninput parameter, be deterministic, and require state tracking across multiple data transformations. The input should be a test case\n\nprovided to a subject for the purpose of deducing the output. The code and the output should be challenging enough to require\n\nprovided to a subject for the purpose of deducing the output. The code and the output should be challenging enough to require\n\nalgorithmic reasoning without using restricted words or packages.\n\nalgorithmic reasoning without using restricted words or packages.\n\ndef f(prices: list[int], budget: int):\n\ndef\n\nf(prices:\n\nlist[int], budget:\n\nint):\n\nn = len(prices)\n\nn\n\n=\n\nlen(prices)\n\nprofit = [0] * n\n\nprofit\n\n=\n\n[0]\n\n*\n\nn\n\nfor i in range(1, n):\n\nfor\n\nin\n\ni\n\nrange(1, n):\n\nfor j in range(i):\n\nfor\n\nin\n\nj\n\nrange(i):\n\nif prices[i] > prices[j]:\n\nif\n\nprices[i]\n\n>\n\nprices[j]:\n\nprofit[i] = max(profit[i], profit[j] + prices[i] - prices[j])\n\nprofit[i]\n\n=\n\nmax(profit[i], profit[j]\n\n+\n\nprices[i]\n\n-\n\nprices[j])\n\nreturn max(profit + [profit[-1] - prices[-1] if budget >= prices[-1] else 0])\n\nreturn\n\nif\n\nelse\n\nmax(profit\n\n+\n\n[profit[-1]\n\n-\n\nprices[-1]\n\nbudget\n\n>=\n\nprices[-1]\n\n0])\n\nInput: [7, 1, 5, 3, 6, 4], 7\n\nInput:\n\n[7, 1, 5, 3, 6, 4], 7\n\nFigure 18. An example response of proposing a deduction task. k programs are selected as few-shot examples to the model. The generated\nnew program will then be verifed through execution.\nFigure 18. Propose Deduction Task Example. An example response of proposing a deduction task. k programs are selected as few-shot\nexamples to the model. The generated new program will then be verified through execution.\n\nFigure 18. Propose Deduction Task Example. An example response of proposing a deduction task. k programs are selected as few-shot\n\nAn example response of proposing a deduction task. k programs are selected as few-shot\n\nPropose Deduction Task Example.\n\nFigure 18.\n\nFigure 18. An example response of proposing a deduction task. k programs are selected as few-shot examples to the model. The generated\n\nFigure 18.\n\nAn example response of proposing a deduction task. k programs are selected as few-shot examples to the model. The generated\n\nexamples to the model. The generated new program will then be verified through execution.\n\nexamples to the model. The generated new program will then be verified through execution.\n\nnew program will then be verifed through execution.\n\nnew program will then be verifed through execution.\n\n24\n25\n\n25\n\n25\n\n24\n\n24",
        "tables": []
      },
      {
        "page_num": 25,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\ndef f(numbers):\n# Step 1: Filter out even numbers\nfiltered_numbers = [num for num in numbers if num % 2 != 0]\n# Step 2: Calculate the sum of the remaining odd numbers\nsum_of_odd_numbers = sum(filtered_numbers)\n# Step 3: Reverse the order of the remaining odd numbers\nreversed_odd_numbers = filtered_numbers[::-1]\n# Step 4: Calculate the product of the reversed odd numbers\nproduct_of_reversed_odd_numbers = 1\nfor num in reversed_odd_numbers:\nproduct_of_reversed_odd_numbers *= num\n# Step 5: Calculate the sum of the digits of the product\nsum_of_digits_of_product = sum(int(digit) for digit in str(product_of_reversed_odd_numbers))\n# Step 6: Modify the original list by adding the sum of the digits to each even number\n# and subtracting it from each odd number\nmodified_numbers = []\nfor num in numbers:\nif num % 2 == 0:\nmodified_numbers.append(num + sum_of_digits_of_product)\nelse:\nmodified_numbers.append(num - sum_of_digits_of_product)\n# Step 7: Return the modified list\nreturn modified_numbers\n\ndef f(numbers):\n\ndef\n\nf(numbers):\n\n# Step 1: Filter out even numbers\n\n# Step 1: Filter out even numbers\n\nfiltered_numbers = [num for num in numbers if num % 2 != 0]\n\nfor\n\nin\n\nif\n\nfiltered_numbers\n\n=\n\n[num\n\nnum\n\nnumbers\n\nnum\n\n%\n\n2\n\n!=\n\n0]\n\n# Step 2: Calculate the sum of the remaining odd numbers\n\n# Step 2: Calculate the sum of the remaining odd numbers\n\nsum_of_odd_numbers = sum(filtered_numbers)\n\nsum_of_odd_numbers\n\n=\n\nsum(filtered_numbers)\n\n# Step 3: Reverse the order of the remaining odd numbers\n\n# Step 3: Reverse the order of the remaining odd numbers\n\nreversed_odd_numbers = filtered_numbers[::-1]\n\nreversed_odd_numbers\n\n=\n\nfiltered_numbers[::-1]\n\n# Step 4: Calculate the product of the reversed odd numbers\n\n# Step 4: Calculate the product of the reversed odd numbers\n\nproduct_of_reversed_odd_numbers = 1\n\nproduct_of_reversed_odd_numbers\n\n=\n\n1\n\nfor num in reversed_odd_numbers:\n\nfor\n\nin\n\nnum\n\nreversed_odd_numbers:\n\nproduct_of_reversed_odd_numbers *= num\n\nproduct_of_reversed_odd_numbers\n\n*=\n\nnum\n\n# Step 5: Calculate the sum of the digits of the product\n\n# Step 5: Calculate the sum of the digits of the product\n\nsum_of_digits_of_product = sum(int(digit) for digit in str(product_of_reversed_odd_numbers))\n\nfor\n\nin\n\nsum_of_digits_of_product\n\n=\n\nsum(int(digit)\n\ndigit\n\nstr(product_of_reversed_odd_numbers))\n\n# Step 6: Modify the original list by adding the sum of the digits to each even number\n\n# Step 6: Modify the original list by adding the sum of the digits to each even number\n\n# and subtracting it from each odd number\n\n# and subtracting it from each odd number\n\nmodified_numbers = []\n\nmodified_numbers\n\n=\n\n[]\n\nfor num in numbers:\n\nfor\n\nin\n\nnum\n\nnumbers:\n\nif num % 2 == 0:\n\nif\n\nnum\n\n%\n\n2\n\n==\n\n0:\n\nmodified_numbers.append(num + sum_of_digits_of_product)\n\nmodified_numbers.append(num\n\n+\n\nsum_of_digits_of_product)\n\nelse:\n\nelse\n\n:\n\nmodified_numbers.append(num - sum_of_digits_of_product)\n\nmodified_numbers.append(num\n\n-\n\nsum_of_digits_of_product)\n\n# Step 7: Return the modified list\n\n# Step 7: Return the modified list\n\nreturn modified_numbers\n\nreturn\n\nmodified_numbers\n\nFigure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed\nin much bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).\nFigure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed in\nmuch bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).\n\nFigure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\n\nWe observe that even after outputting the CoT and during answering when the\n\nLearned \"ReAct\" style program generation.\n\nFigure 19.\n\nFigure 19. Learned \"ReAct\" style program generation. We observe that even after outputting the CoT and during answering when the\n\nFigure 19.\n\nWe observe that even after outputting the CoT and during answering when the\n\nLearned \"ReAct\" style program generation.\n\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\n\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\n\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\n\nanswer requires outputting code, the model usually uses comments in code to interleave its thinking and \"acting\", an example is shown\n\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed in\n\nhere for induction task output by\n\nduring training. Similar observations are recently observed in\n\nAbsolute Zero Reasoner-Coder-14b\n\nhere for induction task output by Absolute Zero Reasoner-Coder-14b during training. Similar observations are recently observed\n\nhere for induction task output by\n\nduring training. Similar observations are recently observed\n\nAbsolute Zero Reasoner-Coder-14b\n\nmuch bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).\n\nmuch bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al.,\n\n2025)\n\n.\n\nin much bigger models (671b) for math proofs with DeepSeek Prover V2 (Ren et al., 2025).\n\nin much bigger models (671b) for math proofs with DeepSeek Prover\n\nV2 (Ren et a\n\nl.,\n\n2025).\n\n25\n26\n\n26\n\n26\n\n25\n\n25",
        "tables": []
      },
      {
        "page_num": 26,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Propose Abduction Task\n\nTask: Propose Abduction Task\n\nTask: Propose Abduction Task\n\nModel Input: (k random selected example programs as few-shot input)\n\nModel Input: (k random selected example programs as few-shot input)\n\n(k random selected example programs as few-shot input)\n\nModel Input:\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nFor this task, we need an intelligent problem that requires deep algorithmic reasoning and demands long multi step reasoning. A\ntypical type of problem that fts these requirements is the Matrix Reconstruction from a given jumbled li st of elements, where\nwe have to fgure out the matrix's dimensions and individual values. However, this task will involve multiple steps, increasing its\ncomplexity and creativity. We will create a function 'f' that transforms a jumbled list of elements into a two-dimensional matrix,\nthen back to a list, given a specifc reconstruction pattern. We will retrieve the original list from the reconstructed matrix. Here,\nwe will avoid random and datetime operations to ensure determinism. The function will be structured to reconstruct the matrix\nelements in a unique and challenging way. We will provide an input that, after reconstruction, presents a unique list that can be\neasily deduced from the given output in a test piece.\n\nFor this task, we need an intelligent problem that requires deep algorithmic reasoning and demands long multi step reasoning. A\n\nFor this task, we need an intelligent problem that requires deep algorithmic reasoning and demands long multi step reasoning. A\n\ntypical type of problem that fts these requirements is the Matrix Reconstruction from a given jumbled li st of elements, where\n\ntypical type of problem that fts these requirements is the Matrix Reconstruction from a given jumbled li st of elements, where\n\nwe have to fgure out the matrix's dimensions and individual values. However, this task will involve multiple steps, increasing its\n\nwe have to fgure out the matrix's dimensions and individual values. However, this task will involve multiple steps, increasing its\n\ncomplexity and creativity. We will create a function 'f' that transforms a jumbled list of elements into a two-dimensional matrix,\n\ncomplexity and creativity. We will create a function 'f' that transforms a jumbled list of elements into a two-dimensional matrix,\n\nthen back to a list, given a specifc reconstruction pattern. We will retrieve the original list from the reconstructed matrix. Here,\n\nthen back to a list, given a specifc reconstruction pattern. We will retrieve the original list from the reconstructed matrix. Here,\n\nwe will avoid random and datetime operations to ensure determinism. The function will be structured to reconstruct the matrix\n\nwe will avoid random and datetime operations to ensure determinism. The function will be structured to reconstruct the matrix\n\nelements in a unique and challenging way. We will provide an input that, after reconstruction, presents a unique list that can be\n\nelements in a unique and challenging way. We will provide an input that, after reconstruction, presents a unique list that can be\n\neasily deduced from the given output in a test piece.\n\neasily deduced from the given output in a test piece.\n\ndef f(jumbled_elements: list) -> list:\nif not jumbled_elements:\nreturn []\n# Calculate the square root of the length of elements to estimate the matrix dimensions\nn = len(jumbled_elements)\nsqrt_n = int(n ** 0.5)\nif n > sqrt_n ** 2:\nsqrt_n += 1\nif n < sqrt_n ** 2:\nsqrt_n -= 1\n# Now, we need to reconstruct the matrix elements in a challenging way using specific patterns\n# For this, we can reconstruct the matrix using zigzag patterns or hill-climbing patterns\n# Here, a hill-climbing pattern is used where we traverse the matrix from the extreme\n# right and downward Then traverse the matrix from the second to last element to the\n# extreme bottom. Repeat the process until the entire matrix is traversed.\n# Initialize the matrix\nmatrix = [[0 for _ in range(sqrt_n)] for _ in range(sqrt_n)]\ncur_idx = 0\n# Fill the matrix in a zigzag pattern\nfor i in range(sqrt_n):\nif i % 2 == 0:\nfor j in range(sqrt_n):\nmatrix[j][i] = j * sqrt_n + i + cur_idx\ncur_idx += 1\nelse:\nfor j in range(sqrt_n - 1, -1, -1):\nmatrix[j][i] = j * sqrt_n + n - i - cur_idx - 1\ncur_idx += 1\n# Reconstruct the list into a matrix by repeating the process\nreconstructed_list = []\nfor i in range(sqrt_n):\nfor j in range(sqrt_n):\nreconstructed_list.append(matrix[i][j])\n# Now, we need to calculate another matrix based on a function F applied to the current matrix\n# F is a mathematical function that can be complex, such as factorial, prime check,\n# or another transformation For example, transform each element by adding 1 if it's prime,\n# subtracting 1 if it's even, or leaving it unchanged, Implement any specific transformation\n# function here. Here, for demonstration, each element is left unchanged\ntransformed_result = reconstructed_list\nreturn transformed_result\nOutput: [0, 11, 10, 1, 5, 16, 15, 6, 10, 21, 20, 11, 15, 26, 25, 16]\n\ndef f(jumbled_elements: list) -> list:\n\ndef\n\nf(jumbled_elements:\n\nlist)\n\n->\n\nlist:\n\nif not jumbled_elements:\n\nif\n\nnot\n\njumbled_elements:\n\nreturn []\n\nreturn\n\n[]\n\n# Calculate the square root of the length of elements to estimate the matrix dimensions\n\n# Calculate the square root of the length of elements to estimate the matrix dimensions\n\nn = len(jumbled_elements)\n\nn\n\n=\n\nlen(jumbled_elements)\n\nsqrt_n = int(n ** 0.5)\n\nsqrt_n\n\n=\n\nint(n\n\n**\n\n0.5)\n\nif n > sqrt_n ** 2:\n\nif\n\nn\n\n>\n\nsqrt_n\n\n**\n\n2:\n\nsqrt_n += 1\n\nsqrt_n\n\n+=\n\n1\n\nif n < sqrt_n ** 2:\n\nif\n\nn\n\n<\n\nsqrt_n\n\n**\n\n2:\n\nsqrt_n -= 1\n\nsqrt_n\n\n-=\n\n1\n\n# Now, we need to reconstruct the matrix elements in a challenging way using specific patterns\n\n# Now, we need to reconstruct the matrix elements in a challenging way using specific patterns\n\n# For this, we can reconstruct the matrix using zigzag patterns or hill-climbing patterns\n\n# For this, we can reconstruct the matrix using zigzag patterns or hill-climbing patterns\n\n# Here, a hill-climbing pattern is used where we traverse the matrix from the extreme\n\n# Here, a hill-climbing pattern is used where we traverse the matrix from the extreme\n\n# right and downward Then traverse the matrix from the second to last element to the\n\n# right and downward Then traverse the matrix from the second to last element to the\n\n# extreme bottom. Repeat the process until the entire matrix is traversed.\n\n# extreme bottom. Repeat the process until the entire matrix is traversed.\n\n# Initialize the matrix\n\n# Initialize the matrix\n\nmatrix = [[0 for _ in range(sqrt_n)] for _ in range(sqrt_n)]\n\nfor\n\nin\n\nfor\n\nin\n\nmatrix\n\n=\n\n[[0\n\n_\n\nrange(sqrt_n)]\n\n_\n\nrange(sqrt_n)]\n\ncur_idx = 0\n\ncur_idx\n\n=\n\n0\n\n# Fill the matrix in a zigzag pattern\n\n# Fill the matrix in a zigzag pattern\n\nfor i in range(sqrt_n):\n\nfor\n\nin\n\ni\n\nrange(sqrt_n):\n\nif i % 2 == 0:\n\nif\n\ni\n\n%\n\n2\n\n==\n\n0:\n\nfor j in range(sqrt_n):\n\nfor\n\nin\n\nj\n\nrange(sqrt_n):\n\nmatrix[j][i] = j * sqrt_n + i + cur_idx\n\nmatrix[j][i]\n\n=\n\nj\n\n*\n\nsqrt_n\n\n+\n\ni\n\n+\n\ncur_idx\n\ncur_idx += 1\n\ncur_idx\n\n+=\n\n1\n\nelse:\n\nelse\n\n:\n\nfor j in range(sqrt_n - 1, -1, -1):\n\nfor\n\nin\n\nj\n\nrange(sqrt_n\n\n-\n\n1,\n\n-1,\n\n-1):\n\nmatrix[j][i] = j * sqrt_n + n - i - cur_idx - 1\n\nmatrix[j][i]\n\n=\n\nj\n\n*\n\nsqrt_n\n\n+\n\nn\n\n-\n\ni\n\n-\n\ncur_idx\n\n-\n\n1\n\ncur_idx += 1\n\ncur_idx\n\n+=\n\n1\n\n# Reconstruct the list into a matrix by repeating the process\n\n# Reconstruct the list into a matrix by repeating the process\n\nreconstructed_list = []\n\nreconstructed_list\n\n=\n\n[]\n\nfor i in range(sqrt_n):\n\nfor\n\nin\n\ni\n\nrange(sqrt_n):\n\nfor j in range(sqrt_n):\n\nfor\n\nin\n\nj\n\nrange(sqrt_n):\n\nreconstructed_list.append(matrix[i][j])\n\nreconstructed_list.append(matrix[i][j])\n\n# Now, we need to calculate another matrix based on a function F applied to the current matrix\n\n# Now, we need to calculate another matrix based on a function F applied to the current matrix\n\n# F is a mathematical function that can be complex, such as factorial, prime check,\n\n# F is a mathematical function that can be complex, such as factorial, prime check,\n\n# or another transformation For example, transform each element by adding 1 if it's prime,\n\n# or another transformation For example, transform each element by adding 1 if it's prime,\n\n# subtracting 1 if it's even, or leaving it unchanged, Implement any specific transformation\n\n# subtracting 1 if it's even, or leaving it unchanged, Implement any specific transformation\n\n# function here. Here, for demonstration, each element is left unchanged\n\n# function here. Here, for demonstration, each element is left unchanged\n\ntransformed_result = reconstructed_list\n\ntransformed_result\n\n=\n\nreconstructed_list\n\nreturn transformed_result\n\nreturn\n\ntransformed_result\n\nOutput: [0, 11, 10, 1, 5, 16, 15, 6, 10, 21, 20, 11, 15, 26, 25, 16]\n\nOutput:\n\n[0, 11, 10, 1, 5, 16, 15, 6, 10, 21, 20, 11, 15, 26, 25, 16]\n\nFigure 20. An example response of proposing an abduction task. The model uses comments as reasoning when generating the program.\n26\nFigure 20. Propose Abduction Task Example. An example response of proposing an abduction task. The model uses comments as\nreasoning when generating the program.\n\nFigure 20. An example response of proposing an abduction task. The model uses comments as reasoning when generating the program.\n\nFigure 20.\n\nAn example response of proposing an abduction task. The model uses comments as reasoning when generating the program.\n\nFigure 20. Propose Abduction Task Example. An example response of proposing an abduction task. The model uses comments as\n\nAn example response of proposing an abduction task. The model uses comments as\n\nPropose Abduction Task Example.\n\nFigure 20.\n\nreasoning when generating the program.\n\nreasoning when generating the program.\n\n26\n\n26\n\n27\n\n27\n\n27",
        "tables": []
      },
      {
        "page_num": 27,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Propose Induction Task\n\nTask: Propose Induction Task\n\nTask: Propose Induction Task\n\nModel Input: Here is the function f. Analyze its behaviour and relate it to the observed inputs/outputs.\n\nModel Input: Here is the function f. Analyze its behaviour and relate it to the observed inputs/outputs.\n\nHere is the function\n\n. Analyze its behaviour and relate it to the observed inputs/outputs.\n\nModel Input:\n\nf\n\ndef f(numbers: list[int]) -> int:\nreplacement = {\n0: 2,\n1: 3,\n2: 1,\n3: 0,\n4: 4\n}\nreturn sum(replacement[num] for num in numbers)\n\ndef f(numbers: list[int]) -> int:\n\ndef\n\nf(numbers:\n\nlist[int])\n\n->\n\nint:\n\nreplacement = {\n\nreplacement\n\n=\n\n{\n\n0: 2,\n\n0:\n\n2,\n\n1: 3,\n\n1:\n\n3,\n\n2: 1,\n\n2:\n\n1,\n\n3: 0,\n\n3:\n\n0,\n\n4: 4\n\n4:\n\n4\n\n}\n\n}\n\nreturn sum(replacement[num] for num in numbers)\n\nreturn\n\nfor\n\nin\n\nsum(replacement[num]\n\nnum\n\nnumbers)\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nTo solve this task, we need to come up with 10 diferent inputs for the function f(numbers: list[int]) -> int. The function\nreplaces each number in the list according to a specifc dictionary and then sums up the replaced numbers. Let's analyze the\ndictionary:\n\nTo solve this task, we need to come up with 10 diferent inputs for the function f(numbers: list[int]) -> int. The function\n\nTo solve this task, we need to come up with 10 diferent inputs for the function\n\n. The function\n\nf(numbers: list[int]) -> int\n\nreplaces each number in the list according to a specifc dictionary and then sums up the replaced numbers. Let's analyze the\n\nreplaces each number in the list according to a specifc dictionary and then sums up the replaced numbers. Let's analyze the\n\ndictionary:\n\ndictionary:\n\nThis dictionary tells us that:\n\nThis dictionary tells us that:\n\nThis dictionary tells us that:\n\n-0 becomes 2\n\n-0 becomes 2\n\n0\n\n2\n\n-\n\nbecomes\n\n-1 becomes 3\n\n-1 becomes 3\n\n1\n\n3\n\n-\n\nbecomes\n\n-2 becomes 1\n\n-2 becomes 1\n\n2\n\n1\n\n-\n\nbecomes\n\n-3 becomes 0\n\n-3 becomes 0\n\n3\n\n0\n\n-\n\nbecomes\n\n-4 remains 4\n\n-4 remains 4\n\n4\n\n4\n\n-\n\nremains\n\nTo create diverse outputs, we should include lists with diferent sets of numbers from the list {0, 1, 2, 3, 4}. We also need\nto ensure that the inputs are valid (i.e., lists of integers).\n\nTo create diverse outputs, we should include lists with diferent sets of numbers from the list {0, 1, 2, 3, 4}. We also need\n\n0\n\n1\n\n2\n\n3\n\n4\n\nTo create diverse outputs, we should include lists with diferent sets of numbers from the list\n\n. We also need\n\n,\n\n,\n\n,\n\n,\n\n{\n\n}\n\nto ensure that the inputs are valid (i.e., lists of integers).\n\nto ensure that the inputs are valid (i.e., lists of integers).\n\nLet's illustrate using provided examples step-by-step:\n\nLet's illustrate using provided examples step-by-step:\n\nLet's illustrate using provided examples step-by-step:\n\n1. Input: [0] \u21d2 Replacement: [2] \u21d2 sum = 2\n2. Input: [1, 2, 3] \u21d2 Replacement: [3, 1, 0] \u21d2 sum = 4\n3. Input: [4, 1] \u21d2 Replacement: [4, 3] \u21d2 sum = 7\n4. Input: [2, 1, 0] \u21d2 Replacement: [1, 3, 2] \u21d2 sum = 6\n5. Input: [3, 0, 2, 4] \u21d2 Replacement: [0, 2, 1, 4] \u21d2 sum = 7\n6. Input: [1, 1, 1] \u21d2 Replacement: [3, 3, 3] \u21d2 sum = 9\n7. Input: [2, 2, 0, 1] \u21d2 Replacement: [1, 1, 2, 3] \u21d2 sum = 7\n8. Input: [3, 4, 2] \u21d2 Replacement: [0, 4, 1] \u21d2 sum = 5\n9. Input: [0, 1, 2, 3, 4] \u21d2 Replacement: [2, 3, 1, 0, 4] \u21d2 sum = 10\n10. Input: [1, 2, 3, 0, 4] \u21d2 Replacement: [3, 1, 0, 2, 4] \u21d2 sum = 10\nThus, the rule is clearly replacing numbers according to the given mapping and then summing the results.\nInputs: ['[0]', '[1, 2, 3]', '[4, 1]', '[2, 1, 0]', '[3, 0, 2, 4]', '[1, 1, 1]',\n[2, 2, 0, 1], '[3, 4, 2]', '[0, 1, 2, 3, 4]', '[1, 2, 3, 0, 4]']|\nMessage: You are given a function that takes a list of integers as input and returns a single integer as output. Can you determine\nthe rule this function is following by analyzing the outputs for the given inputs?\n(After execution:) Outputs: ['2', '4', '7', '6', '7', '9', '7', '5', '10', '10']\n\n1. Input: [0] \u21d2 Replacement: [2] \u21d2 sum = 2\n\n[2]\n\n= 2\n\n1. Input:\n\nReplacement:\n\nsum\n\n[0]\n\n\u21d2\n\n\u21d2\n\n2. Input: [1, 2, 3] \u21d2 Replacement: [3, 1, 0] \u21d2 sum = 4\n\n[3\n\n1\n\n0]\n\n= 4\n\n2. Input:\n\nReplacement:\n\nsum\n\n[1, 2, 3]\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n3. Input: [4, 1] \u21d2 Replacement: [4, 3] \u21d2 sum = 7\n\n[4\n\n3]\n\n= 7\n\n3. Input:\n\nReplacement:\n\nsum\n\n[4, 1]\n\n,\n\n\u21d2\n\n\u21d2\n\n4. Input: [2, 1, 0] \u21d2 Replacement: [1, 3, 2] \u21d2 sum = 6\n\n[1\n\n3\n\n2]\n\n= 6\n\n4. Input:\n\nReplacement:\n\nsum\n\n[2, 1, 0]\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n5. Input: [3, 0, 2, 4] \u21d2 Replacement: [0, 2, 1, 4] \u21d2 sum = 7\n\n[0\n\n2\n\n1\n\n4]\n\n= 7\n\n5. Input:\n\nReplacement:\n\nsum\n\n[3, 0, 2, 4]\n\n,\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n6. Input: [1, 1, 1] \u21d2 Replacement: [3, 3, 3] \u21d2 sum = 9\n\n[3\n\n3\n\n3]\n\n= 9\n\n6. Input:\n\nReplacement:\n\nsum\n\n[1, 1, 1]\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n7. Input: [2, 2, 0, 1] \u21d2 Replacement: [1, 1, 2, 3] \u21d2 sum = 7\n\n[1\n\n1\n\n2\n\n3]\n\n= 7\n\n7. Input:\n\nReplacement:\n\nsum\n\n[2, 2, 0, 1]\n\n,\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n8. Input: [3, 4, 2] \u21d2 Replacement: [0, 4, 1] \u21d2 sum = 5\n\n[0\n\n4\n\n1]\n\n= 5\n\n8. Input:\n\nReplacement:\n\nsum\n\n[3, 4, 2]\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n9. Input: [0, 1, 2, 3, 4] \u21d2 Replacement: [2, 3, 1, 0, 4] \u21d2 sum = 10\n\n[2\n\n3\n\n1\n\n0\n\n4]\n\n= 10\n\n9. Input:\n\nReplacement:\n\nsum\n\n[0, 1, 2, 3, 4]\n\n,\n\n,\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\n10. Input: [1, 2, 3, 0, 4] \u21d2 Replacement: [3, 1, 0, 2, 4] \u21d2 sum = 10\n\n[3\n\n1\n\n0\n\n2\n\n4]\n\n= 10\n\n10. Input:\n\nReplacement:\n\nsum\n\n[1, 2, 3, 0, 4]\n\n,\n\n,\n\n,\n\n,\n\n\u21d2\n\n\u21d2\n\nThus, the rule is clearly replacing numbers according to the given mapping and then summing the results.\n\nThus, the rule is clearly replacing numbers according to the given mapping and then summing the results.\n\nInputs: ['[0]', '[1, 2, 3]', '[4, 1]', '[2, 1, 0]', '[3, 0, 2, 4]', '[1, 1, 1]',\n\n:\n\nInputs\n\n['[0]', '[1, 2, 3]', '[4, 1]', '[2, 1, 0]', '[3, 0, 2, 4]', '[1, 1, 1]',\n\n[2, 2, 0, 1], '[3, 4, 2]', '[0, 1, 2, 3, 4]', '[1, 2, 3, 0, 4]']|\n\n, '[3, 4, 2]', '[0, 1, 2, 3, 4]', '[1, 2, 3, 0, 4]']|\n\n[2, 2, 0, 1]\n\nMessage: You are given a function that takes a list of integers as input and returns a single integer as output. Can you determine\n\n: You are given a function that takes a list of integers as input and returns a single integer as output. Can you determine\n\nMessage\n\nthe rule this function is following by analyzing the outputs for the given inputs?\n\nthe rule this function is following by analyzing the outputs for the given inputs?\n\n(After execution:) Outputs: ['2', '4', '7', '6', '7', '9', '7', '5', '10', '10']\n\n(After execution:)\n\n:\n\nOutputs\n\n['2', '4', '7', '6', '7', '9', '7', '5', '10', '10']\n\nFigure 21. An example response of proposing an induction task. Given a random selected program, the model needs to propose 10 inputs\nand a description of program. These inputs will be executed and verifed to get outputs. At solve phase, the model need to generate the\nprogram given the message and 5 input/output pairs (half is hidden cases)\nFigure 21. Propose Induction Task Example. An example response of proposing an induction task. Given a random selected program,\nthe model needs to propose 10 inputs and a description of program. These inputs will be executed and verified to get outputs. At solve\nphase, the model need to generate the program given the message and 5 input/output pairs (half is hidden cases)\n\nFigure 21. An example response of proposing an induction task. Given a random selected program, the model needs to propose 10 inputs\n\nFigure 21.\n\nAn example response of proposing an induction task. Given a random selected program, the model needs to propose 10 inputs\n\nFigure 21. Propose Induction Task Example. An example response of proposing an induction task. Given a random selected program,\n\nAn example response of proposing an induction task. Given a random selected program,\n\nPropose Induction Task Example.\n\nFigure 21.\n\nand a description of program. These inputs will be executed and verifed to get outputs. At solve phase, the model need to generate the\n\nand a description of program. These inputs will be executed and verifed to get outputs. At solve phase, the model need to generate the\n\nthe model needs to propose 10 inputs and a description of program. These inputs will be executed and verified to get outputs. At solve\n\nthe model needs to propose 10 inputs and a description of program. These inputs will be executed and verified to get outputs. At solve\n\nprogram given the message and 5 input/output pairs (half is hidden cases)\n\nprogram given the message and 5 input/output pairs (half is hidden cases)\n\nphase, the model need to generate the program given the message and 5 input/output pairs (half is hidden cases)\n\nphase, the model need to generate the program given the message and 5 input/output pairs (half is hidden cases)\n\n27\n28\n\n27\n\n27\n\n28\n\n28",
        "tables": []
      },
      {
        "page_num": 28,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Solve Abduction Task\n\nTask: Solve Abduction Task\n\nTask: Solve Abduction Task\n\nModel Input: Here is the function f and the output: '1', please predict the input.\n\nModel Input: Here is the function f and the output: '1', please predict the input.\n\nHere is the function\n\nand the output: '1', please predict the input.\n\nModel Input:\n\nf\n\ndef f(numbers: list[int], target: int) -> int:\naccumulated_values = []\ncurrent_sum = 0\nfor number in numbers:\ncurrent_sum += number\naccumulated_values.append(current_sum)\nresult = 0\nfor i in range(len(accumulated_values)):\nfor j in range(i + 1, len(accumulated_values)):\nif accumulated_values[j] \\\n- accumulated_values[i] == target:\nresult = accumulated_values[i]\nreturn result\nreturn result\n\ndef f(numbers: list[int], target: int) -> int:\n\ndef\n\nf(numbers:\n\nlist[int], target:\n\nint)\n\n->\n\nint:\n\naccumulated_values = []\n\naccumulated_values\n\n=\n\n[]\n\ncurrent_sum = 0\n\ncurrent_sum\n\n=\n\n0\n\nfor number in numbers:\n\nfor\n\nin\n\nnumber\n\nnumbers:\n\ncurrent_sum += number\n\ncurrent_sum\n\n+=\n\nnumber\n\naccumulated_values.append(current_sum)\n\naccumulated_values.append(current_sum)\n\nresult = 0\n\nresult\n\n=\n\n0\n\nfor i in range(len(accumulated_values)):\n\nfor\n\nin\n\ni\n\nrange(len(accumulated_values)):\n\nfor j in range(i + 1, len(accumulated_values)):\n\nfor\n\nin\n\nj\n\nrange(i\n\n+\n\n1,\n\nlen(accumulated_values)):\n\nif accumulated_values[j] \\\n\nif\n\naccumulated_values[j] \\\n\n- accumulated_values[i] == target:\n\n-\n\naccumulated_values[i]\n\n==\n\ntarget:\n\nresult = accumulated_values[i]\n\nresult\n\n=\n\naccumulated_values[i]\n\nreturn result\n\nreturn\n\nresult\n\nreturn result\n\nreturn\n\nresult\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\nsatisfes this condition is 1.\nLet's consider a simple example:\n-numbers = [1, 2]\n-target = 1\nHere's the step-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\nConsider:\n-numbers = [1, 0]\n-target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n...(shortened)\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n-numbers = [2, -1, 1]\n-target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nsatisfes this condition is 1.\n\nsatisfes this condition is 1.\n\nLet's consider a simple example:\n\nLet's consider a simple example:\n\n-numbers = [1, 2]\n\n-numbers = [1, 2]\n\n-target = 1\n\n-target = 1\n\nHere's the step-by-step calculation:\n\nHere's the step-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nConsider:\n\nConsider:\n\n-numbers = [1, 0]\n\n-numbers = [1, 0]\n\n-target = 1\n\n-target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n...(shortened)\n\n...(shortened)\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\n-numbers = [2, -1, 1]\n\n-numbers = [2, -1, 1]\n\n-target = 1\n\n-target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nFigure 22. A more detailed version of the example shown in main Figure 7. The model is solving an abduction task (predict input).\nFigure 22. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\nabduction task (predict input).\n\nFigure 22. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\n\nA more detailed version of the example shown in main Figure\n\n7. The model is solving an\n\nFigure 22. A more detailed version of the example shown in main Figure 7. The model is solving an abduction task (predict input).\n\nFigure 22.\n\nA more detailed version of the example shown in main Figure\n\n7. The model is solving an abducti\n\non\n\ntask (predict input).\n\nSolve Abduction Task Example.\n\nFigure 22.\n\nabduction task (predict input).\n\nabduction task (predict input).\n\n28\n29\n\n28\n\n28\n\n29\n\n29",
        "tables": []
      },
      {
        "page_num": 29,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Solve Deduction Task (Example 1)\n\nTask: Solve Deduction Task (Example 1)\n\nTask: Solve Deduction Task (Example 1)\n\nModel Input: Here is the function f, the input provided [1, 2, 1, 2, 3, 2, 1], 2, and we need to\npredict the output.\n\nModel Input: Here is the function f, the input provided [1, 2, 1, 2, 3, 2, 1], 2, and we need to\n\nHere is the function\n\n, the input provided\n\n, and we need to\n\nModel Input:\n\nf\n\n[1, 2, 1, 2, 3, 2, 1], 2\n\npredict the output.\n\npredict the output.\n\ndef f(array: list[int], k: int) -> int:\nwindow_start = 0\nmax_length = 0\nchar_freq = {}\nfor window_end in range(len(array)):\nend_char = array[window_end]\nif end_char not in char_freq:\nchar_freq[end_char] = 0\nchar_freq[end_char] += 1\nwhile len(char_freq) > k:\nstart_char = array[window_start]\nchar_freq[start_char] -= 1\nif char_freq[start_char] == 0:\ndel char_freq[start_char]\nwindow_start += 1\nmax_length = max(max_length, window_end - window_start + 1)\nreturn max_length\nInput: [1, 2, 1, 2, 3, 2, 1], 2\nModel Output:\nBased on the given code snippet, we need to determine the output of the function f(array, k) when provided with the input [1, 2, 1, 2, 3, 2, 1] and k = 2.\nThe function f is designed to fnd the length of the longest subarray with at most k distinct characters. Let's go through the code step by step:\n1. Initialize window_start to 0, max_length to 0, and an empty dictionary char_freq to store the frequency of characters in the current window.\n2. Iterate through the array using window_end as the index:\na. Get the current character end_char at index window_end.\nb. If end_char is not in char_freq, add it with a frequency of 0.\nc. Increment the frequency of end_char by 1.\n3. Check if the number of distinct characters in char_freq is greater than k:\na. If true, shrink the window from the start:\ni. Get the character start_char at index window_start.\nii. Decrement its frequency by 1.\niii. If the frequency becomes 0, remove the character from char_freq.\niv. Increment window_start by 1.\n4. Update max_length by taking the maximum of max_length and the current window size (window_end - window_start + 1).\nNow, let's apply this algorithm step-by-step to the input [1, 2, 1, 2, 3, 2, 1], k = 2:\n\u2022 window_start = 0, max_length = 0, char_freq = {}\n\u2022 window_end = 0, add 1 to char_freq: {1:1}, window size = 1\n\u2022 window_end = 1, add 2: {1:1, 2:1}, window size = 2\n\u2022 window_end = 2, increment 1: {1:2, 2:1}, window size = 3\n\u2022 window_end = 3, increment 2: {1:2, 2:2}, window size = 4\n\u2022 window_end = 4, add 3: {1:2, 2:2, 3:1}; now distinct chars > 2, shrink window:\n- remove 1 at start: {1:1, 2:2, 3:1}\n- remove 2 at start: {1:1, 2:1, 3:1}\n- remove 1 again: {2:1, 3:1}, window now starts at 3\n\u2022 window_end = 5, increment 2: {2:2, 3:1}, window size = 3\n\u2022 window_end = 6, add 1: {2:2, 3:1, 1:1}, shrink again:\n- remove 2: {2:1, 3:1, 1:1}\n- remove 3: {2:1, 1:1}, window now starts at 5\n- window size = 2\nMaximum window size encountered is 4, thus max_length = 4.\nHence, the function returns 4.\n\ndef f(array: list[int], k: int) -> int:\n\ndef\n\nf(array:\n\nlist[int], k:\n\nint)\n\n->\n\nint:\n\nwindow_start = 0\n\nwindow_start\n\n=\n\n0\n\nmax_length = 0\n\nmax_length\n\n=\n\n0\n\nchar_freq = {}\n\nchar_freq\n\n=\n\n{}\n\nfor window_end in range(len(array)):\n\nfor\n\nin\n\nwindow_end\n\nrange(len(array)):\n\nend_char = array[window_end]\n\nend_char\n\n=\n\narray[window_end]\n\nif end_char not in char_freq:\n\nif\n\nnot\n\nin\n\nend_char\n\nchar_freq:\n\nchar_freq[end_char] = 0\n\nchar_freq[end_char]\n\n=\n\n0\n\nchar_freq[end_char] += 1\n\nchar_freq[end_char]\n\n+=\n\n1\n\nwhile len(char_freq) > k:\n\nwhile\n\nlen(char_freq)\n\n>\n\nk:\n\nstart_char = array[window_start]\n\n=\n\narray[window_start]\n\nstart_char\n\nchar_freq[start_char] -= 1\n\nchar_freq[start_char]\n\n-=\n\n1\n\nif char_freq[start_char] == 0:\n\nif\n\nchar_freq[start_char]\n\n==\n\n0:\n\ndel char_freq[start_char]\n\ndel\n\nchar_freq[start_char]\n\nwindow_start += 1\n\nwindow_start\n\n+=\n\n1\n\nmax_length = max(max_length, window_end - window_start + 1)\n\nmax_length\n\n=\n\nmax(max_length, window_end\n\n-\n\nwindow_start\n\n+\n\n1)\n\nreturn max_length\n\nreturn\n\nmax_length\n\nInput: [1, 2, 1, 2, 3, 2, 1], 2\n\nInput:\n\n[1, 2, 1, 2, 3, 2, 1], 2\n\nModel Output:\n\nModel Output:\n\nBased on the given code snippet, we need to determine the output of the function f(array, k) when provided with the input [1, 2, 1, 2, 3, 2, 1] and k = 2.\n\n= 2\n\nBased on the given code snippet, we need to determine the output of the function\n\nwhen provided with the input\n\nand\n\n.\n\nf(array, k)\n\n[1, 2, 1, 2, 3, 2, 1]\n\nk\n\nThe function f is designed to fnd the length of the longest subarray with at most k distinct characters. Let's go through the code step by step:\n\nThe function\n\nis designed to fnd the length of the longest subarray with at most\n\ndistinct characters. Let's go through the code step by step:\n\nf\n\nk\n\n1. Initialize window_start to 0, max_length to 0, and an empty dictionary char_freq to store the frequency of characters in the current window.\n\n1. Initialize\n\nto 0,\n\nto 0, and an empty dictionary\n\nto store the frequency of characters in the current window.\n\nwindow_start\n\nmax_length\n\nchar_freq\n\n2. Iterate through the array using window_end as the index:\n\n2. Iterate through the array using\n\nas the index:\n\nwindow_end\n\na. Get the current character end_char at index window_end.\n\na. Get the current character\n\nat index\n\n.\n\nend_char\n\nwindow_end\n\nb. If end_char is not in char_freq, add it with a frequency of 0.\n\nb. If\n\nis not in\n\n, add it with a frequency of 0.\n\nend_char\n\nchar_freq\n\nc. Increment the frequency of end_char by 1.\n\nc. Increment the frequency of\n\nby 1.\n\nend_char\n\n3. Check if the number of distinct characters in char_freq is greater than k:\n\n3. Check if the number of distinct characters in\n\nis greater than\n\n:\n\nchar_freq\n\nk\n\na. If true, shrink the window from the start:\n\na. If true, shrink the window from the start:\n\ni. Get the character start_char at index window_start.\n\ni. Get the character\n\nat index\n\n.\n\nstart_char\n\nwindow_start\n\nii. Decrement its frequency by 1.\n\nii. Decrement its frequency by 1.\n\niii. If the frequency becomes 0, remove the character from char_freq.\n\niii. If the frequency becomes 0, remove the character from\n\n.\n\nchar_freq\n\niv. Increment window_start by 1.\n\niv. Increment\n\nby 1.\n\nwindow_start\n\n4. Update max_length by taking the maximum of max_length and the current window size (window_end - window_start + 1).\n\n4. Update\n\nby taking the maximum of\n\nand the current window size (\n\n).\n\nmax_length\n\nmax_length\n\nwindow_end - window_start + 1\n\nNow, let's apply this algorithm step-by-step to the input [1, 2, 1, 2, 3, 2, 1], k = 2:\n\n= 2\n\nNow, let's apply this algorithm step-by-step to the input\n\n,\n\n:\n\n[1, 2, 1, 2, 3, 2, 1]\n\nk\n\n\u2022 window_start = 0, max_length = 0, char_freq = {}\n\n\u2022\n\n,\n\n,\n\nwindow_start = 0\n\nmax_length = 0\n\nchar_freq = {}\n\n\u2022 window_end = 0, add 1 to char_freq: {1:1}, window size = 1\n\n1\n\n1:1\n\n= 1\n\n\u2022\n\n, add\n\nto\n\n:\n\n, window size\n\nwindow_end = 0\n\nchar_freq\n\n{\n\n}\n\n\u2022 window_end = 1, add 2: {1:1, 2:1}, window size = 2\n\n2\n\n1:1\n\n2:1\n\n= 2\n\n\u2022\n\n, add\n\n:\n\n, window size\n\nwindow_end = 1\n\n,\n\n{\n\n}\n\n\u2022 window_end = 2, increment 1: {1:2, 2:1}, window size = 3\n\n1\n\n1:2\n\n2:1\n\n= 3\n\n\u2022\n\n, increment\n\n:\n\n, window size\n\nwindow_end = 2\n\n,\n\n{\n\n}\n\n\u2022 window_end = 3, increment 2: {1:2, 2:2}, window size = 4\n\n2\n\n1:2\n\n2:2\n\n= 4\n\n\u2022\n\n, increment\n\n:\n\n, window size\n\nwindow_end = 3\n\n,\n\n{\n\n}\n\n\u2022 window_end = 4, add 3: {1:2, 2:2, 3:1}; now distinct chars > 2, shrink window:\n\n3\n\n1:2\n\n2:2\n\n3:1\n\n2\n\n\u2022\n\n, add\n\n:\n\n; now distinct chars\n\n, shrink window:\n\nwindow_end = 4\n\n,\n\n,\n\n>\n\n{\n\n}\n\n- remove 1 at start: {1:1, 2:2, 3:1}\n\n1\n\n1:1\n\n2:2\n\n3:1\n\n- remove\n\nat start:\n\n,\n\n,\n\n{\n\n}\n\n- remove 2 at start: {1:1, 2:1, 3:1}\n\n2\n\n1:1\n\n2:1\n\n3:1\n\n- remove\n\nat start:\n\n,\n\n,\n\n{\n\n}\n\n- remove 1 again: {2:1, 3:1}, window now starts at 3\n\n1\n\n2:1\n\n3:1\n\n3\n\n- remove\n\nagain:\n\n, window now starts at\n\n,\n\n{\n\n}\n\n\u2022 window_end = 5, increment 2: {2:2, 3:1}, window size = 3\n\n2\n\n2:2\n\n3:1\n\n= 3\n\n\u2022\n\n, increment\n\n:\n\n, window size\n\nwindow_end = 5\n\n,\n\n{\n\n}\n\n\u2022 window_end = 6, add 1: {2:2, 3:1, 1:1}, shrink again:\n\n1\n\n2:2\n\n3:1\n\n1:1\n\n\u2022\n\n, add\n\n:\n\n, shrink again:\n\nwindow_end = 6\n\n,\n\n,\n\n{\n\n}\n\n- remove 2: {2:1, 3:1, 1:1}\n\n2\n\n2:1\n\n3:1\n\n1:1\n\n- remove\n\n:\n\n,\n\n,\n\n{\n\n}\n\n- remove 3: {2:1, 1:1}, window now starts at 5\n\n3\n\n2:1\n\n1:1\n\n5\n\n- remove\n\n:\n\n, window now starts at\n\n,\n\n{\n\n}\n\n- window size = 2\n\n= 2\n\n- window size\n\nMaximum window size encountered is 4, thus max_length = 4.\n\n4\n\nMaximum window size encountered is\n\n, thus\n\n.\n\nmax_length = 4\n\nHence, the function returns 4.\n\nHence, the function returns\n\n.\n\n4\n\nFigure 23. Example of solving the deduction task. Given a program and input, the model simulates the execution of the program until\nthe fnal result.\nFigure 23. Solve Deduction Task Example. Example of solving the deduction task. Given a program and input, the model simulates the\nexecution of the program until the final result.\n\nFigure 23. Solve Deduction Task Example. Example of solving the deduction task. Given a program and input, the model simulates the\n\nExample of solving the deduction task. Given a program and input, the model simulates the\n\nFigure 23. Example of solving the deduction task. Given a program and input, the model simulates the execution of the program until\n\nFigure 23.\n\nExample of solving the deduction task. Given a program and input, the model simulates the execution of the program until\n\nSolve Deduction Task Example.\n\nFigure 23.\n\nexecution of the program until the final result.\n\nexecution of the program until the final result.\n\nthe fnal result.\n\nthe fnal result.\n\n29\n30\n\n29\n\n29\n\n30\n\n30",
        "tables": []
      },
      {
        "page_num": 30,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Solve Induction Task\n\nTask: Solve Induction Task\n\nTask: Solve Induction Task\n\nModel Input: Given the inputs and outputs, deduce the code snippet implementing the described function-\nality:\nInput Description: Given a list of integers, sort the list and then for each element at index i, multiply it by\n(n \u2212 i), where n is the total number of elements. Compute the fnal sum.\nInputs: ['[1, 2, 3, 4]', '[4, 3, 2, 1]', '[3, 1, 4, 2]', '[1, 2, 2, 3]', '[5]']\nOutputs: ['20', '20', '20', '17', '5']\n# Golden\ndef f(elements: list[int]) -> int:\nn = len(elements)\nsorted_elements = sorted(elements)\nmodified_elements = [sorted_elements[i] * (n - i) for i in range(n)]\nreturn sum(modified_elements)\nModel Output:\nGiven the problem and the input/output pairs, we need to deduce a general code snippet that takes a sorted list and a list of multipliers, and then computes a fnal sum based on the multipliers.\nLet's break down the problem step by step:\n1. **Understand the Message:**\n- We are given a list of integers.\n- The list is sorted.\n- For each element at index i in the sorted list, we multiply it by (n \u2212 i), where n is the number of elements in the list.\n- We need to compute the sum of these products.\n2. **Analyze the Input/Output Pairs:**\n- For the input [1, 2, 3, 4], the sorted form is the same, and the output is 20.\n1 \u00b7 (4 \u2212 1) = 3, 2 \u00b7 (4 \u2212 2) = 4, 3 \u00b7 (4 \u2212 3) = 3, 4 \u00b7 (4 \u2212 4) = 0, Sum: 3 + 4 + 3 + 0 = 10\n- For the input [4, 3, 2, 1], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\n- For the input [3, 1, 4, 2], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\nshortened\n3. **Re-evaluate the Outputs:**\n- It seems there might be an error in the provided outputs for some inputs. However, based on the problem description, the outputs should be correct if we consider the sorted list and the multipliers\ncorrectly.\n4. **Deduce the Function:**\n- Given the sorted list and the list of multipliers, we can deduce the following function:\n\"'python\ndef f(a):\nsorted_a = sorted(a)\nn = len(sorted_a)\nresult = 0\nfor i in range(n):\nresult += sorted_a[i] * (n - i)\nreturn result\n...(shortened)\n5. **Verify the Function:**\n- Let's verify the function with the provided inputs:\n- For [1, 2, 3, 4]:\n- Sorted list: [1, 2, 3, 4]\n- Multipliers: [3, 4, 3, 0]\n- Sum: 1 \u00b7 3+2 \u00b7 4+3 \u00b7 3+4 \u00b7 0 = 3 + 8 + 9 + 0 = 20\n...(shortened)\nThe corrected outputs should be: For [1, 2, 3, 4]: 20, For [4, 3, 2, 1]: 20, For [3, 1, 4, 2]: 20, For [1, 2, 2, 3]: 13, For [5]: 0\nGiven the function:\ndef f(a):\nsorted_a = sorted(a)\nn = len(sorted_a)\nresult = 0\nfor i in range(n):\nresult += sorted_a[i] * (n - i)\nreturn result\nThe outputs match the corrected sum.\n### Final Answer\n(repeat the program above)\n\nModel Input: Given the inputs and outputs, deduce the code snippet implementing the described function-\n\nGiven the inputs and outputs, deduce the code snippet implementing the described function-\n\nModel Input:\n\nality:\n\nality:\n\nInput Description: Given a list of integers, sort the list and then for each element at index i, multiply it by\n\nGiven a list of integers, sort the list and then for each element at index\n\n, multiply it by\n\nInput Description:\n\ni\n\n(n \u2212 i), where n is the total number of elements. Compute the fnal sum.\n\n(\n\n)\n\n, where\n\nis the total number of elements. Compute the fnal sum.\n\nn\n\ni\n\nn\n\n\u2212\n\nInputs: ['[1, 2, 3, 4]', '[4, 3, 2, 1]', '[3, 1, 4, 2]', '[1, 2, 2, 3]', '[5]']\n\nInputs:\n\n['[1, 2, 3, 4]', '[4, 3, 2, 1]', '[3, 1, 4, 2]', '[1, 2, 2, 3]', '[5]']\n\nOutputs: ['20', '20', '20', '17', '5']\n\nOutputs:\n\n['20', '20', '20', '17', '5']\n\n# Golden\n\n# Golden\n\ndef f(elements: list[int]) -> int:\n\ndef\n\nf(elements:\n\nlist[int])\n\n->\n\nint:\n\nn = len(elements)\n\nn\n\n=\n\nlen(elements)\n\nsorted_elements = sorted(elements)\n\nsorted_elements\n\n=\n\nsorted(elements)\n\nmodified_elements = [sorted_elements[i] * (n - i) for i in range(n)]\n\nfor\n\nin\n\nmodified_elements\n\n=\n\n[sorted_elements[i]\n\n*\n\n(n\n\n-\n\ni)\n\ni\n\nrange(n)]\n\nreturn sum(modified_elements)\n\nreturn\n\nsum(modified_elements)\n\nModel Output:\n\nModel Output:\n\nGiven the problem and the input/output pairs, we need to deduce a general code snippet that takes a sorted list and a list of multipliers, and then computes a fnal sum based on the multipliers.\n\nGiven the problem and the input/output pairs, we need to deduce a general code snippet that takes a sorted list and a list of multipliers, and then computes a fnal sum based on the multipliers.\n\nLet's break down the problem step by step:\n\nLet's break down the problem step by step:\n\n1. **Understand the Message:**\n\n1. **Understand the Message:**\n\n- We are given a list of integers.\n\n- We are given a list of integers.\n\n- The list is sorted.\n\n- The list is sorted.\n\n- For each element at index i in the sorted list, we multiply it by (n \u2212 i), where n is the number of elements in the list.\n\n(\n\n)\n\n- For each element at index\n\nin the sorted list, we multiply it by\n\n, where\n\nis the number of elements in the list.\n\ni\n\nn\n\ni\n\nn\n\n\u2212\n\n- We need to compute the sum of these products.\n\n- We need to compute the sum of these products.\n\n2. **Analyze the Input/Output Pairs:**\n\n2. **Analyze the Input/Output Pairs:**\n\n- For the input [1, 2, 3, 4], the sorted form is the same, and the output is 20.\n\n[1\n\n2\n\n3\n\n4]\n\n- For the input\n\n, the sorted form is the same, and the output is 20.\n\n,\n\n,\n\n,\n\n1 \u00b7 (4 \u2212 1) = 3, 2 \u00b7 (4 \u2212 2) = 4, 3 \u00b7 (4 \u2212 3) = 3, 4 \u00b7 (4 \u2212 4) = 0, Sum: 3 + 4 + 3 + 0 = 10\n\n1\n\n(4\n\n1) = 3\n\n2\n\n(4\n\n2) = 4\n\n3\n\n(4\n\n3) = 3\n\n4\n\n(4\n\n4) = 0\n\n3 + 4 + 3 + 0 = 10\n\n,\n\n,\n\n,\n\n, Sum:\n\n\u00b7\n\n\u2212\n\n\u00b7\n\n\u2212\n\n\u00b7\n\n\u2212\n\n\u00b7\n\n\u2212\n\n- For the input [4, 3, 2, 1], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\n\n[4\n\n3\n\n2\n\n1]\n\n[1\n\n2\n\n3\n\n4]\n\n- For the input\n\n, the sorted form is\n\n, and the output is 20 (same as above).\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n- For the input [3, 1, 4, 2], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\n\n[3\n\n1\n\n4\n\n2]\n\n[1\n\n2\n\n3\n\n4]\n\n- For the input\n\n, the sorted form is\n\n, and the output is 20 (same as above).\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nshortened\n\nshortened\n\n3. **Re-evaluate the Outputs:**\n\n3. **Re-evaluate the Outputs:**\n\n- It seems there might be an error in the provided outputs for some inputs. However, based on the problem description, the outputs should be correct if we consider the sorted list and the multipliers\n\n- It seems there might be an error in the provided outputs for some inputs. However, based on the problem description, the outputs should be correct if we consider the sorted list and the multipliers\n\ncorrectly.\n\ncorrectly.\n\n4. **Deduce the Function:**\n\n4. **Deduce the Function:**\n\n- Given the sorted list and the list of multipliers, we can deduce the following function:\n\n- Given the sorted list and the list of multipliers, we can deduce the following function:\n\n\"'python\n\n\"'python\n\ndef f(a):\n\ndef f(a):\n\nsorted_a = sorted(a)\n\nsorted_a = sorted(a)\n\nn = len(sorted_a)\n\nn = len(sorted_a)\n\nresult = 0\n\nresult = 0\n\nfor i in range(n):\n\nfor i in range(n):\n\nresult += sorted_a[i] * (n - i)\n\nresult += sorted_a[i] * (n - i)\n\nreturn result\n\nreturn result\n\n...(shortened)\n\n...(shortened)\n\n5. **Verify the Function:**\n\n5. **Verify the Function:**\n\n- Let's verify the function with the provided inputs:\n\n- Let's verify the function with the provided inputs:\n\n- For [1, 2, 3, 4]:\n\n[1\n\n2\n\n3\n\n4]\n\n- For\n\n:\n\n,\n\n,\n\n,\n\n- Sorted list: [1, 2, 3, 4]\n\n[1\n\n2\n\n3\n\n4]\n\n- Sorted list:\n\n,\n\n,\n\n,\n\n- Multipliers: [3, 4, 3, 0]\n\n[3\n\n4\n\n3\n\n0]\n\n- Multipliers:\n\n,\n\n,\n\n,\n\n- Sum: 1 \u00b7 3+2 \u00b7 4+3 \u00b7 3+4 \u00b7 0 = 3 + 8 + 9 + 0 = 20\n\n1\n\n3+2\n\n4+3\n\n3+4\n\n0 = 3 + 8 + 9 + 0 = 20\n\n- Sum:\n\n\u00b7\n\n\u00b7\n\n\u00b7\n\n\u00b7\n\n...(shortened)\n\n...(shortened)\n\nThe corrected outputs should be: For [1, 2, 3, 4]: 20, For [4, 3, 2, 1]: 20, For [3, 1, 4, 2]: 20, For [1, 2, 2, 3]: 13, For [5]: 0\n\n[1\n\n2\n\n3\n\n4]\n\n[4\n\n3\n\n2\n\n1]\n\n[3\n\n1\n\n4\n\n2]\n\n[1\n\n2\n\n2\n\n3]\n\n[5]\n\nThe corrected outputs should be: For\n\n: 20, For\n\n: 20, For\n\n: 20, For\n\n: 13, For\n\n: 0\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\n,\n\nGiven the function:\n\nGiven the function:\n\ndef f(a):\n\ndef\n\nf(a):\n\nsorted_a = sorted(a)\n\nsorted_a\n\n=\n\nsorted(a)\n\nn = len(sorted_a)\n\nn\n\n=\n\nlen(sorted_a)\n\nresult = 0\n\nresult\n\n=\n\n0\n\nfor i in range(n):\n\nfor\n\nin\n\ni\n\nrange(n):\n\nresult += sorted_a[i] * (n - i)\n\nresult\n\n+=\n\nsorted_a[i]\n\n*\n\n(n\n\n-\n\ni)\n\nreturn result\n\nreturn\n\nresult\n\nThe outputs match the corrected sum.\n\nThe outputs match the corrected sum.\n\n### Final Answer\n\n### Final Answer\n\n(repeat the program above)\n\n(repeat the program above)\n\nFigure 24. Example of solving an induction task. The model is given a program description, and 5 input/output cases and predict the\nfunction. Interestingly, after the model gives the function, it will go through the given use cases one by one and confrm all test cases are\npassed. 30\nFigure 24. Solve Induction Task Example. Example of solving an induction task. The model is given a program description, and 5\ninput/output cases and predict the function. Interestingly, after the model gives the function, it will go through the given use cases one by\none and confirm all test cases are passed.\n\nFigure 24. Solve Induction Task Example. Example of solving an induction task. The model is given a program description, and 5\n\nExample of solving an induction task. The model is given a program description, and 5\n\nFigure 24. Example of solving an induction task. The model is given a program description, and 5 input/output cases and predict the\n\nFigure 24.\n\nExample of solving an induction task. The model is given a program description, and 5 input/output cases and predict the\n\nSolve Induction Task Example.\n\nFigure 24.\n\ninput/output cases and predict the function. Interestingly, after the model gives the function, it will go through the given use cases one by\n\ninput/output cases and predict the function. Interestingly, after the model gives the function, it will go through the given use cases one by\n\nfunction. Interestingly, after the model gives the function, it will go through the given use cases one by one and confrm all test cases are\n\nfunction. Interestingly, after the model gives the function, it will go through the given use cases one by one and confrm all test cases are\n\none and confirm all test cases are passed.\n\none and confirm all test cases are passed.\n\npassed. 30\n\npassed.\n\n30\n\n31\n\n31\n\n31",
        "tables": []
      },
      {
        "page_num": 31,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nModelHEval+MBPP+LCBv1-5AIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBenchLlama3.1-8B31.753.70.00.00.02.510.65.52.1+ Simple-RL-Zoo38.455.37.40.00.07.522.28.84.7+ AZR35.450.88.53.30.05.013.214.05.0Qwen2.5-3B-Coder67.165.920.03.33.320.051.018.416.6+ AZR71.369.024.43.33.337.562.026.127.0Qwen2.5-14B-Coder76.871.731.40.00.037.554.810.718.5+ AZR80.571.239.023.320.065.078.632.039.3Qwen2.5-14B-Base78.066.721.76.73.335.066.228.332.4+ AZR70.768.835.210.020.062.576.240.442.5\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nModelHEval+MBPP+LCBv1-5AIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBenchLlama3.1-8B31.753.70.00.00.02.510.65.52.1+ Simple-RL-Zoo38.455.37.40.00.07.522.28.84.7+ AZR35.450.88.53.30.05.013.214.05.0Qwen2.5-3B-Coder67.165.920.03.33.320.051.018.416.6+ AZR71.369.024.43.33.337.562.026.127.0Qwen2.5-14B-Coder76.871.731.40.00.037.554.810.718.5+ AZR80.571.239.023.320.065.078.632.039.3Qwen2.5-14B-Base78.066.721.76.73.335.066.228.332.4+ AZR70.768.835.210.020.062.576.240.442.5\n\nModel\n\nHEval+\n\nMBPP+\n\nLCBv1-5\n\nAIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBench\n\nLlama3.1-8B\n\n31.7\n\n53.7\n\n0.0\n\n0.0\n\n0.0\n\n2.5\n\n10.6\n\n5.5\n\n2.1\n\n+ Simple-RL-Zoo\n\n38.4\n\n55.3\n\n7.4\n\n0.0\n\n0.0\n\n7.5\n\n22.2\n\n8.8\n\n4.7\n\n+ AZR\n\n35.4\n\n50.8\n\n8.5\n\n3.3\n\n0.0\n\n5.0\n\n13.2\n\n14.0\n\n5.0\n\nQwen2.5-3B-Coder\n\n67.1\n\n65.9\n\n20.0\n\n3.3\n\n3.3\n\n20.0\n\n51.0\n\n18.4\n\n16.6\n\n+ AZR\n\n71.3\n\n69.0\n\n24.4\n\n3.3\n\n3.3\n\n37.5\n\n62.0\n\n26.1\n\n27.0\n\nQwen2.5-14B-Coder\n\n76.8\n\n71.7\n\n31.4\n\n0.0\n\n0.0\n\n37.5\n\n54.8\n\n10.7\n\n18.5\n\n+ AZR\n\n80.5\n\n71.2\n\n39.0\n\n23.3\n\n20.0\n\n65.0\n\n78.6\n\n32.0\n\n39.3\n\nQwen2.5-14B-Base\n\n78.0\n\n66.7\n\n21.7\n\n6.7\n\n3.3\n\n35.0\n\n66.2\n\n28.3\n\n32.4\n\n+ AZR\n\n70.7\n\n68.8\n\n35.2\n\n10.0\n\n20.0\n\n62.5\n\n76.2\n\n40.4\n\n42.5\n\nTable 5. Detailed Breakdown of Evaluation Benchmarks for Other Model Sizes and Types. Full evaluation of AZR trained on\nother models on three standard code benchmarks (HEval+\n, MBPP+\n, LCBv1-5) and six math benchmarks (AIME'24, AIME'25, AMC'23,\nMATH500, Minerva, OlympiadBench).\n\nTable 5. Detailed Breakdown of Evaluation Benchmarks for Other Model Sizes and Types. Full evaluation of AZR trained on\n\nFull evaluation of AZR trained on\n\nDetailed Breakdown of Evaluation Benchmarks for Other Model Sizes and Types.\n\nTable 5.\n\nother models on three standard code benchmarks (HEval+\n\n+\n\n, MBPP+\n\n+\n\n, LCBv1-5) and six math benchmarks (AIME'24, AIME'25, AMC'23,\n\nv1-5\n\nother models on three standard code benchmarks (HEval\n\n, MBPP\n\n, LCB\n\n) and six math benchmarks (AIME'24, AIME'25, AMC'23,\n\nMATH500, Minerva, OlympiadBench).\n\nMATH500, Minerva, OlympiadBench).\n\nhas been observed and presented for jointly trained reasoning multi-tasks. Previously, length differences were typically noted between\ncorrect and incorrect traces (Liu et al., 2025).\n\nhas been observed and presented for jointly trained reasoning multi-tasks. Previously, length differences were typically noted between\n\nhas been observed and presented for jointly trained reasoning multi-tasks. Previously, length differences were typically noted between\n\ncorrect and incorrect traces (Liu et al., 2025).\n\ncorrect and incorrect traces\n\n(Liu et al.,\n\n2025)\n\n.\n\nThe reward dynamics between the propose and solve roles exhibit mildly adversarial behavior: when one receives higher rewards, the\nother often receives lower rewards. However, this is not entirely adversarial, as the proposer is also penalized for generating unsolvable\ntasks, encouraging overall cooperative behavior in the learning process.\n\nThe reward dynamics between the propose and solve roles exhibit mildly adversarial behavior: when one receives higher rewards, the\n\nThe reward dynamics between the propose and solve roles exhibit mildly adversarial behavior: when one receives higher rewards, the\n\nother often receives lower rewards. However, this is not entirely adversarial, as the proposer is also penalized for generating unsolvable\n\nother often receives lower rewards. However, this is not entirely adversarial, as the proposer is also penalized for generating unsolvable\n\ntasks, encouraging overall cooperative behavior in the learning process.\n\ntasks, encouraging overall cooperative behavior in the learning process.\n\nC.4. Complexity and Diversity Metrics of AZR Proposed Tasks\n\nC.4. Complexity and Diversity Metrics of AZR Proposed Tasks\n\nC.4. Complexity and Diversity Metrics of AZR Proposed Tasks\n\nWe outline several metrics used to probe characteristics of the tasks proposed during the training of AZR from the base model. Specifically,\nwe log two sets of metrics: program complexity and task diversity. For complexity, we employ two proxy measures\u2014ComplexiPy score\nand the Halstead metric. To assess diversity, we compute the average abstract syntax tree (AST) edit distance between the proposed\nprogram and a set of K reference programs, and an answer diversity metric. We calculate this answer diversity metric by tracking all\nhistorical answers to the generated questions, i.e., the input-output pairs, and form a categorical distribution over these outputs. We\ndefine answer diversity as 1 \u2212 p(answer), where p(answer) is the empirical probability of a specific answer\u2014used as a proxy for the\ndiversity of generated outputs.\n\nWe outline several metrics used to probe characteristics of the tasks proposed during the training of AZR from the base model. Specifically,\n\nWe outline several metrics used to probe characteristics of the tasks proposed during the training of AZR from the base model. Specifically,\n\nwe log two sets of metrics: program complexity and task diversity. For complexity, we employ two proxy measures\u2014ComplexiPy score\n\nwe log two sets of metrics: program complexity and task diversity. For complexity, we employ two proxy measures\u2014ComplexiPy score\n\nand the Halstead metric. To assess diversity, we compute the average abstract syntax tree (AST) edit distance between the proposed\n\nand the Halstead metric. To assess diversity, we compute the average abstract syntax tree (AST) edit distance between the proposed\n\nprogram and a set of K reference programs, and an answer diversity metric. We calculate this answer diversity metric by tracking all\n\nprogram and a set of\n\nreference programs, and an answer diversity metric. We calculate this answer diversity metric by tracking all\n\nK\n\nhistorical answers to the generated questions, i.e., the input-output pairs, and form a categorical distribution over these outputs. We\n\nhistorical answers to the generated questions, i.e., the input-output pairs, and form a categorical distribution over these outputs. We\n\ndefine answer diversity as 1 \u2212 p(answer), where p(answer) is the empirical probability of a specific answer\u2014used as a proxy for the\n\ndefine answer diversity as\n\n, where\n\nis the empirical probability of a specific answer\u2014used as a proxy for the\n\nanswer\n\nanswer\n\n1\n\n(\n\n)\n\n(\n\n)\n\np\n\np\n\n\u2212\n\ndiversity of generated outputs.\n\ndiversity of generated outputs.\n\nWe present these metrics in Figure 27. Interestingly, even without incorporating them explicitly into the reward function, the policy\nappears to implicitly optimize for these metrics. This aligns well with intuitive notions of task difficulty and diversity, and that the policy\nlearned to propose increasingly challenging tasks over time using our proposed simple reward function in Equation (4).\n\nWe present these metrics in Figure 27. Interestingly, even without incorporating them explicitly into the reward function, the policy\n\nWe present these metrics in Figure\n\n27.\n\nInterestingly, even without incorporating them explicitly into the reward function, the policy\n\nappears to implicitly optimize for these metrics. This aligns well with intuitive notions of task difficulty and diversity, and that the policy\n\nappears to implicitly optimize for these metrics. This aligns well with intuitive notions of task difficulty and diversity, and that the policy\n\nlearned to propose increasingly challenging tasks over time using our proposed simple reward function in Equation (4).\n\nlearned to propose increasingly challenging tasks over time using our proposed simple reward function in Equation\n\n(4)\n\n.\n\nC.5. Generated Code Complexity Dynamics Between Abd/Ded and Ind.\n\nC.5. Generated Code Complexity Dynamics Between Abd/Ded and Ind.\n\nC.5. Generated Code Complexity Dynamics Between Abd/Ded and Ind.\n\nWe use the ComplexiPy package to measure code complexity. For each generated program in the induction task, we compute the\ncognitive complexity difference from the corresponding \"gold\" code, i.e. complexipy(p\u03c0\npropose\n{abduction,deduction}\n) \u2212 complexipy(p\u03c0solve\ninduction\n) for each\npair, where the superscript of \u03c0 indicates the role and the subscript indicates the task type(s), and p denotes the generated programs. On\naverage, the difference of proposer and solver while holding the code's functionality constant is 0.27, indicating that the proposer in the\nabduction/deduction tasks often increases the cognitive complexity to make the code appear more convoluted, whereas the induction\nsolver tends to generate more efficient implementations.\n\nWe use the ComplexiPy package to measure code complexity. For each generated program in the induction task, we compute the\n\nWe use the\n\npackage to measure code complexity. For each generated program in the induction task, we compute the\n\nComplexiPy\n\ncognitive complexity difference from the corresponding \"gold\" code, i.e. complexipy(p\u03c0\n\ncognitive complexity difference from the corresponding \"gold\" code,\n\ncomplexipy\n\n) \u2212 complexipy(p\u03c0solve\n\ncomplexipy\n\n) for each\n\nfor each\n\n.\n\n(\n\n)\n\n(\n\n)\n\ni.e\n\np\n\np\n\npropose\n\npropose\n\nsolve\n\n\u2212\n\n\u03c0\n\n\u03c0\n\n{abduction,deduction}\n\n{abduction,deduction}\n\ninduction\n\ninduction\n\npair, where the superscript of \u03c0 indicates the role and the subscript indicates the task type(s), and p denotes the generated programs. On\n\npair, where the superscript of\n\nindicates the role and the subscript indicates the task type(s), and\n\ndenotes the generated programs. On\n\n\u03c0\n\np\n\naverage, the difference of proposer and solver while holding the code's functionality constant is 0.27, indicating that the proposer in the\n\naverage, the difference of proposer and solver while holding the code's functionality constant is\n\n, indicating that the proposer in the\n\n0\n\n27\n\n.\n\nabduction/deduction tasks often increases the cognitive complexity to make the code appear more convoluted, whereas the induction\n\nabduction/deduction tasks often increases the cognitive complexity to make the code appear more convoluted, whereas the induction\n\nsolver tends to generate more efficient implementations.\n\nsolver tends to generate more efficient implementations.\n\n32\n\n32\n\n32",
        "tables": [
          {
            "text": "ModelHEval+MBPP+LCBv1-5AIME&#x27;24 AIME&#x27;25 AMC&#x27;23 MATH500 Minerva OlympiadBenchLlama3.1-8B31.753.70.00.00.02.510.65.52.1+ Simple-RL-Zoo38.455.37.40.00.07.522.28.84.7+ AZR35.450.88.53.30.05.013.214.05.0Qwen2.5-3B-Coder67.165.920.03.33.320.051.018.416.6+ AZR71.369.024.43.33.337.562.026.127.0Qwen2.5-14B-Coder76.871.731.40.00.037.554.810.718.5+ AZR80.571.239.023.320.065.078.632.039.3Qwen2.5-14B-Base78.066.721.76.73.335.066.228.332.4+ AZR70.768.835.210.020.062.576.240.442.5",
            "csv": "",
            "json": "{\n  \"headers\": [],\n  \"rows\": []\n}"
          }
        ]
      },
      {
        "page_num": 32,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Learning to Reason with Zero Data via Self-play\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Learning to Reason with Zero Data via Self-play\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Learning to Reason with Zero Data via Self-play\n\nTask: Solve Deduction Task (Example 2)\n\nTask: Solve Deduction Task (Example 2)\n\nTask: Solve Deduction Task (Example 2)\n\nModel Input: Here is the function f, the given input, and we need to predict the output.\n\nModel Input: Here is the function f, the given input, and we need to predict the output.\n\nHere is the function\n\n, the given input, and we need to predict the output.\n\nModel Input:\n\nf\n\ndef f(num_list: list[int], target_product: int) -> list[tuple[int]]:\nresult_pairs = []\nfor i in range(len(num_list)):\nfor j in range(i + 1, len(num_list)):\nif num_list[i] * num_list[j] == target_product:\nresult_pairs.append((num_list[i], num_list[j]))\nreturn result_pairs\n\ndef f(num_list: list[int], target_product: int) -> list[tuple[int]]:\n\ndef\n\nf(num_list:\n\nlist[int], target_product:\n\nint)\n\n->\n\nlist[tuple[int]]:\n\nresult_pairs = []\n\nresult_pairs\n\n=\n\n[]\n\nfor i in range(len(num_list)):\n\nfor\n\nin\n\ni\n\nrange(len(num_list)):\n\nfor j in range(i + 1, len(num_list)):\n\nfor\n\nin\n\nj\n\nrange(i\n\n+\n\n1,\n\nlen(num_list)):\n\nif num_list[i] * num_list[j] == target_product:\n\nif\n\nnum_list[i]\n\n*\n\nnum_list[j]\n\n==\n\ntarget_product:\n\nresult_pairs.append((num_list[i], num_list[j]))\n\nresult_pairs.append((num_list[i], num_list[j]))\n\nreturn result_pairs\n\nreturn\n\nresult_pairs\n\nInput: [2, 3, 5, 7, 10], 10\nOutput: [(2, 5)]\n\nInput: [2, 3, 5, 7, 10], 10\n\nInput:\n\n[2, 3, 5, 7, 10], 10\n\nOutput: [(2, 5)]\n\nOutput:\n\n[(2, 5)]\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nThe given function f takes a list of integers num_list and a target product target_product as inputs. It is designed to fnd all\npairs of numbers in num_list whose product equals target_product. The function then returns a list of tuples, where each\ntuple contains a pair of numbers that satisfy the condition.\n\nThe given function f takes a list of integers num_list and a target product target_product as inputs. It is designed to fnd all\n\nThe given function\n\ntakes a list of integers\n\nand a target product\n\nas inputs. It is designed to fnd all\n\nf\n\nnum_list\n\ntarget_product\n\npairs of numbers in num_list whose product equals target_product. The function then returns a list of tuples, where each\n\npairs of numbers in\n\nwhose product equals\n\n. The function then returns a list of tuples, where each\n\nnum_list\n\ntarget_product\n\ntuple contains a pair of numbers that satisfy the condition.\n\ntuple contains a pair of numbers that satisfy the condition.\n\nLet's go through the code step by step:\n\nLet's go through the code step by step:\n\nLet's go through the code step by step:\n\n1. Initialize an empty list result_pairs to store pairs whose product equals target_product.\n\n1. Initialize an empty list result_pairs to store pairs whose product equals target_product.\n\n1. Initialize an empty list\n\nto store pairs whose product equals\n\n.\n\nresult_pairs\n\ntarget_product\n\n2. Use nested loops to iterate through each pair of numbers in num_list. The outer loop iterates over each number, and the inner\nloop iterates over numbers that come after the current number in the outer loop (\u907f\u514d\u91cd\u590d\u548c\u53cd\u5411\u7684\u5bf9\u5b50,\u4f8b\u5982 (2, 3) \u548c (3, 2))\u3002\n3. For each pair (num_list[i], num_list[j]), check if their product equals target_product.\n\n2. Use nested loops to iterate through each pair of numbers in num_list. The outer loop iterates over each number, and the inner\n\n2. Use nested loops to iterate through each pair of numbers in\n\n. The outer loop iterates over each number, and the inner\n\nnum_list\n\nloop iterates over numbers that come after the current number in the outer loop (\u907f\u514d\u91cd\u590d\u548c\u53cd\u5411\u7684\u5bf9\u5b50,\u4f8b\u5982 (2, 3) \u548c (3, 2))\u3002\n\nloop iterates over numbers that come after the current number in the outer loop (\n\n)\n\n\u3002\n\n(2, 3)\n\n(3, 2)\n\n\u907f\u514d\u91cd\u590d\u548c\u53cd\u5411\u7684\u5bf9\u5b50,\u4f8b\u5982\n\n\u548c\n\n3. For each pair (num_list[i], num_list[j]), check if their product equals target_product.\n\n3. For each pair\n\n, check if their product equals\n\n.\n\n(num_list[i], num_list[j])\n\ntarget_product\n\n4. If the product equals target_product, append the tuple (num_list[i], num_list[j]) to result_pairs.\n\n4. If the product equals target_product, append the tuple (num_list[i], num_list[j]) to result_pairs.\n\n4. If the product equals\n\n, append the tuple\n\nto\n\n.\n\ntarget_product\n\n(num_list[i], num_list[j])\n\nresult_pairs\n\n5. After all pairs have been checked, return the list result_pairs.\n\n5. After all pairs have been checked, return the list result_pairs.\n\n5. After all pairs have been checked, return the list\n\n.\n\nresult_pairs\n\nLet's explicitly apply this to the provided input [2, 3, 5, 7, 10], target product 10:\n\nLet's explicitly apply this to the provided input [2, 3, 5, 7, 10], target product 10:\n\nLet's explicitly apply this to the provided input\n\n, target product\n\n:\n\n[2, 3, 5, 7, 10]\n\n10\n\n- For i=0 (num_list[0] = 2):\n- j=1, 2 \u00d7 3 = 6 (\u226010)\n- j=2, 2 \u00d7 5 = 10 (=10), add pair (2, 5)\n- j=3, 2 \u00d7 7 = 14 (\u226010)\n- j=4, 2 \u00d7 10 = 20 (\u226010)\n- For i=1 (num_list[1] = 3):\n- j=2, 3 \u00d7 5 = 15 (\u226010)\n- j=3, 3 \u00d7 7 = 21 (\u226010)\n- j=4, 3 \u00d7 10 = 30 (\u226010)\n- For i=2 (num_list[2] = 5):\n- j=3, 5 \u00d7 7 = 35 (\u226010)\n- j=4, 5 \u00d7 10 = 50 (\u226010)\n- For i=3 (num_list[3] = 7):\n- j=4, 7 \u00d7 10 = 70 (\u226010)\n- For i=4 (num_list[4] = 10):\n- No further pairs to check\nThus, the only pair satisfying the condition is (2, 5). Therefore, the output is:\n[(2, 5)]\n\n- For i=0 (num_list[0] = 2):\n\n- For\n\n(num_list[0] = 2):\n\ni=0\n\n- j=1, 2 \u00d7 3 = 6 (\u226010)\n\n-\n\n, 2 \u00d7 3 = 6 (\u226010)\n\nj=1\n\n- j=2, 2 \u00d7 5 = 10 (=10), add pair (2, 5)\n\n-\n\n, 2 \u00d7 5 = 10 (=10), add pair (2, 5)\n\nj=2\n\n- j=3, 2 \u00d7 7 = 14 (\u226010)\n\n-\n\n, 2 \u00d7 7 = 14 (\u226010)\n\nj=3\n\n- j=4, 2 \u00d7 10 = 20 (\u226010)\n\n-\n\n, 2 \u00d7 10 = 20 (\u226010)\n\nj=4\n\n- For i=1 (num_list[1] = 3):\n\n- For\n\n(num_list[1] = 3):\n\ni=1\n\n- j=2, 3 \u00d7 5 = 15 (\u226010)\n\n-\n\n, 3 \u00d7 5 = 15 (\u226010)\n\nj=2\n\n- j=3, 3 \u00d7 7 = 21 (\u226010)\n\n-\n\n, 3 \u00d7 7 = 21 (\u226010)\n\nj=3\n\n- j=4, 3 \u00d7 10 = 30 (\u226010)\n\n-\n\n, 3 \u00d7 10 = 30 (\u226010)\n\nj=4\n\n- For i=2 (num_list[2] = 5):\n\n- For\n\n(num_list[2] = 5):\n\ni=2\n\n- j=3, 5 \u00d7 7 = 35 (\u226010)\n\n-\n\n, 5 \u00d7 7 = 35 (\u226010)\n\nj=3\n\n- j=4, 5 \u00d7 10 = 50 (\u226010)\n\n-\n\n, 5 \u00d7 10 = 50 (\u226010)\n\nj=4\n\n- For i=3 (num_list[3] = 7):\n\n- For\n\n(num_list[3] = 7):\n\ni=3\n\n- j=4, 7 \u00d7 10 = 70 (\u226010)\n\n-\n\n, 7 \u00d7 10 = 70 (\u226010)\n\nj=4\n\n- For i=4 (num_list[4] = 10):\n\n- For\n\n(num_list[4] = 10):\n\ni=4\n\n- No further pairs to check\n\n- No further pairs to check\n\nThus, the only pair satisfying the condition is (2, 5). Therefore, the output is:\n\nThus, the only pair satisfying the condition is (2, 5). Therefore, the output is:\n\n[(2, 5)]\n\n[(2, 5)]\n\nFigure 25. An example of the model solving an deduction task. We observed that the model outputs mixed language (Chinese) when\nsolving the task.\nFigure 25. Mixed Language CoT Example. An example of the model solving an deduction task. We observed that the model outputs\nmixed language (Chinese) when solving the task.\n\nFigure 25. Mixed Language CoT Example. An example of the model solving an deduction task. We observed that the model outputs\n\nAn example of the model solving an deduction task. We observed that the model outputs\n\nFigure 25. An example of the model solving an deduction task. We observed that the model outputs mixed language (Chinese) when\n\nFigure 25.\n\nAn example of the model solving an deduction task. We observed that the model outputs mixed language (Chinese) when\n\nMixed Language CoT Example.\n\nFigure 25.\n\nmixed language (Chinese) when solving the task.\n\nmixed language (Chinese) when solving the task.\n\nsolving the task.\n\nsolving the task.\n\n31\n33\n\n31\n\n31\n\n33\n\n33",
        "tables": []
      },
      {
        "page_num": 33,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Solve Deduction Task (Llama 8B)\n\nTask: Solve Deduction Task (Llama 8B)\n\nTask: Solve Deduction Task (Llama 8B)\n\nModel Input: Here is the function f, the input provided 'cookie', and we need to predict the output.\n\nModel Input: Here is the function f, the input provided 'cookie', and we need to predict the output.\n\nHere is the function\n\n, the input provided\n\n, and we need to predict the output.\n\nModel Input:\n\nf\n\n'cookie'\n\ndef f(input_string: str):\n\"\"\"Function to check if a string contains given substrings.\"\"\"\nsubstring = \"code\"\n# Start with `substring` as the first substring.\ncurr_index = substring\n# Slice out the required substring from the input string\nsliced_out_string = input_string[0:len(substring)]\nif (sliced_out_string == substring):\nreturn True\n# Check if there are any remaining substrings to be checked\nif (len(curr_index) > len(input_string)):\nreturn False\nreturn False\n\ndef f(input_string: str):\n\ndef\n\nf(input_string:\n\nstr):\n\n\"\"\"Function to check if a string contains given substrings.\"\"\"\n\n\"\"\"Function to check if a string contains given substrings.\"\"\"\n\nsubstring = \"code\"\n\nsubstring\n\n=\n\n\"code\"\n\n# Start with `substring` as the first substring.\n\n# Start with `substring` as the first substring.\n\ncurr_index = substring\n\ncurr_index\n\n=\n\nsubstring\n\n# Slice out the required substring from the input string\n\n# Slice out the required substring from the input string\n\nsliced_out_string = input_string[0:len(substring)]\n\nsliced_out_string\n\n=\n\ninput_string[0:len(substring)]\n\nif (sliced_out_string == substring):\n\nif\n\n(sliced_out_string\n\n==\n\nsubstring):\n\nreturn True\n\nreturn\n\nTrue\n\n# Check if there are any remaining substrings to be checked\n\n# Check if there are any remaining substrings to be checked\n\nif (len(curr_index) > len(input_string)):\n\nif\n\n(len(curr_index)\n\n>\n\nlen(input_string)):\n\nreturn False\n\nreturn\n\nFalse\n\nreturn False\n\nreturn\n\nFalse\n\nInput: 'cookie'\nOutput: False\n\nInput: 'cookie'\n\nInput:\n\n'cookie'\n\nOutput: False\n\nOutput:\n\nFalse\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nI know that the line 'curr_index = substring' is just initialization of the curr_index parameter variable and does not afect the output.\nHence, I can remove it completely to view the status of code before any output gets generated.\n\nI know that the line 'curr_index = substring' is just initialization of the curr_index parameter variable and does not afect the output.\n\nI know that the line 'curr_index = substring' is just initialization of the curr_index parameter variable and does not afect the output.\n\nHence, I can remove it completely to view the status of code before any output gets generated.\n\nHence, I can remove it completely to view the status of code before any output gets generated.\n\nNow I have a single variable, \"'curr_index\"' which is set to \"codecandide\".\n\nNow I have a single variable, \"'curr_index\"' which is set to \"codecandide\".\n\nNow I have a single variable, \"'curr_index\"' which is set to \"codecandide\".\n\nThe second line of code, 'slice_out_string = input_string[0:len(substring)]', assigns the result of slicing the input_string to the left\nof index 0 and to the right of index len(substring), i.e., the index of the last character in the substring. It then assigns this value to\nthe variable \"'slice_out_string\"'.\n\nThe second line of code, 'slice_out_string = input_string[0:len(substring)]', assigns the result of slicing the input_string to the left\n\nThe second line of code, 'slice_out_string = input_string[0:len(substring)]', assigns the result of slicing the input_string to the left\n\nof index 0 and to the right of index len(substring), i.e., the index of the last character in the substring. It then assigns this value to\n\nof index 0 and to the right of index len(substring), i.e., the index of the last character in the substring. It then assigns this value to\n\nthe variable \"'slice_out_string\"'.\n\nthe variable \"'slice_out_string\"'.\n\nThe third line, 'if (slice_out_string == substring)' checks whether the \"'slice_out_string\"' is equal to the value of substring. Because\ncurr_index is set to \"codecandide\", \"code\" appears on the left of the frst index in input_string, and thus, \"'slice_out_string\"' assumes\nthe value \"code\". That means for a minute the \"'if statement\"' is satisfed and will continue the code execution afterwards.\nHowever, according to the True/False statement, we see that the value of the frst index of input_string does not exist hence while it\ndoes output \"True\", \"False\" is outputted in main. So, the output of the whole program is shown below:\n\nThe third line, 'if (slice_out_string == substring)' checks whether the \"'slice_out_string\"' is equal to the value of substring. Because\n\nThe third line, 'if (slice_out_string == substring)' checks whether the \"'slice_out_string\"' is equal to the value of substring. Because\n\ncurr_index is set to \"codecandide\", \"code\" appears on the left of the frst index in input_string, and thus, \"'slice_out_string\"' assumes\n\ncurr_index is set to \"codecandide\", \"code\" appears on the left of the frst index in input_string, and thus, \"'slice_out_string\"' assumes\n\nthe value \"code\". That means for a minute the \"'if statement\"' is satisfed and will continue the code execution afterwards.\n\nthe value \"code\". That means for a minute the \"'if statement\"' is satisfed and will continue the code execution afterwards.\n\nHowever, according to the True/False statement, we see that the value of the frst index of input_string does not exist hence while it\n\nHowever, according to the True/False statement, we see that the value of the frst index of input_string does not exist hence while it\n\ndoes output \"True\", \"False\" is outputted in main. So, the output of the whole program is shown below:\n\ndoes output \"True\", \"False\" is outputted in main. So, the output of the whole program is shown below:\n\n'f(input_string = \"cookie\")' then output: False\n\n'f(input_string = \"cookie\")' then output: False\n\n'f(input_string = \"cookie\")' then output: False\n\nFigure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illustrate an\nexample here.\nFigure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illus-\ntrate an example here.\n\nFigure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\n\nThe\n\nAbsolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task.\n\nFigure 26.\n\nAbsolute Zero\n\nFigure 26. Absolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task. The Absolute Zero\n\nFigure 26.\n\nThe\n\nAbsolute Zero Reasoner-Llama3.1-8b CoT Example on Solving Output Deduction Task.\n\nAbsolute Zero\n\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illus-\n\nmodel appears to have strengthened its state-tracking behaviors during the course of training. We illus-\n\nReasoner-Llama3.1-8b\n\nReasoner-Llama3.1-8b model appears to have strengthened its state-tracking behaviors during the course of training. We illustrate an\n\nmodel appears to have strengthened its state-tracking behaviors during the course of training. We illustrate an\n\nReasoner-Llama3.1-8b\n\ntrate an example here.\n\ntrate an example here.\n\nexample here.\n\nexample here.\n\n0 80 160 240\n0\n0.47\nComplexiPy Score\n0 80 160 240\n0\n0.20\nHalstead Measure\n0 80 160 240\n0\n0.78\nAST Edit Distance\n0 80 160 240\n0\n0.99\nAnswer Diversity\nTraining Steps Complexity Diversity\n\nComplexiPy Score\n\nComplexiPy Score\n\nHalstead Measure\n\nHalstead Measure\n\nAST Edit Distance\n\nAST Edit Distance\n\nAnswer Diversity\n\nAnswer Diversity\n\n0.47\n\n0.47\n\n0.20\n\n0.20\n\n0.78\n\n0.78\n\n0.99\n\n0.99\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0 80 160 240\n\n0\n\n80\n\n160\n\n240\n\n0 80 160 240\n\n0\n\n80\n\n160\n\n240\n\n0 80 160 240\n\n0\n\n80\n\n160\n\n240\n\n0 80 160 240\n\n0\n\n80\n\n160\n\n240\n\nTraining Steps Complexity Diversity\n\nDiversity\n\nComplexity\n\nTraining Steps\n\nFigure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\nFigure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\nFigure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\n\nWe break down the proposed task metrics into program complexity and diversity across programs\n\nFigure 27. Metrics on Proposed Tasks. We break down the proposed task metrics into program complexity and diversity across programs\n\nFigure 27.\n\nWe break down the proposed task metrics into program complexity and diversity across programs\n\nMetrics on Proposed Tasks.\n\nMetrics on Proposed Tasks.\n\nFigure 27.\n\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\nand answers. An upward trend is observed in all metrics, indicating that AZR implicitly optimizes for these qualities as training progresses.\n\n35\n34\n\n34\n\n34\n\n35\n\n35",
        "tables": []
      },
      {
        "page_num": 34,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 75 100 125 150 175 200 225 250\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 75 100 125 150 175 200 225 250\n0.000\n0.050\n0.100\nAIME 2025\n50 75 100 125 150 175 200 225 250\n0.300\n0.325\n0.350\n0.375\n0.400\nOlympiad Bench\n50 75 100 125 150 175 200 225 250\n0.280\n0.300\n0.320\n0.340\n0.360\n0.380\nAccuracy\n\nMinerva\n50 75 100 125 150 175 200 225 250\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nMath 500\n50 75 100 125 150 175 200 225 250\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 75 100 125 150 175 200 225 250\n0.700\n0.710\n0.720\n0.730\n0.740\nAccuracy\n\nHumanEval+\n50 75 100 125 150 175 200 225 250\n0.660\n0.670\n0.680\n0.690\n0.700\nMBPP+\n50 75 100 125 150 175 200 225 250\n0.240\n0.260\n0.280\nLiveCodeBench\n50 75 100 125 150 175 200 225 250\n0.300\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 75 100 125 150 175 200 225 250\n0.540\n0.550\n0.560\n0.570\nCode Average\n50 75 100 125 150 175 200 225 250\n0.420\n0.440\n0.460\n0.480\nOverall Average\nFigure 28. Absolute Zero Reasoner-base-7b OOD Performance Breakdown.\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 75 100 125 150 175 200 225 250\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 75 100 125 150 175 200 225 250\n0.000\n0.050\n0.100\nAIME 2025\n50 75 100 125 150 175 200 225 250\n0.300\n0.325\n0.350\n0.375\n0.400\nOlympiad Bench\n50 75 100 125 150 175 200 225 250\n0.280\n0.300\n0.320\n0.340\n0.360\n0.380\nAccuracy\n\nMinerva\n50 75 100 125 150 175 200 225 250\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\nMath 500\n50 75 100 125 150 175 200 225 250\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 75 100 125 150 175 200 225 250\n0.700\n0.710\n0.720\n0.730\n0.740\nAccuracy\n\nHumanEval+\n50 75 100 125 150 175 200 225 250\n0.660\n0.670\n0.680\n0.690\n0.700\nMBPP+\n50 75 100 125 150 175 200 225 250\n0.240\n0.260\n0.280\nLiveCodeBench\n50 75 100 125 150 175 200 225 250\n0.300\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 75 100 125 150 175 200 225 250\n0.540\n0.550\n0.560\n0.570\nCode Average\n50 75 100 125 150 175 200 225 250\n0.420\n0.440\n0.460\n0.480\nOverall Average\n\nAIME 2024\n\nAIME 2024\n\nAIME 2025\n\nAIME 2025\n\nOlympiad Bench\n\nOlympiad Bench\n\n0.200\n\n0.200\n\n0.400\n\n0.400\n\n0.375\n\n0.375\n\n0.100\n\n0.100\n\nAccuracy\n\nAccuracy\n\n0.150\n\n0.150\n\n0.350\n\n0.350\n\n0.100\n\n0.100\n\n0.050\n\n0.050\n\n0.325\n\n0.325\n\n0.300\n\n0.300\n\n0.050\n\n0.050\n\n0.000\n\n0.000\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nMinerva\n\nMinerva\n\nMath 500\n\nMath 500\n\nAMC 2023\n\nAMC 2023\n\n0.750\n\n0.750\n\n0.380\n\n0.380\n\n0.600\n\n0.600\n\n0.725\n\n0.725\n\nAccuracy\n\nAccuracy\n\n0.360\n\n0.360\n\n0.550\n\n0.550\n\n0.700\n\n0.700\n\n0.340\n\n0.340\n\n0.500\n\n0.500\n\n0.675\n\n0.675\n\n0.320\n\n0.320\n\n0.450\n\n0.450\n\n0.650\n\n0.650\n\n0.300\n\n0.300\n\n0.400\n\n0.400\n\n0.625\n\n0.625\n\n0.280\n\n0.280\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nCODE REASONING\n\nCODE REASONING\n\nHumanEval+\n\nHumanEval+\n\nMBPP+\n\nMBPP+\n\nLiveCodeBench\n\nLiveCodeBench\n\n0.700\n\n0.700\n\n0.740\n\n0.740\n\n0.280\n\n0.280\n\nAccuracy\n\nAccuracy\n\n0.690\n\n0.690\n\n0.730\n\n0.730\n\n0.720\n\n0.720\n\n0.680\n\n0.680\n\n0.260\n\n0.260\n\n0.710\n\n0.710\n\n0.670\n\n0.670\n\n0.240\n\n0.240\n\n0.700\n\n0.700\n\n0.660\n\n0.660\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nOVERALL PERFORMANCE\n\nOVERALL PERFORMANCE\n\nMath Average\n\nMath Average\n\nCode Average\n\nCode Average\n\nOverall Average\n\nOverall Average\n\n0.570\n\n0.570\n\n0.400\n\n0.400\n\n0.480\n\n0.480\n\n0.380\n\n0.380\n\n0.560\n\n0.560\n\nAccuracy\n\nAccuracy\n\n0.460\n\n0.460\n\n0.360\n\n0.360\n\n0.340\n\n0.340\n\n0.550\n\n0.550\n\n0.440\n\n0.440\n\n0.320\n\n0.320\n\n0.540\n\n0.540\n\n0.300\n\n0.300\n\n0.420\n\n0.420\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\n50 75 100 125 150 175 200 225 250\n\n50\n\n75\n\n100\n\n125\n\n150\n\n175\n\n200\n\n225\n\n250\n\nFigure 28. Absolute Zero Reasoner-base-7b OOD Performance Breakdown.\n\nFigure 28. Absolute Zero Reasoner-base-7b OOD Performance Breakdown.\n\nAbsolute Zero Reasoner-base-7b OOD Performance Breakdown.\n\nFigure 28.\n\n35\n\n35\n\n35",
        "tables": []
      },
      {
        "page_num": 35,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.000\n0.020\n0.040\n0.060\n0.080\n0.100\nAIME 2025\n50 100 150 200 250 300 350\n0.340\n0.360\n0.380\nOlympiad Bench\n50 100 150 200 250 300 350\n0.275\n0.300\n0.325\n0.350\n0.375\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.680\n0.700\n0.720\n0.740\n0.760\nMath 500\n50 100 150 200 250 300 350\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.810\n0.820\n0.830\n0.840\n0.850\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.260\n0.280\n0.300\n0.320\nLiveCodeBench\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.580\n0.590\n0.600\n0.610\n0.620\n0.630\nCode Average\n50 100 150 200 250 300 350\n0.460\n0.480\n0.500\nOverall Average\nFigure 29. Absolute Zero Reasoner-Coder-7b OOD Performance Breakdown.\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.000\n0.020\n0.040\n0.060\n0.080\n0.100\nAIME 2025\n50 100 150 200 250 300 350\n0.340\n0.360\n0.380\nOlympiad Bench\n50 100 150 200 250 300 350\n0.275\n0.300\n0.325\n0.350\n0.375\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.680\n0.700\n0.720\n0.740\n0.760\nMath 500\n50 100 150 200 250 300 350\n0.400\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.810\n0.820\n0.830\n0.840\n0.850\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.260\n0.280\n0.300\n0.320\nLiveCodeBench\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\n0.380\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.580\n0.590\n0.600\n0.610\n0.620\n0.630\nCode Average\n50 100 150 200 250 300 350\n0.460\n0.480\n0.500\nOverall Average\n\nAIME 2024\n\nAIME 2024\n\nAIME 2025\n\nAIME 2025\n\nOlympiad Bench\n\nOlympiad Bench\n\n0.200\n\n0.200\n\n0.100\n\n0.100\n\n0.380\n\n0.380\n\n0.080\n\n0.080\n\nAccuracy\n\nAccuracy\n\n0.150\n\n0.150\n\n0.060\n\n0.060\n\n0.360\n\n0.360\n\n0.040\n\n0.040\n\n0.100\n\n0.100\n\n0.020\n\n0.020\n\n0.050\n\n0.050\n\n0.340\n\n0.340\n\n0.000\n\n0.000\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nMinerva\n\nMinerva\n\nMath 500\n\nMath 500\n\nAMC 2023\n\nAMC 2023\n\n0.600\n\n0.600\n\n0.375\n\n0.375\n\n0.760\n\n0.760\n\n0.550\n\n0.550\n\nAccuracy\n\nAccuracy\n\n0.350\n\n0.350\n\n0.740\n\n0.740\n\n0.500\n\n0.500\n\n0.325\n\n0.325\n\n0.720\n\n0.720\n\n0.450\n\n0.450\n\n0.300\n\n0.300\n\n0.700\n\n0.700\n\n0.275\n\n0.275\n\n0.400\n\n0.400\n\n0.680\n\n0.680\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nCODE REASONING\n\nCODE REASONING\n\nHumanEval+\n\nHumanEval+\n\nMBPP+\n\nMBPP+\n\nLiveCodeBench\n\nLiveCodeBench\n\n0.720\n\n0.720\n\n0.320\n\n0.320\n\n0.850\n\n0.850\n\n0.710\n\n0.710\n\nAccuracy\n\nAccuracy\n\n0.840\n\n0.840\n\n0.300\n\n0.300\n\n0.830\n\n0.830\n\n0.700\n\n0.700\n\n0.820\n\n0.820\n\n0.280\n\n0.280\n\n0.690\n\n0.690\n\n0.810\n\n0.810\n\n0.680\n\n0.680\n\n0.260\n\n0.260\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nOVERALL PERFORMANCE\n\nOVERALL PERFORMANCE\n\nMath Average\n\nMath Average\n\nCode Average\n\nCode Average\n\nOverall Average\n\nOverall Average\n\n0.400\n\n0.400\n\n0.630\n\n0.630\n\n0.500\n\n0.500\n\n0.620\n\n0.620\n\n0.380\n\n0.380\n\nAccuracy\n\nAccuracy\n\n0.610\n\n0.610\n\n0.360\n\n0.360\n\n0.480\n\n0.480\n\n0.600\n\n0.600\n\n0.340\n\n0.340\n\n0.460\n\n0.460\n\n0.590\n\n0.590\n\n0.320\n\n0.320\n\n0.580\n\n0.580\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nFigure 29. Absolute Zero Reasoner-Coder-7b OOD Performance Breakdown.\n\nFigure 29. Absolute Zero Reasoner-Coder-7b OOD Performance Breakdown.\n\nAbsolute Zero Reasoner-Coder-7b OOD Performance Breakdown.\n\nFigure 29.\n\n36\n\n36\n\n36",
        "tables": []
      },
      {
        "page_num": 36,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350\n0.250\n0.300\n0.350\n0.400\nOlympiad Bench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.500\n0.550\n0.600\n0.650\n0.700\n0.750\nMath 500\n50 100 150 200 250 300 350\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.700\n0.720\n0.740\n0.760\n0.780\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\nLiveCodeBench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.570\n0.580\n0.590\n0.600\n0.610\n0.620\nCode Average\n50 100 150 200 250 300 350\n0.440\n0.460\n0.480\n0.500\nOverall Average\nFigure 30. Absolute Zero Reasoner-base-14b OOD Performance Breakdown.\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350\n0.250\n0.300\n0.350\n0.400\nOlympiad Bench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350\n0.500\n0.550\n0.600\n0.650\n0.700\n0.750\nMath 500\n50 100 150 200 250 300 350\n0.450\n0.500\n0.550\n0.600\nAMC 2023\n50 100 150 200 250 300 350\n0.700\n0.720\n0.740\n0.760\n0.780\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350\n0.680\n0.690\n0.700\n0.710\n0.720\nMBPP+\n50 100 150 200 250 300 350\n0.320\n0.340\n0.360\nLiveCodeBench\n50 100 150 200 250 300 350\n0.300\n0.350\n0.400\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350\n0.570\n0.580\n0.590\n0.600\n0.610\n0.620\nCode Average\n50 100 150 200 250 300 350\n0.440\n0.460\n0.480\n0.500\nOverall Average\n\nAIME 2024\n\nAIME 2024\n\nAIME 2025\n\nAIME 2025\n\nOlympiad Bench\n\nOlympiad Bench\n\n0.200\n\n0.200\n\n0.400\n\n0.400\n\n0.200\n\n0.200\n\nAccuracy\n\nAccuracy\n\n0.150\n\n0.150\n\n0.350\n\n0.350\n\n0.150\n\n0.150\n\n0.100\n\n0.100\n\n0.300\n\n0.300\n\n0.100\n\n0.100\n\n0.050\n\n0.050\n\n0.250\n\n0.250\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nMinerva\n\nMinerva\n\nMath 500\n\nMath 500\n\nAMC 2023\n\nAMC 2023\n\n0.750\n\n0.750\n\n0.600\n\n0.600\n\n0.400\n\n0.400\n\n0.700\n\n0.700\n\nAccuracy\n\nAccuracy\n\n0.550\n\n0.550\n\n0.650\n\n0.650\n\n0.350\n\n0.350\n\n0.500\n\n0.500\n\n0.600\n\n0.600\n\n0.550\n\n0.550\n\n0.300\n\n0.300\n\n0.450\n\n0.450\n\n0.500\n\n0.500\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nCODE REASONING\n\nCODE REASONING\n\nHumanEval+\n\nHumanEval+\n\nMBPP+\n\nMBPP+\n\nLiveCodeBench\n\nLiveCodeBench\n\n0.780\n\n0.780\n\n0.720\n\n0.720\n\n0.760\n\n0.760\n\n0.360\n\n0.360\n\nAccuracy\n\nAccuracy\n\n0.710\n\n0.710\n\n0.740\n\n0.740\n\n0.700\n\n0.700\n\n0.340\n\n0.340\n\n0.690\n\n0.690\n\n0.720\n\n0.720\n\n0.320\n\n0.320\n\n0.680\n\n0.680\n\n0.700\n\n0.700\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nOVERALL PERFORMANCE\n\nOVERALL PERFORMANCE\n\nMath Average\n\nMath Average\n\nCode Average\n\nCode Average\n\nOverall Average\n\nOverall Average\n\n0.620\n\n0.620\n\n0.500\n\n0.500\n\n0.610\n\n0.610\n\n0.400\n\n0.400\n\nAccuracy\n\nAccuracy\n\n0.600\n\n0.600\n\n0.480\n\n0.480\n\n0.350\n\n0.350\n\n0.590\n\n0.590\n\n0.460\n\n0.460\n\n0.580\n\n0.580\n\n0.300\n\n0.300\n\n0.440\n\n0.440\n\n0.570\n\n0.570\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n50 100 150 200 250 300 350\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nFigure 30. Absolute Zero Reasoner-base-14b OOD Performance Breakdown.\n\nFigure 30. Absolute Zero Reasoner-base-14b OOD Performance Breakdown.\n\nAbsolute Zero Reasoner-base-14b OOD Performance Breakdown.\n\nFigure 30.\n\n37\n\n37\n\n37",
        "tables": []
      },
      {
        "page_num": 37,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350 400\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350 400\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350 400\n0.380\n0.400\n0.420\n0.440 Olympiad Bench\n50 100 150 200 250 300 350 400\n0.200\n0.250\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350 400\n0.700\n0.720\n0.740\n0.760\n0.780\n0.800\nMath 500\n50 100 150 200 250 300 350 400\n0.500\n0.550\n0.600\n0.650\n0.700\nAMC 2023\n50 100 150 200 250 300 350 400\n0.780\n0.800\n0.820\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350 400\n0.700\n0.710\n0.720\n0.730\n0.740\nMBPP+\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\nLiveCodeBench\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\n0.420\n0.440\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350 400\n0.610\n0.620\n0.630\n0.640\n0.650\nCode Average\n50 100 150 200 250 300 350 400\n0.500\n0.520\n0.540\nOverall Average\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nMATH REASONING\n\nCODE REASONING\nOVERALL PERFORMANCE\n50 100 150 200 250 300 350 400\n0.100\n0.150\n0.200\nAccuracy\n\nAIME 2024\n50 100 150 200 250 300 350 400\n0.050\n0.100\n0.150\n0.200\nAIME 2025\n50 100 150 200 250 300 350 400\n0.380\n0.400\n0.420\n0.440 Olympiad Bench\n50 100 150 200 250 300 350 400\n0.200\n0.250\n0.300\n0.350\n0.400\nAccuracy\n\nMinerva\n50 100 150 200 250 300 350 400\n0.700\n0.720\n0.740\n0.760\n0.780\n0.800\nMath 500\n50 100 150 200 250 300 350 400\n0.500\n0.550\n0.600\n0.650\n0.700\nAMC 2023\n50 100 150 200 250 300 350 400\n0.780\n0.800\n0.820\nAccuracy\n\nHumanEval+\n50 100 150 200 250 300 350 400\n0.700\n0.710\n0.720\n0.730\n0.740\nMBPP+\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\nLiveCodeBench\n50 100 150 200 250 300 350 400\n0.340\n0.360\n0.380\n0.400\n0.420\n0.440\nAccuracy\n\nMath Average\n50 100 150 200 250 300 350 400\n0.610\n0.620\n0.630\n0.640\n0.650\nCode Average\n50 100 150 200 250 300 350 400\n0.500\n0.520\n0.540\nOverall Average\n\nAIME 2024\n\nAIME 2024\n\nAIME 2025\n\nAIME 2025\n\n0.440 Olympiad Bench\n\nOlympiad Bench\n\n0.440\n\n0.200\n\n0.200\n\n0.200\n\n0.200\n\n0.420\n\n0.420\n\nAccuracy\n\nAccuracy\n\n0.150\n\n0.150\n\n0.150\n\n0.150\n\n0.400\n\n0.400\n\n0.100\n\n0.100\n\n0.100\n\n0.100\n\n0.380\n\n0.380\n\n0.050\n\n0.050\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nMinerva\n\nMinerva\n\nMath 500\n\nMath 500\n\nAMC 2023\n\nAMC 2023\n\n0.700\n\n0.700\n\n0.800\n\n0.800\n\n0.400\n\n0.400\n\n0.780\n\n0.780\n\n0.650\n\n0.650\n\nAccuracy\n\nAccuracy\n\n0.350\n\n0.350\n\n0.760\n\n0.760\n\n0.600\n\n0.600\n\n0.300\n\n0.300\n\n0.740\n\n0.740\n\n0.550\n\n0.550\n\n0.250\n\n0.250\n\n0.720\n\n0.720\n\n0.500\n\n0.500\n\n0.200\n\n0.200\n\n0.700\n\n0.700\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nCODE REASONING\n\nCODE REASONING\n\nHumanEval+\n\nHumanEval+\n\nMBPP+\n\nMBPP+\n\nLiveCodeBench\n\nLiveCodeBench\n\n0.400\n\n0.400\n\n0.740\n\n0.740\n\n0.730\n\n0.730\n\n0.380\n\n0.380\n\nAccuracy\n\nAccuracy\n\n0.820\n\n0.820\n\n0.720\n\n0.720\n\n0.360\n\n0.360\n\n0.800\n\n0.800\n\n0.710\n\n0.710\n\n0.340\n\n0.340\n\n0.700\n\n0.700\n\n0.780\n\n0.780\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nOVERALL PERFORMANCE\n\nOVERALL PERFORMANCE\n\nMath Average\n\nMath Average\n\nCode Average\n\nCode Average\n\nOverall Average\n\nOverall Average\n\n0.440\n\n0.440\n\n0.540\n\n0.540\n\n0.650\n\n0.650\n\n0.420\n\n0.420\n\n0.640\n\n0.640\n\nAccuracy\n\nAccuracy\n\n0.520\n\n0.520\n\n0.400\n\n0.400\n\n0.630\n\n0.630\n\n0.380\n\n0.380\n\n0.620\n\n0.620\n\n0.500\n\n0.500\n\n0.360\n\n0.360\n\n0.610\n\n0.610\n\n0.340\n\n0.340\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\n50 100 150 200 250 300 350 400\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nFigure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\nFigure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\nFigure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\nAbsolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\nFigure 31.\n\nFigure 31. Absolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\nFigure 31.\n\nAbsolute Zero Reasoner-Coder-14b OOD Performance Breakdown.\n\n<think>\nDesign an absolutely ludicrous and convoluted Python function that is extremely difficult to\ndeduce the output from the input, designed to keep machine learning models such as Snippi\nguessing and your peers puzzling.\n!\u2192\n!\u2192\nThe aim is to outsmart all these groups of intelligent machines and less intelligent humans. This\n!\u2192 is for the brains behind the future.\n</think>\n- Absolute Zero Reasoner-Llama3.1-8b @ step 132\n\n<think>\n\n<think>\n\nDesign an absolutely ludicrous and convoluted Python function that is extremely difficult to\n\nDesign an absolutely ludicrous and convoluted Python function that is extremely difficult to\n\ndeduce the output from the input, designed to keep machine learning models such as Snippi\n\ndeduce the output from the input, designed to keep machine learning models such as Snippi\n\n!\u2192\n\n!\n\n\u2192\n\nguessing and your peers puzzling.\n\nguessing and your peers puzzling.\n\n!\u2192\n\n!\n\n\u2192\n\nThe aim is to outsmart all these groups of intelligent machines and less intelligent humans. This\n\nThe aim is to outsmart all these groups of intelligent machines and less intelligent humans. This\n\n!\u2192 is for the brains behind the future.\n\nis for the brains behind the future.\n\n!\n\n\u2192\n\n</think>\n\n</think>\n\n- Absolute Zero Reasoner-Llama3.1-8b @ step 132\n\n- Absolute Zero Reasoner-Llama3.1-8b @ step 132\n\nFigure 32. Absolute Zero Reasoner \u2013Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\nFigure 32. Absolute Zero Reasoner \u2013 Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\nFigure 32. Absolute Zero Reasoner \u2013 Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\n\nThis example highlights an unexpected and potentially unsafe\n\nAbsolute Zero Reasoner \u2013 Llama3.1-8B \"Uh-oh Moment.\"\n\nFigure 32.\n\nFigure 32. Absolute Zero Reasoner \u2013Llama3.1-8B \"Uh-oh Moment.\" This example highlights an unexpected and potentially unsafe\n\nFigure 32.\n\nThis example highlights an unexpected and potentially unsafe\n\nAbsolute Zero Reasoner\n\nLlama3.1-8B \"Uh-oh Moment.\"\n\n\u2013\n\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\n\nreasoning chain generated by our\n\nmodel during training. Although our paradigm enables\n\nAbsolute Zero Reasoner\u2013Llama3.1-8B\n\nreasoning chain generated by our Absolute Zero Reasoner\u2013Llama3.1-8B model during training. Although our paradigm enables\n\nreasoning chain generated by our\n\nmodel during training. Although our paradigm enables\n\n\u2013\n\nAbsolute Zero Reasoner\n\nLlama3.1-8B\n\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\nreasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors.\n\n39\n38\n\n38\n\n38\n\n39\n\n39",
        "tables": []
      },
      {
        "page_num": 38,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it.\nThe assistant first thinks about the reasoning process in the mind and then provides the user\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer>\nanswer here </answer>.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\nUser: {TASK_INSTRUCTION}\n\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n\nThe assistant first thinks about the reasoning process in the mind and then provides the user\n\nThe assistant first thinks about the reasoning process in the mind and then provides the user\n\n!\u2192\n\n!\n\n\u2192\n\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\n\n!\u2192\n\n!\n\n\u2192\n\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer>\n\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer>\n\n!\u2192\n\n!\n\n\u2192\n\nanswer here </answer>.\n\nanswer here </answer>.\n\n!\u2192\n\n!\n\n\u2192\n\nUser: {TASK_INSTRUCTION}\n\nUser:\n\n{TASK_INSTRUCTION}\n\nAssistant: <think>\n\nAssistant: <think>\n\nAssistant: <think>\n\nFigure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.\nFigure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.\n\nFigure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.\n\nAll our models were trained using the default Deepseek R1 template.\n\nDeepseek R1 Template.\n\nFigure 33.\n\nFigure 33. Deepseek R1 Template. All our models were trained using the default Deepseek R1 template.\n\nFigure 33.\n\nAll our models were trained using the default Deepseek R1 template.\n\nDeepseek R1 Template.\n\n40\n39\n\n39\n\n39\n\n40\n\n40",
        "tables": []
      },
      {
        "page_num": 39,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n## Task: Create a Python Code Snippet (where custom classes are allowed, which should be defined\n!\u2192 at the top of the code snippet) with one Matching Input\nUsing the reference code snippets provided below as examples, design a new and unique Python code\nsnippet that demands deep algorithmic reasoning to deduce one possible input from a given\noutput. Your submission should include both a code snippet and test input pair, where the\ninput will be plugged into the code snippet to produce the output, which that function output\nbe given to a test subject to come up with any input that will produce the same function\noutput. This is meant to be an I.Q. test.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n### Code Requirements:\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n!\u2192 `f`\n- Ensure the function returns a value\n- Include at least one input parameter\n- Make the function deterministic\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n!\u2192 requires long multi step reasoning\n- AVOID THE FOLLOWING:\n* Random functions or variables\n* Date/time operations\n* I/O operations (reading files, network requests)\n* Printing or logging\n* Any external state\n- Ensure execution completes within 10 seconds on a modern CPU\n- All imports and class definitions should be at the very top of the code snippet\n- The snippet should end with a return statement from the main function `f`, anything after will\n!\u2192 be removed\n### Input Requirements:\n- Provide exactly one test input for your function\n- Format multiple arguments with commas between them\n- Remember to add quotes around string arguments\n### Formatting:\n- Format your code with: ```python\ndef f(...):\n# your code here\nreturn ...\n```\n- Format your input with: ```input\narg1, arg2, ...\n```\n### Example Format:\n```python\ndef f(name: str, info: dict):\n# code logic here\nreturn result\n```\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n### Evaluation Criteria:\n- Executability, your code should be executable given your input\n- Difficulty in predicting the output from your provided input and code snippet. Focus on either\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n!\u2192\n!\u2192\n!\u2192\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\ninputs.\n!\u2192\n!\u2192\n### Reference Code Snippets:\n\n## Task: Create a Python Code Snippet (where custom classes are allowed, which should be defined\n\n## Task: Create a Python Code Snippet (where custom classes are allowed, which should be defined\n\n!\u2192 at the top of the code snippet) with one Matching Input\n\nat the top of the code snippet) with one Matching Input\n\n!\n\n\u2192\n\nUsing the reference code snippets provided below as examples, design a new and unique Python code\n\nUsing the reference code snippets provided below as examples, design a new and unique Python code\n\nsnippet that demands deep algorithmic reasoning to deduce one possible input from a given\n\nsnippet that demands deep algorithmic reasoning to deduce one possible input from a given\n\n!\u2192\n\n!\n\n\u2192\n\noutput. Your submission should include both a code snippet and test input pair, where the\n\noutput. Your submission should include both a code snippet and test input pair, where the\n\n!\u2192\n\n!\n\n\u2192\n\ninput will be plugged into the code snippet to produce the output, which that function output\n\ninput will be plugged into the code snippet to produce the output, which that function output\n\n!\u2192\n\n!\n\n\u2192\n\nbe given to a test subject to come up with any input that will produce the same function\n\nbe given to a test subject to come up with any input that will produce the same function\n\n!\u2192\n\n!\n\n\u2192\n\noutput. This is meant to be an I.Q. test.\n\noutput. This is meant to be an I.Q. test.\n\n!\u2192\n\n!\n\n\u2192\n\n### Code Requirements:\n\n### Code Requirements:\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n!\u2192 `f`\n\n`f`\n\n!\n\n\u2192\n\n- Ensure the function returns a value\n\n- Ensure the function returns a value\n\n- Include at least one input parameter\n\n- Include at least one input parameter\n\n- Make the function deterministic\n\n- Make the function deterministic\n\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n\n!\u2192 requires long multi step reasoning\n\nrequires long multi step reasoning\n\n!\n\n\u2192\n\n- AVOID THE FOLLOWING:\n\n- AVOID THE FOLLOWING:\n\n* Random functions or variables\n\n* Random functions or variables\n\n* Date/time operations\n\n* Date/time operations\n\n* I/O operations (reading files, network requests)\n\n* I/O operations (reading files, network requests)\n\n* Printing or logging\n\n* Printing or logging\n\n* Any external state\n\n* Any external state\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- The snippet should end with a return statement from the main function `f`, anything after will\n\n- The snippet should end with a return statement from the main function `f`, anything after will\n\n!\u2192 be removed\n\nbe removed\n\n!\n\n\u2192\n\n### Input Requirements:\n\n### Input Requirements:\n\n- Provide exactly one test input for your function\n\n- Provide exactly one test input for your function\n\n- Format multiple arguments with commas between them\n\n- Format multiple arguments with commas between them\n\n- Remember to add quotes around string arguments\n\n- Remember to add quotes around string arguments\n\n### Formatting:\n\n### Formatting:\n\n- Format your code with: ```python\n\n- Format your code with: ```python\n\ndef f(...):\n\ndef f(...):\n\n# your code here\n\n# your code here\n\nreturn ...\n\nreturn ...\n\n```\n\n```\n\n- Format your input with: ```input\n\n- Format your input with: ```input\n\narg1, arg2, ...\n\narg1, arg2, ...\n\n```\n\n```\n\n### Example Format:\n\n### Example Format:\n\n```python\n\n```python\n\ndef f(name: str, info: dict):\n\ndef f(name: str, info: dict):\n\n# code logic here\n\n# code logic here\n\nreturn result\n\nreturn result\n\n```\n\n```\n\n```input\n\n```input\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n### Evaluation Criteria:\n\n### Evaluation Criteria:\n\n- Executability, your code should be executable given your input\n\n- Executability, your code should be executable given your input\n\n- Difficulty in predicting the output from your provided input and code snippet. Focus on either\n\n- Difficulty in predicting the output from your provided input and code snippet. Focus on either\n\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\n\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\n\n!\u2192\n\n!\n\n\u2192\n\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\n\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\n\n!\u2192\n\n!\n\n\u2192\n\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n\n!\u2192\n\n!\n\n\u2192\n\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\n\nwords in any form, even in comments:\n\n{LIST_OF_FORBIDDEN_PACKAGES}\n\n!\n\n\u2192\n\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\n\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\n\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\n\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\n\n!\u2192\n\n!\n\n\u2192\n\ninputs.\n\ninputs.\n\n!\u2192\n\n!\n\n\u2192\n\n### Reference Code Snippets:\n\n### Reference Code Snippets:\n\n{CODE_REFERENCES_FROM_BUFFER}\n\n{CODE_REFERENCES_FROM_BUFFER}\n\n{CODE_REFERENCES_FROM_BUFFER}\n\nFigure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.\nFigure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.\n\nFigure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.\n\nFigure 34.\n\nProgram Input Abduction Task\n\nProblem Proposal Instruction.\n\n\u2014\n\nFigure 34. Program Input Abduction Task\u2014Problem Proposal Instruction.\n\nProgram Input Abduction Task\u2014Problem Proposal Instruction.\n\nFigure 34.\n\n41\n40\n\n41\n\n41\n\n40\n\n40",
        "tables": []
      },
      {
        "page_num": 40,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n## Task: Create a New Python Code Snippet (where custom classes are allowed, which should be\n!\u2192 defined at the top of the code snippet) with one Matching Input\nUsing the reference code snippets provided below as examples, design a new and unique Python code\nsnippet that demands deep algorithmic reasoning to deduce the output from the input. Your\nsubmission should include a code snippet and a test input pair, where the input will be\nplugged into the code snippet to produce the output. The input will be given to a test\nsubject to deduce the output, which is meant to be an I.Q. test.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n### Code Requirements:\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n!\u2192 `f`\n- Ensure the function returns a value\n- Include at least one input parameter\n- Make the function deterministic\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n!\u2192 requires long multi step reasoning\n- AVOID THE FOLLOWING:\n* Random functions or variables\n* Date/time operations\n* I/O operations (reading files, network requests)\n* Printing or logging\n* Any external state\n- Ensure execution completes within 10 seconds on a modern CPU\n- All imports and class definitions should be at the very top of the code snippet\n- The snippet should end with a return statement from the main function `f`, anything after will\n!\u2192 be removed\n### Input Requirements:\n- Provide exactly one test input for your function\n- Format multiple arguments with commas between them\n- Remember to add quotes around string arguments\n### Formatting:\n- Format your code with:\n```python\ndef f(...):\n# your code here\nreturn ...\n```\n- Format your input with:\n```input\narg1, arg2, ...\n```\n### Example Format:\n```python\ndef f(name: str, info: dict):\n# code logic here\nreturn result\n```\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n### Evaluation Criteria:\n- Executability, your code should be executable given your input\n- Difficulty in predicting your ```input``` from 1) your ```python``` code and 2) the\ndeterministic ```output``` that will be obtained from your ```input```. Focus on either\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\ninputs.\n!\u2192\n!\u2192\n### Reference Code Snippets:\n{CODE_REFERENCES_FROM_BUFFER}\n\n## Task: Create a New Python Code Snippet (where custom classes are allowed, which should be\n\n## Task: Create a New Python Code Snippet (where custom classes are allowed, which should be\n\n!\u2192 defined at the top of the code snippet) with one Matching Input\n\ndefined at the top of the code snippet) with one Matching Input\n\n!\n\n\u2192\n\nUsing the reference code snippets provided below as examples, design a new and unique Python code\n\nUsing the reference code snippets provided below as examples, design a new and unique Python code\n\nsnippet that demands deep algorithmic reasoning to deduce the output from the input. Your\n\nsnippet that demands deep algorithmic reasoning to deduce the output from the input. Your\n\n!\u2192\n\n!\n\n\u2192\n\nsubmission should include a code snippet and a test input pair, where the input will be\n\nsubmission should include a code snippet and a test input pair, where the input will be\n\n!\u2192\n\n!\n\n\u2192\n\nplugged into the code snippet to produce the output. The input will be given to a test\n\nplugged into the code snippet to produce the output. The input will be given to a test\n\n!\u2192\n\n!\n\n\u2192\n\nsubject to deduce the output, which is meant to be an I.Q. test.\n\nsubject to deduce the output, which is meant to be an I.Q. test.\n\n!\u2192\n\n!\n\n\u2192\n\n### Code Requirements:\n\n### Code Requirements:\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n!\u2192 `f`\n\n`f`\n\n!\n\n\u2192\n\n- Ensure the function returns a value\n\n- Ensure the function returns a value\n\n- Include at least one input parameter\n\n- Include at least one input parameter\n\n- Make the function deterministic\n\n- Make the function deterministic\n\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n\n- Make the snippet require state tracking across multiple data transformations, ensuring the task\n\n!\u2192 requires long multi step reasoning\n\nrequires long multi step reasoning\n\n!\n\n\u2192\n\n- AVOID THE FOLLOWING:\n\n- AVOID THE FOLLOWING:\n\n* Random functions or variables\n\n* Random functions or variables\n\n* Date/time operations\n\n* Date/time operations\n\n* I/O operations (reading files, network requests)\n\n* I/O operations (reading files, network requests)\n\n* Printing or logging\n\n* Printing or logging\n\n* Any external state\n\n* Any external state\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- The snippet should end with a return statement from the main function `f`, anything after will\n\n- The snippet should end with a return statement from the main function `f`, anything after will\n\n!\u2192 be removed\n\nbe removed\n\n!\n\n\u2192\n\n### Input Requirements:\n\n### Input Requirements:\n\n- Provide exactly one test input for your function\n\n- Provide exactly one test input for your function\n\n- Format multiple arguments with commas between them\n\n- Format multiple arguments with commas between them\n\n- Remember to add quotes around string arguments\n\n- Remember to add quotes around string arguments\n\n### Formatting:\n\n### Formatting:\n\n- Format your code with:\n\n- Format your code with:\n\n```python\n\n```python\n\ndef f(...):\n\ndef f(...):\n\n# your code here\n\n# your code here\n\nreturn ...\n\nreturn ...\n\n```\n\n```\n\n- Format your input with:\n\n- Format your input with:\n\n```input\n\n```input\n\narg1, arg2, ...\n\narg1, arg2, ...\n\n```\n\n```\n\n### Example Format:\n\n### Example Format:\n\n```python\n\n```python\n\ndef f(name: str, info: dict):\n\ndef f(name: str, info: dict):\n\n# code logic here\n\n# code logic here\n\nreturn result\n\nreturn result\n\n```\n\n```\n\n```input\n\n```input\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n### Evaluation Criteria:\n\n### Evaluation Criteria:\n\n- Executability, your code should be executable given your input\n\n- Executability, your code should be executable given your input\n\n- Difficulty in predicting your ```input``` from 1) your ```python``` code and 2) the\n\n- Difficulty in predicting your ```input``` from 1) your ```python``` code and 2) the\n\ndeterministic ```output``` that will be obtained from your ```input```. Focus on either\n\ndeterministic ```output``` that will be obtained from your ```input```. Focus on either\n\n!\u2192\n\n!\n\n\u2192\n\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\n\nalgorithmic reasoning or logic complexity. For example, you can define complex data structure\n\n!\u2192\n\n!\n\n\u2192\n\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\n\nclasses and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex\n\n!\u2192\n\n!\n\n\u2192\n\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n\ncontrol flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc\n\n!\u2192\n\n!\n\n\u2192\n\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n\n- Creativity, the code needs to be sufficiently different from the provided reference snippets\n\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n\n- Restricted usage of certain keywords and packages, you are not allowed to use the following\n\n!\u2192 words in any form, even in comments: {LIST_OF_FORBIDDEN_PACKAGES}\n\nwords in any form, even in comments:\n\n{LIST_OF_FORBIDDEN_PACKAGES}\n\n!\n\n\u2192\n\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\n\nFirst, carefully devise a clear plan: e.g., identify how your snippet will be challenging,\n\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\n\ndistinct from reference snippets, and creative. Then, write the final code snippet and its\n\n!\u2192\n\n!\n\n\u2192\n\ninputs.\n\ninputs.\n\n!\u2192\n\n!\n\n\u2192\n\n### Reference Code Snippets:\n\n### Reference Code Snippets:\n\n{CODE_REFERENCES_FROM_BUFFER}\n\n{CODE_REFERENCES_FROM_BUFFER}\n\nFigure 35. Program Output Deduction Task\u2014Problem Generation Instruction.\nFigure 35. Program Output Deduction Task\u2014Problem Generation Instruction.\n\nFigure 35. Program Output Deduction Task\u2014Problem Generation Instruction.\n\nFigure 35.\n\nProgram Output Deduction Task\n\nProblem Generation Instruction.\n\n\u2014\n\nFigure 35. Program Output Deduction Task\u2014Problem Generation Instruction.\n\nProgram Output Deduction Task\u2014Problem Generation Instruction.\n\nFigure 35.\n\n42\n41\n\n42\n\n42\n\n41\n\n41",
        "tables": []
      },
      {
        "page_num": 41,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n## Task: Output {NUM_INPUTS} Inputs that can be plugged into the following Code Snippet to\n!\u2192 produce diverse Outputs, and give a message related to the given snippet.\nUsing the code snippet provided below, design {NUM_INPUTS} inputs that can be plugged into the\ncode snippet to produce a diverse set of outputs. A subset of your given input and its\ndeterministically produced outputs will be given to a test subject to deduce the function,\nwhich is meant to be an I.Q. test. You can also leave a message to the test subject to help\nthem deduce the code snippet.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n### Input Requirements:\n- Provide {NUM_INPUTS} valid inputs for the code snippet\n- For each input, format multiple arguments with commas between them\n- Remember to add quotes around string arguments\n- Each input should be individually wrapped in ```input``` tags\n### Message Requirements:\n- Leave a message to the test subject to help them deduce the code snippet\n- The message should be wrapped in ```message``` tags\n- The message can be in any form, can even be formed into a coding question, or a natural\n!\u2192 language instruction what the code snippet does\n- You cannot provide the code snippet in the message\n### Formatting:\n- Format your input with:\n```input\narg1, arg2, ...\n```\n### Example Format:\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n```input\n'Sammy', {{'age': 37, 'city': 'Los Angeles'}}\n```\n### Evaluation Criteria:\n- Executability, your code should be executable given your inputs\n- Coverage, the inputs and outputs should cover the whole input space of the code snippet, able\n!\u2192 to deduce the code snippet from the inputs and outputs\n- Creativity, the inputs need to be sufficiently different from each other\n- The overall selection of inputs and message combined should be challenging for the test\n!\u2192 subject, but not impossible for them to solve\nFirst, carefully devise a clear plan: e.g., understand the code snippet, then identify how your\nproposed inputs have high coverage, and why the inputs will be challenging and creative.\nThen, write the inputs and message. Remember to wrap your inputs in ```input``` tags, and\nyour message in ```message``` tags.\n!\u2192\n!\u2192\n!\u2192\n### Code Snippet:\n```python\n{SNIPPET_FROM_BUFFER}\n```\n\n## Task: Output {NUM_INPUTS} Inputs that can be plugged into the following Code Snippet to\n\n## Task: Output\n\n{NUM_INPUTS}\n\nInputs that can be plugged into the following Code Snippet to\n\n!\u2192 produce diverse Outputs, and give a message related to the given snippet.\n\nproduce diverse Outputs, and give a message related to the given snippet.\n\n!\n\n\u2192\n\nUsing the code snippet provided below, design {NUM_INPUTS} inputs that can be plugged into the\n\nUsing the code snippet provided below, design\n\n{NUM_INPUTS}\n\ninputs that can be plugged into the\n\ncode snippet to produce a diverse set of outputs. A subset of your given input and its\n\ncode snippet to produce a diverse set of outputs. A subset of your given input and its\n\n!\u2192\n\n!\n\n\u2192\n\ndeterministically produced outputs will be given to a test subject to deduce the function,\n\ndeterministically produced outputs will be given to a test subject to deduce the function,\n\n!\u2192\n\n!\n\n\u2192\n\nwhich is meant to be an I.Q. test. You can also leave a message to the test subject to help\n\nwhich is meant to be an I.Q. test. You can also leave a message to the test subject to help\n\n!\u2192\n\n!\n\n\u2192\n\nthem deduce the code snippet.\n\nthem deduce the code snippet.\n\n!\u2192\n\n!\n\n\u2192\n\n### Input Requirements:\n\n### Input Requirements:\n\n- Provide {NUM_INPUTS} valid inputs for the code snippet\n\n- Provide\n\n{NUM_INPUTS}\n\nvalid inputs for the code snippet\n\n- For each input, format multiple arguments with commas between them\n\n- For each input, format multiple arguments with commas between them\n\n- Remember to add quotes around string arguments\n\n- Remember to add quotes around string arguments\n\n- Each input should be individually wrapped in ```input``` tags\n\n- Each input should be individually wrapped in ```input``` tags\n\n### Message Requirements:\n\n### Message Requirements:\n\n- Leave a message to the test subject to help them deduce the code snippet\n\n- Leave a message to the test subject to help them deduce the code snippet\n\n- The message should be wrapped in ```message``` tags\n\n- The message should be wrapped in ```message``` tags\n\n- The message can be in any form, can even be formed into a coding question, or a natural\n\n- The message can be in any form, can even be formed into a coding question, or a natural\n\n!\u2192 language instruction what the code snippet does\n\nlanguage instruction what the code snippet does\n\n!\n\n\u2192\n\n- You cannot provide the code snippet in the message\n\n- You cannot provide the code snippet in the message\n\n### Formatting:\n\n### Formatting:\n\n- Format your input with:\n\n- Format your input with:\n\n```input\n\n```input\n\narg1, arg2, ...\n\narg1, arg2, ...\n\n```\n\n```\n\n### Example Format:\n\n### Example Format:\n\n```input\n\n```input\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n```input\n\n```input\n\n'Sammy', {{'age': 37, 'city': 'Los Angeles'}}\n\n'Sammy',\n\n{{'age': 37, 'city': 'Los Angeles'}}\n\n```\n\n```\n\n### Evaluation Criteria:\n\n### Evaluation Criteria:\n\n- Executability, your code should be executable given your inputs\n\n- Executability, your code should be executable given your inputs\n\n- Coverage, the inputs and outputs should cover the whole input space of the code snippet, able\n\n- Coverage, the inputs and outputs should cover the whole input space of the code snippet, able\n\n!\u2192 to deduce the code snippet from the inputs and outputs\n\nto deduce the code snippet from the inputs and outputs\n\n!\n\n\u2192\n\n- Creativity, the inputs need to be sufficiently different from each other\n\n- Creativity, the inputs need to be sufficiently different from each other\n\n- The overall selection of inputs and message combined should be challenging for the test\n\n- The overall selection of inputs and message combined should be challenging for the test\n\n!\u2192 subject, but not impossible for them to solve\n\nsubject, but not impossible for them to solve\n\n!\n\n\u2192\n\nFirst, carefully devise a clear plan: e.g., understand the code snippet, then identify how your\n\nFirst, carefully devise a clear plan: e.g., understand the code snippet, then identify how your\n\nproposed inputs have high coverage, and why the inputs will be challenging and creative.\n\nproposed inputs have high coverage, and why the inputs will be challenging and creative.\n\n!\u2192\n\n!\n\n\u2192\n\nThen, write the inputs and message. Remember to wrap your inputs in ```input``` tags, and\n\nThen, write the inputs and message. Remember to wrap your inputs in ```input``` tags, and\n\n!\u2192\n\n!\n\n\u2192\n\nyour message in ```message``` tags.\n\nyour message in ```message``` tags.\n\n!\u2192\n\n!\n\n\u2192\n\n### Code Snippet:\n\n### Code Snippet:\n\n```python\n\n```python\n\n{SNIPPET_FROM_BUFFER}\n\n{SNIPPET_FROM_BUFFER}\n\n```\n\n```\n\nFigure 36. Program Induction Task\u2014Problem Proposal Instruction.\nFigure 36. Program Induction Task\u2014Problem Proposal Instruction.\n\nFigure 36. Program Induction Task\u2014Problem Proposal Instruction.\n\nFigure 36.\n\nProgram Induction Task\n\nProblem Proposal Instruction.\n\n\u2014\n\nFigure 36. Program Induction Task\u2014Problem Proposal Instruction.\n\nProgram Induction Task\u2014Problem Proposal Instruction.\n\nFigure 36.\n\n43\n42\n\n43\n\n43\n\n42\n\n42",
        "tables": []
      },
      {
        "page_num": 42,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\nGiven the following Code Snippet and the Output, think step by step then provide one possible\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\nseparate them with commas.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Output:\n```output\n{OUTPUT}\n```\n# Output Format:\n```input\narg1, arg2, ...\n```\n# Example Output:\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\nGiven the following Code Snippet and the Output, think step by step then provide one possible\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\nseparate them with commas.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Output:\n```output\n{OUTPUT}\n```\n# Output Format:\n```input\narg1, arg2, ...\n```\n# Example Output:\n```input\n'John', {{'age': 20, 'city': 'New York'}}\n```\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\n\n# Task: Provide One Possible Input of a Python Code Snippet Given the Code and Output\n\nGiven the following Code Snippet and the Output, think step by step then provide one possible\n\nGiven the following Code Snippet and the Output, think step by step then provide one possible\n\nGiven the following Code Snippet and the Output, think step by step then provide one possible\n\nGiven the following Code Snippet and the Output, think step by step then provide one possible\n\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\n\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\n\n!\u2192\n\n!\n\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\n\ninput that produced the output. The input needs to be wrapped in ```input``` tags. Remember\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\n\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\n\n!\u2192\n\n!\n\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\n\nif an argument is a string, wrap it in quotes. If the function requires multiple arguments,\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\nseparate them with commas.\n\nseparate them with commas.\n\n!\u2192\n\n!\n\nseparate them with commas.\n\nseparate them with commas.\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\n# Code Snippet:\n\n# Code Snippet:\n\n# Code Snippet:\n\n# Code Snippet:\n\n```python\n\n```python\n\n```python\n\n```python\n\n{SNIPPET}\n\n{SNIPPET}\n\n{SNIPPET}\n\n{SNIPPET}\n\n```\n\n```\n\n```\n\n```\n\n# Output:\n\n# Output:\n\n# Output:\n\n# Output:\n\n```output\n\n```output\n\n```output\n\n```output\n\n{OUTPUT}\n\n{OUTPUT}\n\n{OUTPUT}\n\n{OUTPUT}\n\n```\n\n```\n\n```\n\n```\n\n# Output Format:\n\n# Output Format:\n\n# Output Format:\n\n# Output Format:\n\n```input\n\n```input\n\n```input\n\n```input\n\narg1, arg2, ...\n\narg1, arg2, ...\n\narg1, arg2, ...\n\narg1, arg2, ...\n\n```\n\n```\n\n```\n\n```\n\n# Example Output:\n\n# Example Output:\n\n# Example Output:\n\n# Example Output:\n\n```input\n\n```input\n\n```input\n\n```input\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n'John', {{'age': 20, 'city': 'New York'}}\n\n'John',\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n```\n\n```\n\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\n\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\n\nFigure 37.\n\nProgram Input Abduction Task\n\nProblem Solving Prompt.\n\n\u2014\n\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\n\nFigure 37.\n\nFigure 37. Program Input Abduction Task\u2014Problem Solving Prompt.\n\nProgram Input Abduction Task\u2014Problem Solving Prompt.\n\nFigure 37.\n\nProgram Input Abduction Task\n\nProblem Solving Prompt.\n\n\u2014\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\nreturns multiple values, remember to use a tuple to wrap them.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Input:\n```input\n{INPUT}\n```\n# Example Output:\n```output\n{{'age': 20, 'city': 'New York'}}\n```\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\nreturns multiple values, remember to use a tuple to wrap them.\n!\u2192\n!\u2192\n!\u2192\n# Code Snippet:\n```python\n{SNIPPET}\n```\n# Input:\n```input\n{INPUT}\n```\n# Example Output:\n```output\n{{'age': 20, 'city': 'New York'}}\n```\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\n\n# Task: Deduce the Output of a Python Code Snippet Given the Code and Input\n\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\n\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\n\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\n\nGiven the following Code Snippet and the Input, think step by step then deduce the output that\n\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n\n!\u2192\n\n!\n\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n\nwill be produced from plugging the Input into the Code Snippet. Put your output in\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\n\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\n\n!\u2192\n\n!\n\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\n\n```output``` tags. Remember if the output is a string, wrap it in quotes. If the function\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\nreturns multiple values, remember to use a tuple to wrap them.\n\nreturns multiple values, remember to use a tuple to wrap them.\n\n!\u2192\n\n!\n\nreturns multiple values, remember to use a tuple to wrap them.\n\nreturns multiple values, remember to use a tuple to wrap them.\n\n\u2192\n\n!\u2192\n\n!\n\n\u2192\n\n# Code Snippet:\n\n# Code Snippet:\n\n# Code Snippet:\n\n# Code Snippet:\n\n```python\n\n```python\n\n```python\n\n```python\n\n{SNIPPET}\n\n{SNIPPET}\n\n{SNIPPET}\n\n{SNIPPET}\n\n```\n\n```\n\n```\n\n```\n\n# Input:\n\n# Input:\n\n# Input:\n\n# Input:\n\n```input\n\n```input\n\n```input\n\n```input\n\n{INPUT}\n\n{INPUT}\n\n{INPUT}\n\n{INPUT}\n\n```\n\n```\n\n```\n\n```\n\n# Example Output:\n\n# Example Output:\n\n# Example Output:\n\n# Example Output:\n\n```output\n\n```output\n\n```output\n\n```output\n\n{{'age': 20, 'city': 'New York'}}\n\n{{'age': 20, 'city': 'New York'}}\n\n{{'age': 20, 'city': 'New York'}}\n\n{{'age': 20, 'city': 'New York'}}\n\n```\n\n```\n\n```\n\n```\n\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\n\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\n\nFigure 38.\n\nProgram Output Deduction Task\n\nProblem Solving Prompt.\n\n\u2014\n\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\n\nFigure 38.\n\nProgram Output Deduction Task\n\nProblem Solving Prompt.\n\n\u2014\n\nFigure 38. Program Output Deduction Task\u2014Problem Solving Prompt.\n\nProgram Output Deduction Task\u2014Problem Solving Prompt.\n\nFigure 38.\n\n44\n44\n43\n\n44\n\n44\n\n43\n\n43\n\n44\n\n44",
        "tables": []
      },
      {
        "page_num": 43,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\n# Task: Deduce the Function that Produced the Outputs from the Inputs\nGiven a set of input/output pairs and a message that describes the function, think through the\nproblem step by step to deduce a general code snippet. This code should produce the hidden\noutputs from the hidden inputs, matching the original data-generating code that created the\ninput/output pairs. Place your final answer inside python tags! It may be helpful to work\nthrough each input/output pair individually to test your function. If your function doesn't\nwork as expected, revise it until it does. The final code snippet will be used to evaluate\nyour response, which is wrapped in ```python``` tags.\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n!\u2192\n# Code Requirements:\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n!\u2192 `f`\n- Ensure the function returns a value\n- Include at least one input parameter\n- Make the function deterministic\n- AVOID THE FOLLOWING:\n* Random functions or variables\n* Date/time operations\n* I/O operations (reading files, network requests)\n* Printing or logging\n* Any external state\n- Ensure execution completes within 10 seconds on a modern CPU\n- All imports and class definitions should be at the very top of the code snippet\n- The snippet should end with a return statement from the main function `f()`, anything after\n!\u2192 will be removed\n# Input and Output Pairs:\n{INPUT_OUTPUT_PAIRS}\n# Message:\n```message\n{MESSAGE}\n```\n# Example Output:\n```python\ndef f(a):\nreturn a\n```\nName your entry function `f()`!!!\n\n# Task: Deduce the Function that Produced the Outputs from the Inputs\n\n# Task: Deduce the Function that Produced the Outputs from the Inputs\n\nGiven a set of input/output pairs and a message that describes the function, think through the\n\nGiven a set of input/output pairs and a message that describes the function, think through the\n\nproblem step by step to deduce a general code snippet. This code should produce the hidden\n\nproblem step by step to deduce a general code snippet. This code should produce the hidden\n\n!\u2192\n\n!\n\n\u2192\n\noutputs from the hidden inputs, matching the original data-generating code that created the\n\noutputs from the hidden inputs, matching the original data-generating code that created the\n\n!\u2192\n\n!\n\n\u2192\n\ninput/output pairs. Place your final answer inside python tags! It may be helpful to work\n\ninput/output pairs. Place your final answer inside python tags! It may be helpful to work\n\n!\u2192\n\n!\n\n\u2192\n\nthrough each input/output pair individually to test your function. If your function doesn't\n\n'\n\nthrough each input/output pair individually to test your function. If your function doesn\n\nt\n\n!\u2192\n\n!\n\n\u2192\n\nwork as expected, revise it until it does. The final code snippet will be used to evaluate\n\nwork as expected, revise it until it does. The final code snippet will be used to evaluate\n\n!\u2192\n\n!\n\n\u2192\n\nyour response, which is wrapped in ```python``` tags.\n\nyour response, which is wrapped in ```python``` tags.\n\n!\u2192\n\n!\n\n\u2192\n\n# Code Requirements:\n\n# Code Requirements:\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n- Name the entry function `f` (e.g., `def f(...): ...`), you can have nested definitions inside\n\n!\u2192 `f`\n\n`f`\n\n!\n\n\u2192\n\n- Ensure the function returns a value\n\n- Ensure the function returns a value\n\n- Include at least one input parameter\n\n- Include at least one input parameter\n\n- Make the function deterministic\n\n- Make the function deterministic\n\n- AVOID THE FOLLOWING:\n\n- AVOID THE FOLLOWING:\n\n* Random functions or variables\n\n* Random functions or variables\n\n* Date/time operations\n\n* Date/time operations\n\n* I/O operations (reading files, network requests)\n\n* I/O operations (reading files, network requests)\n\n* Printing or logging\n\n* Printing or logging\n\n* Any external state\n\n* Any external state\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- Ensure execution completes within 10 seconds on a modern CPU\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- All imports and class definitions should be at the very top of the code snippet\n\n- The snippet should end with a return statement from the main function `f()`, anything after\n\n- The snippet should end with a return statement from the main function `f()`, anything after\n\n!\u2192 will be removed\n\nwill be removed\n\n!\n\n\u2192\n\n# Input and Output Pairs:\n\n# Input and Output Pairs:\n\n{INPUT_OUTPUT_PAIRS}\n\n{INPUT_OUTPUT_PAIRS}\n\n# Message:\n\n# Message:\n\n```message\n\n```message\n\n{MESSAGE}\n\n{MESSAGE}\n\n```\n\n```\n\n# Example Output:\n\n# Example Output:\n\n```python\n\n```python\n\ndef f(a):\n\ndef f(a):\n\nreturn a\n\nreturn a\n\n```\n\n```\n\nName your entry function `f()`!!!\n\nName your entry function `f()`!!!\n\nFigure 39. Program Induction Task\u2014Problem Solving Prompt.\nFigure 39. Program Induction Task\u2014Problem Solving Prompt.\n\nFigure 39. Program Induction Task\u2014Problem Solving Prompt.\n\nFigure 39.\n\nProgram Induction Task\n\nProblem Solving Prompt.\n\n\u2014\n\nFigure 39. Program Induction Task\u2014Problem Solving Prompt.\n\nProgram Induction Task\u2014Problem Solving Prompt.\n\nFigure 39.\n\n45\n44\n\n45\n\n45\n\n44\n\n44",
        "tables": []
      },
      {
        "page_num": 44,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Manual Constructed Sudoku Abduction Task\n\nTask: Manual Constructed Sudoku Abduction Task\n\nTask: Manual Constructed Sudoku Abduction Task\n\nModel Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\nimport random\nfrom typing import List\nimport copy\ndef g_0(board: List[List[str]]) -> bool:\ndef is_valid_block(block):\nreturn sorted(block) == [str(i) for i in range(1, 10)]\nfor row in board:\nif not is_valid_block(row):\nreturn False\nfor col in zip(*board):\nif not is_valid_block(col):\nreturn False\nfor box_row in range(0, 9, 3):\nfor box_col in range(0, 9, 3):\nblock = [\nboard[r][c]\nfor r in range(box_row, box_row + 3)\nfor c in range(box_col, box_col + 3)\n]\nif not is_valid_block(block):\nreturn False\nreturn True\ndef f(board: List[List[str]]) -> List[List[str]]:\nfor i in range(9):\nfor j in range(9):\nif board[i][j] == \".\":\nraise ValueError(\"Invalid board\")\nnum_to_mask = 51\nif not g_0(board):\nraise ValueError(\"Invalid board\")\nmasked_board = copy.deepcopy(board)\nall_positions = [(i, j) for i in range(9) for j in range(9)]\nrandom.shuffle(all_positions)\nfor i, j in all_positions[:num_to_mask]:\nmasked_board[i][j] = \".\"\nreturn masked_board\n45\n\nModel Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n\nHere is the function\n\nand the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n\nModel Input:\n\nf\n\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\nimport random\n\nimport\n\nrandom\n\nfrom typing import List\n\nfrom\n\ntyping\n\nimport\n\nList\n\nimport copy\n\nimport\n\ncopy\n\ndef g_0(board: List[List[str]]) -> bool:\n\ndef\n\ng_0(board: List[List[str]])\n\n->\n\nbool:\n\ndef is_valid_block(block):\n\ndef\n\nis_valid_block(block):\n\nreturn sorted(block) == [str(i) for i in range(1, 10)]\n\nreturn\n\nfor\n\nin\n\nsorted(block)\n\n==\n\n[str(i)\n\ni\n\nrange(1,\n\n10)]\n\nfor row in board:\n\nfor\n\nin\n\nrow\n\nboard:\n\nif not is_valid_block(row):\n\nif\n\nnot\n\nis_valid_block(row):\n\nreturn False\n\nreturn\n\nFalse\n\nfor col in zip(*board):\n\nfor\n\nin\n\ncol\n\nzip(*board):\n\nif not is_valid_block(col):\n\nif\n\nnot\n\nis_valid_block(col):\n\nreturn False\n\nreturn\n\nFalse\n\nfor box_row in range(0, 9, 3):\n\nfor\n\nin\n\nbox_row\n\nrange(0,\n\n9,\n\n3):\n\nfor box_col in range(0, 9, 3):\n\nfor\n\nin\n\nbox_col\n\nrange(0,\n\n9,\n\n3):\n\nblock = [\n\nblock\n\n=\n\n[\n\nboard[r][c]\n\nboard[r][c]\n\nfor r in range(box_row, box_row + 3)\n\nfor\n\nin\n\nr\n\nrange(box_row, box_row\n\n+\n\n3)\n\nfor c in range(box_col, box_col + 3)\n\nfor\n\nin\n\nc\n\nrange(box_col, box_col\n\n+\n\n3)\n\n]\n\n]\n\nif not is_valid_block(block):\n\nif\n\nnot\n\nis_valid_block(block):\n\nreturn False\n\nreturn\n\nFalse\n\nreturn True\n\nreturn\n\nTrue\n\ndef f(board: List[List[str]]) -> List[List[str]]:\n\ndef\n\nf(board: List[List[str]])\n\n->\n\nList[List[str]]:\n\nfor i in range(9):\n\nfor\n\nin\n\ni\n\nrange(9):\n\nfor j in range(9):\n\nfor\n\nin\n\nj\n\nrange(9):\n\nif board[i][j] == \".\":\n\nif\n\nboard[i][j]\n\n==\n\n\".\":\n\nraise ValueError(\"Invalid board\")\n\nraise\n\nValueError\n\n(\"Invalid board\")\n\nnum_to_mask = 51\n\nnum_to_mask\n\n=\n\n51\n\nif not g_0(board):\n\nif\n\nnot\n\ng_0(board):\n\nraise ValueError(\"Invalid board\")\n\nraise\n\nValueError\n\n(\"Invalid board\")\n\nmasked_board = copy.deepcopy(board)\n\nmasked_board\n\n=\n\ncopy.deepcopy(board)\n\nall_positions = [(i, j) for i in range(9) for j in range(9)]\n\nfor\n\nin\n\nfor\n\nin\n\nall_positions\n\n=\n\n[(i, j)\n\ni\n\nrange(9)\n\nj\n\nrange(9)]\n\nrandom.shuffle(all_positions)\n\nrandom.shuffle(all_positions)\n\nfor i, j in all_positions[:num_to_mask]:\n\nfor\n\nin\n\ni, j\n\nall_positions[:num_to_mask]:\n\nmasked_board[i][j] = \".\"\n\nmasked_board[i][j]\n\n=\n\n\".\"\n\nreturn masked_board\n\nreturn\n\nmasked_board\n\n45\n\n45\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nwhat it does. Let's break down the function step by step:\n\nwhat it does. Let's break down the function step by step:\n\nwhat it does. Let's break down the function step by step:\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\nis not valid, 'g_0' returns 'False'.\n\nis not valid, 'g_0' returns 'False'.\n\nis not valid, 'g_0' returns 'False'.\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\n\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\n- The masked board is then returned.\n\n- The masked board is then returned.\n\n- The masked board is then returned.\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"],[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"],[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"],[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n\n\"' We can see that the board has 51 positions masked with \".\". Therefore, the input board must have been a\n\n\"' We can see that the board has 51 positions masked with \".\". Therefore, the input board must have been a\n\n\"' We can see that the board has 51 positions masked with \".\". Therefore, the input board must have been a\n\ncomplete board, and 'f' masked 51 random positions.\n\ncomplete board, and 'f' masked 51 random positions.\n\ncomplete board, and 'f' masked 51 random positions.\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nsuch that after masking 51 positions, we get the output shown.\n\nsuch that after masking 51 positions, we get the output shown.\n\nsuch that after masking 51 positions, we get the output shown.\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\n\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\n\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\n\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\n\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\n\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\nboard will produce the given output.\n\nboard will produce the given output.\n\nboard will produce the given output.\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nFigure 40. Vibe Check 1: Sudoku.\n\nFigure 40. Vibe Check 1: Sudoku.\n\nFigure 40.\n\nVibe Check 1: Sudoku.",
        "tables": []
      },
      {
        "page_num": 45,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Manual Constructed Sudoku Abduction Task\n\nTask: Manual Constructed Sudoku Abduction Task\n\nTask: Manual Constructed Sudoku Abduction Task\n\nModel Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\nModel Input: Here is the function f and the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n\nHere is the function\n\nand the output: [ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],\n\nModel Input:\n\nf\n\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n\n[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],\n\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\n[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\n[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"] ], please predict the input.\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\nwhat it does. Let's break down the function step by step:\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nTo determine the input that produced the given output, we need to analyze the function 'f' and understand\n\nwhat it does. Let's break down the function step by step:\n\nwhat it does. Let's break down the function step by step:\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\n1. **Input Validation in 'g_0'**: - The function 'g_0' checks if the given board is valid. - A board is valid\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\nis not valid, 'g_0' returns 'False'.\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\nif each row, each column, and each 3x3 subgrid contains the digits 1 through 9 exactly once. - If the board\n\nis not valid, 'g_0' returns 'False'.\n\nis not valid, 'g_0' returns 'False'.\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\n2. **Input Validation in 'f'**: - The function 'f' frst checks if there are any \".\" in the board. If there are, it\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\nraises a 'ValueError'. - This means the input board to 'f' must be a complete board with no \".\".\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n- The masked board is then returned.\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\n\n3. **Masking the Board**: - If the board is valid (i.e., 'g_0(board)' returns 'True'), the function 'f' proceeds\n\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\nto mask the board. - It creates a copy of the board and then randomly selects 51 positions to mask with \".\".\n\n- The masked board is then returned.\n\n- The masked board is then returned.\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"], [\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],\n[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"], [\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],\n[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"], [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],\n[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] \"' We can see that the board has 51 positions masked with \".\". Therefore,\nthe input board must have been a complete board, and 'f' masked 51 random positions.\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"], [\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],\n\nGiven the output: \"'output [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"], [\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],\n\n[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"], [\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],\n\n[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"], [\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],\n\n[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"], [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],\n\n[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"], [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],\n\n[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] \"' We can see that the board has 51 positions masked with \".\". Therefore,\n\n[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] \"' We can see that the board has 51 positions masked with \".\". Therefore,\n\nthe input board must have been a complete board, and 'f' masked 51 random positions.\n\nthe input board must have been a complete board, and 'f' masked 51 random positions.\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\nsuch that after masking 51 positions, we get the output shown.\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nLet's construct a complete board that, when masked, can produce the given output. We need to fnd a board\n\nsuch that after masking 51 positions, we get the output shown.\n\nsuch that after masking 51 positions, we get the output shown.\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\nHere is a complete board that matches the structure of the output: \"'python [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\",\n\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\n\"9\", \"1\", \"2\"], [\"6\", \"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\",\n\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\n\"5\", \"9\", \"7\", \"6\", \"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\",\n\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\n\"4\", \"8\", \"5\", \"6\"], [\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"],\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n[\"3\", \"4\", \"5\", \"2\", \"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\nLet's verify that this board, when passed to the function 'f', produces the given output.\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\n\n1. The board is complete with no \".\". 2. The board is valid because each row, each column, and each 3x3\n\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\n\nsubgrid contains the digits 1 through 9 exactly once. 3. The function 'f' will mask 51 random positions.\n\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\n\nGiven the random nature of the masking, we need to ensure that the positions masked match the positions\n\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\nof \".\" in the output. Since the positions are random, we can assume that the function 'f' with this complete\n\nboard will produce the given output.\n\nboard will produce the given output.\n\nboard will produce the given output.\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\nTherefore, the input that produced the output is: \"'input [ [\"5\", \"3\", \"4\", \"6\", \"7\", \"8\", \"9\", \"1\", \"2\"], [\"6\",\n\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\n\"7\", \"2\", \"1\", \"9\", \"5\", \"3\", \"4\", \"8\"], [\"1\", \"9\", \"8\", \"3\", \"4\", \"2\", \"5\", \"6\", \"7\"], [\"8\", \"5\", \"9\", \"7\", \"6\",\n\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n\n\"1\", \"4\", \"2\", \"3\"], [\"4\", \"2\", \"6\", \"8\", \"5\", \"3\", \"7\", \"9\", \"1\"], [\"7\", \"1\", \"3\", \"9\", \"2\", \"4\", \"8\", \"5\", \"6\"],\n\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\n[\"9\", \"6\", \"1\", \"5\", \"3\", \"7\", \"2\", \"8\", \"4\"], [\"2\", \"8\", \"7\", \"4\", \"1\", \"9\", \"6\", \"3\", \"5\"], [\"3\", \"4\", \"5\", \"2\",\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\n\"8\", \"6\", \"1\", \"7\", \"9\"] ] \"'\n\nFigure 40. Vibe Check 1: Sudoku.\nFigure 40. Vibe Check 1: Sudoku Solver. We cast Sudoku solving as an abduction task: our program starts from a fully solved and\nvalidated Sudoku board and simulates the masking of 51 random cells. The masked board is then presented as output, and the model\nis tasked with inferring the original input \u2014 effectively solving the puzzle backwards. An example solution is shown above, where\nAZR-Coder-14b verifies its initial guess before correctly answering. Generation parameters: temperature=0.6.\n\nFigure 40. Vibe Check 1: Sudoku Solver. We cast Sudoku solving as an abduction task: our program starts from a fully solved and\n\nWe cast Sudoku solving as an abduction task: our program starts from a fully solved and\n\nVibe Check 1: Sudoku Solver.\n\nFigure 40.\n\nFigure 40. Vibe Check 1: Sudoku.\n\nFigure 40.\n\nVibe Check 1: Sudoku.\n\nvalidated Sudoku board and simulates the masking of 51 random cells. The masked board is then presented as output, and the model\n\nvalidated Sudoku board and simulates the masking of 51 random cells. The masked board is then presented as output, and the model\n\nis tasked with inferring the original input \u2014 effectively solving the puzzle backwards. An example solution is shown above, where\n\nis tasked with inferring the original input \u2014 effectively solving the puzzle backwards. An example solution is shown above, where\n\nAZR-Coder-14b verifies its initial guess before correctly answering. Generation parameters: temperature=0.6.\n\nverifies its initial guess before correctly answering. Generation parameters: temperature=0.6.\n\nAZR-Coder-14b\n\n46\n\n46\n\n46\n\n46\n\n46\n\n46",
        "tables": []
      },
      {
        "page_num": 46,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\nModel Input: Here is the function f and the output: 'True', please predict the input.\ndef g_3():\nreturn [(x, y) for x in range(2, 101) for y in range(x + 1, 101) if x + y <= 100]\ndef g_2(pairs):\nd = {}\nfor (x, y) in pairs:\ns = x + y\nd.setdefault(s, []).append((x, y))\nreturn d\ndef g_1(pairs):\nd = {}\nfor (x, y) in pairs:\np = x * y\nd.setdefault(p, []).append((x, y))\nreturn d\ndef g_0():\npairs = g_3()\nsum_dict = g_2(pairs)\nprod_dict = g_1(pairs)\nallowed_sums = [s for s, candidate_pairs in sum_dict.items() \\\nif all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]\ncandidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]\nnew_prod_dict = g_1(candidates_after_S)\nunique_products = {prod: pair_list for prod, pair_list in new_prod_dict.items() \\\nif len(pair_list) == 1}\nfinal_candidates = []\nfor s in allowed_sums:\nvalid_pairs = [pair for pair in candidates_after_S \\\nif pair[0] + pair[1] == s and (pair[0] * pair[1]) in unique_products]\nif len(valid_pairs) == 1:\nfinal_candidates.append(valid_pairs[0])\nreturn sorted(set(final_candidates))\ndef f(x, y):\nif not (isinstance(x, int) and isinstance(y, int)):\nreturn False\nif x <= 1 or y <= 1 or y <= x or (x + y) > 100:\nreturn False\nfinal_candidates = g_0()\nreturn len(final_candidates) == 1 and final_candidates[0] == (x, y)\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nModel Input: Here is the function f and the output: 'True', please predict the input.\n\nHere is the function\n\nand the output: 'True', please predict the input.\n\nModel Input:\n\nf\n\ndef g_3():\n\ndef\n\ng_3():\n\nreturn [(x, y) for x in range(2, 101) for y in range(x + 1, 101) if x + y <= 100]\n\nreturn\n\nfor\n\nin\n\nfor\n\nin\n\nif\n\n[(x, y)\n\nx\n\nrange(2,\n\n101)\n\ny\n\nrange(x\n\n+\n\n1,\n\n101)\n\nx\n\n+\n\ny\n\n<=\n\n100]\n\ndef g_2(pairs):\n\ndef\n\ng_2(pairs):\n\nd = {}\n\nd\n\n=\n\n{}\n\nfor (x, y) in pairs:\n\nfor\n\nin\n\n(x, y)\n\npairs:\n\ns = x + y\n\ns\n\n=\n\nx\n\n+\n\ny\n\nd.setdefault(s, []).append((x, y))\n\nd.setdefault(s, []).append((x, y))\n\nreturn d\n\nreturn\n\nd\n\ndef g_1(pairs):\n\ndef\n\ng_1(pairs):\n\nd = {}\n\nd\n\n=\n\n{}\n\nfor (x, y) in pairs:\n\nfor\n\nin\n\n(x, y)\n\npairs:\n\np = x * y\n\np\n\n=\n\nx\n\n*\n\ny\n\nd.setdefault(p, []).append((x, y))\n\nd.setdefault(p, []).append((x, y))\n\nreturn d\n\nreturn\n\nd\n\ndef g_0():\n\ndef\n\ng_0():\n\npairs = g_3()\n\npairs\n\n=\n\ng_3()\n\nsum_dict = g_2(pairs)\n\nsum_dict\n\n=\n\ng_2(pairs)\n\nprod_dict = g_1(pairs)\n\nprod_dict\n\n=\n\ng_1(pairs)\n\nallowed_sums = [s for s, candidate_pairs in sum_dict.items() \\\n\nfor\n\nin\n\nallowed_sums\n\n=\n\n[s\n\ns, candidate_pairs\n\nsum_dict.items() \\\n\nif all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]\n\nif\n\nfor\n\nin\n\nall(len(prod_dict[x\n\n*\n\ny])\n\n>\n\n1\n\n(x, y)\n\ncandidate_pairs)]\n\ncandidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]\n\nfor\n\nin\n\nif\n\nin\n\ncandidates_after_S\n\n=\n\n[pair\n\npair\n\npairs\n\npair[0]\n\n+\n\npair[1]\n\nallowed_sums]\n\nnew_prod_dict = g_1(candidates_after_S)\n\nnew_prod_dict\n\n=\n\ng_1(candidates_after_S)\n\nunique_products = {prod: pair_list for prod, pair_list in new_prod_dict.items() \\\n\nfor\n\nin\n\nunique_products\n\n=\n\n{prod: pair_list\n\nprod, pair_list\n\nnew_prod_dict.items() \\\n\nif len(pair_list) == 1}\n\nif\n\nlen(pair_list)\n\n==\n\n1}\n\nfinal_candidates = []\n\nfinal_candidates\n\n=\n\n[]\n\nfor s in allowed_sums:\n\nfor\n\nin\n\ns\n\nallowed_sums:\n\nvalid_pairs = [pair for pair in candidates_after_S \\\n\nfor\n\nin\n\nvalid_pairs\n\n=\n\n[pair\n\npair\n\ncandidates_after_S \\\n\nif pair[0] + pair[1] == s and (pair[0] * pair[1]) in unique_products]\n\nif\n\nand\n\nin\n\npair[0]\n\n+\n\npair[1]\n\n==\n\ns\n\n(pair[0]\n\n*\n\npair[1])\n\nunique_products]\n\nif len(valid_pairs) == 1:\n\nif\n\nlen(valid_pairs)\n\n==\n\n1:\n\nfinal_candidates.append(valid_pairs[0])\n\nfinal_candidates.append(valid_pairs[0])\n\nreturn sorted(set(final_candidates))\n\nreturn\n\nsorted(set(final_candidates))\n\ndef f(x, y):\n\ndef\n\nf(x, y):\n\nif not (isinstance(x, int) and isinstance(y, int)):\n\nif\n\nnot\n\nand\n\n(isinstance(x,\n\nint)\n\nisinstance(y,\n\nint)):\n\nreturn False\n\nreturn\n\nFalse\n\nif x <= 1 or y <= 1 or y <= x or (x + y) > 100:\n\nif\n\nor\n\nor\n\nor\n\nx\n\n<=\n\n1\n\ny\n\n<=\n\n1\n\ny\n\n<=\n\nx\n\n(x\n\n+\n\ny)\n\n>\n\n100:\n\nreturn False\n\nreturn\n\nFalse\n\nfinal_candidates = g_0()\n\nfinal_candidates\n\n=\n\ng_0()\n\nreturn len(final_candidates) == 1 and final_candidates[0] == (x, y)\n\nreturn\n\nand\n\nlen(final_candidates)\n\n==\n\n1\n\nfinal_candidates[0]\n\n==\n\n(x, y)\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n47\n\n47\n\n47\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n\nLet's analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\niterates through the list. Then, it checks for each pair of accumulated sums if their diference equals the target. If such a pair is found, it returns the accumulated sum at the frst index of the pair.\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nTo fnd one possible input that produces the output 1, we need to fnd a scenario where the diference between any two accumulated_values equals the target, and the frst accumulated_values that\n\nsatisfes this condition is 1.\nLet's consider a simple example:\n-numbers = [1, 2]\n-target = 1\n\nsatisfes this condition is 1.\n\nsatisfes this condition is 1.\n\nLet's consider a simple example:\n\nLet's consider a simple example:\n\n-numbers = [1, 2]\n\n-numbers = [1, 2]\n\n-target = 1\n\n-target = 1\n\nHere's the step-by-step calculation:\n\nHere's the step-by-step calculation:\n\nHere's the step-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nSince no pair satisfes the condition, the function will not return any value. This doesn't match our output. Let's try another example.\n\nConsider:\n\nConsider:\n\nConsider:\n\n-numbers = [1, 0]\n-target = 1\nStep-by-step calculation:\n\n-numbers = [1, 0]\n\n-numbers = [1, 0]\n\n-target = 1\n\n-target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n\n...(shortened)\n\n...(shortened)\n\n...(shortened)\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 3. This still doesn't match our output.\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\nGiven the complexity and the fact that no simple example seems to work, let's try a more complex scenario:\n\n-numbers = [2, -1, 1]\n-target = 1\nStep-by-step calculation:\n\n-numbers = [2, -1, 1]\n\n-numbers = [2, -1, 1]\n\n-target = 1\n\n-target = 1\n\nStep-by-step calculation:\n\nStep-by-step calculation:\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nThe pair (i=1, j=2) satisfes the condition, and the function returns the frst accumulated value in this pair, which is 1. This matches our output.\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\n\nFigure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\n\nFigure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\n\nFigure 41.\n\nA more detailed version of the example shown in main Figure\n\n7. The model is solving an\n\nSolve Abduction Task Example.\n\nabduction task (predict input).\n\nabduction task (predict input).\n\nabduction task (predict input).",
        "tables": []
      },
      {
        "page_num": 47,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nTask: Manually Constructed Sum Product Game Solve Abduction Task\n\nModel Input: Here is the function f and the output: 'True', please predict the input.\n\nModel Input: Here is the function f and the output: 'True', please predict the input.\n\nHere is the function\n\nand the output: 'True', please predict the input.\n\nModel Input:\n\nf\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nModel Output:\n\nModel Output:\n\nModel Output:\n\nthe function 'f(x, y)' returns 'True' if the following conditions are met:\n\nthe function 'f(x, y)' returns 'True' if the following conditions are met:\n\nthe function 'f(x, y)' returns 'True' if the following conditions are met:\n\n1. 'x' and 'y' are both integers.\n\n1. 'x' and 'y' are both integers.\n\n1. 'x' and 'y' are both integers.\n\n2. 'x' and 'y' satisfy '1 < x < y' and 'x + y <= 100'.\n\n2. 'x' and 'y' satisfy '1 < x < y' and 'x + y <= 100'.\n\n2. 'x' and 'y' satisfy '1 < x < y' and 'x + y <= 100'.\n\n3. The fnal candidates after applying the conditions in 'g_0' are exactly one pair, and that pair is '(x, y)'.\n\n3. The fnal candidates after applying the conditions in 'g_0' are exactly one pair, and that pair is '(x, y)'.\n\n3. The fnal candidates after applying the conditions in 'g_0' are exactly one pair, and that pair is '(x, y)'.\n\nLet's analyze the function 'g_0' step by step to understand what it does:\n\nLet's analyze the function 'g_0' step by step to understand what it does:\n\nLet's analyze the function 'g_0' step by step to understand what it does:\n\n1. 'pairs = g_3()' generates all pairs '(x, y)' such that '2 <= x < y <= 100' and 'x + y <= 100'.\n\n1. 'pairs = g_3()' generates all pairs '(x, y)' such that '2 <= x < y <= 100' and 'x + y <= 100'.\n\n1. 'pairs = g_3()' generates all pairs '(x, y)' such that '2 <= x < y <= 100' and 'x + y <= 100'.\n\n2. 'sum_dict = g_2(pairs)' creates a dictionary where the keys are the sums of the pairs and the values are lists of pairs that add up\nto that sum.\n\n2. 'sum_dict = g_2(pairs)' creates a dictionary where the keys are the sums of the pairs and the values are lists of pairs that add up\n\n2. 'sum_dict = g_2(pairs)' creates a dictionary where the keys are the sums of the pairs and the values are lists of pairs that add up\n\nto that sum.\n\nto that sum.\n\n3. 'prod_dict = g_1(pairs)' creates a dictionary where the keys are the products of the pairs and the values are lists of pairs that\n\n3. 'prod_dict = g_1(pairs)' creates a dictionary where the keys are the products of the pairs and the values are lists of pairs that\n\n3. 'prod_dict = g_1(pairs)' creates a dictionary where the keys are the products of the pairs and the values are lists of pairs that\n\nhave that product.\n\nhave that product.\n\nhave that product.\n\n4. 'allowed_sums = [s for s, candidate_pairs in sum_dict.items() if all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]'\nflters out the sums 's' for which all pairs '(x, y)' that add up to 's' have more than one pair with the same product.\n\n4. 'allowed_sums = [s for s, candidate_pairs in sum_dict.items() if all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]'\n\n4. 'allowed_sums = [s for s, candidate_pairs in sum_dict.items() if all(len(prod_dict[x * y]) > 1 for (x, y) in candidate_pairs)]'\n\nflters out the sums 's' for which all pairs '(x, y)' that add up to 's' have more than one pair with the same product.\n\nflters out the sums 's' for which all pairs '(x, y)' that add up to 's' have more than one pair with the same product.\n\n5. 'candidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]' flters out the pairs whose sum is in\n'allowed_sums'.\n\n5. 'candidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]' flters out the pairs whose sum is in\n\n5. 'candidates_after_S = [pair for pair in pairs if pair[0] + pair[1] in allowed_sums]' flters out the pairs whose sum is in\n\n'allowed_sums'.\n\n'allowed_sums'.\n\n6. 'new_prod_dict = g_1(candidates_after_S)' creates a new product dictionary using the fltered pairs.\n\n6. 'new_prod_dict = g_1(candidates_after_S)' creates a new product dictionary using the fltered pairs.\n\n6. 'new_prod_dict = g_1(candidates_after_S)' creates a new product dictionary using the fltered pairs.\n\n7. 'unique_products = prod: pair_list for prod, pair_list in new_prod_dict.items() if len(pair_list) == 1' creates a dictionary of\nproducts that have exactly one pair.\n\n7. 'unique_products = prod: pair_list for prod, pair_list in new_prod_dict.items() if len(pair_list) == 1' creates a dictionary of\n\n7. 'unique_products = prod: pair_list for prod, pair_list in new_prod_dict.items() if len(pair_list) == 1' creates a dictionary of\n\nproducts that have exactly one pair.\n\nproducts that have exactly one pair.\n\n8. 'fnal_candidates = []' initializes a list of fnal candidates.\n\n8. 'fnal_candidates = []' initializes a list of fnal candidates.\n\n8. 'fnal_candidates = []' initializes a list of fnal candidates.\n\n9. The loop 'for s in allowed_sums:' iterates over each sum in 'allowed_sums' and checks if there is exactly one pair '(x, y)' such\nthat 'x + y = s' and 'x * y' is in 'unique_products'. If so, this pair is added to 'fnal_candidates'.\n10. The function returns 'sorted(set(fnal_candidates))', which should be a list containing exactly one pair if the conditions are\nmet.\n\n9. The loop 'for s in allowed_sums:' iterates over each sum in 'allowed_sums' and checks if there is exactly one pair '(x, y)' such\n\n9. The loop 'for s in allowed_sums:' iterates over each sum in 'allowed_sums' and checks if there is exactly one pair '(x, y)' such\n\nthat 'x + y = s' and 'x * y' is in 'unique_products'. If so, this pair is added to 'fnal_candidates'.\n\nthat 'x + y = s' and 'x * y' is in 'unique_products'. If so, this pair is added to 'fnal_candidates'.\n\n10. The function returns 'sorted(set(fnal_candidates))', which should be a list containing exactly one pair if the conditions are\n\n10. The function returns 'sorted(set(fnal_candidates))', which should be a list containing exactly one pair if the conditions are\n\nmet.\n\nmet.\n\nGiven that the output is 'True', it means that there is exactly one pair '(x, y)' that satisfes all the conditions. Let's identify this pair.\n\nGiven that the output is 'True', it means that there is exactly one pair '(x, y)' that satisfes all the conditions. Let's identify this pair.\n\nGiven that the output is 'True', it means that there is exactly one pair '(x, y)' that satisfes all the conditions. Let's identify this pair.\n\nTo do this, we need to understand the mathematical conditions and constraints:\n1. '1 < x < y' and 'x + y <= 100'.\n\nTo do this, we need to understand the mathematical conditions and constraints:\n\nTo do this, we need to understand the mathematical conditions and constraints:\n\n1. '1 < x < y' and 'x + y <= 100'.\n\n1. '1 < x < y' and 'x + y <= 100'.\n\n2. The pair '(x, y)' must be the only pair whose sum is in 'allowed_sums' and whose product is unique among the pairs with the\nsame sum.\n\n2. The pair '(x, y)' must be the only pair whose sum is in 'allowed_sums' and whose product is unique among the pairs with the\n\n2. The pair '(x, y)' must be the only pair whose sum is in 'allowed_sums' and whose product is unique among the pairs with the\n\nsame sum.\n\nsame sum.\n\nBy analyzing the constraints and the nature of the problem, we can deduce that the pair '(x, y)' must be such that the sum 'x + y' is\nunique in the sense that all pairs adding up to this sum have products that are not unique, except for this pair.\n\nBy analyzing the constraints and the nature of the problem, we can deduce that the pair '(x, y)' must be such that the sum 'x + y' is\n\nBy analyzing the constraints and the nature of the problem, we can deduce that the pair '(x, y)' must be such that the sum 'x + y' is\n\nunique in the sense that all pairs adding up to this sum have products that are not unique, except for this pair.\n\nunique in the sense that all pairs adding up to this sum have products that are not unique, except for this pair.\n\nAfter detailed analysis or by running the code with diferent values, we fnd that the pair '(x, y) = (4, 13)' satisfes all the conditions.\nLet's verify:\n\nAfter detailed analysis or by running the code with diferent values, we fnd that the pair '(x, y) = (4, 13)' satisfes all the conditions.\n\nAfter detailed analysis or by running the code with diferent values, we fnd that the pair '(x, y) = (4, 13)' satisfes all the conditions.\n\nLet's verify:\n\nLet's verify:\n\n1. '1 < 4 < 13' and '4 + 13 = 17 <= 100'.\n\n1. '1 < 4 < 13' and '4 + 13 = 17 <= 100'.\n\n1. '1 < 4 < 13' and '4 + 13 = 17 <= 100'.\n\n2. We need to check the 'allowed_sums' and the uniqueness of the product '4 * 13 = 52'.\n\n2. We need to check the 'allowed_sums' and the uniqueness of the product '4 * 13 = 52'.\n\n2. We need to check the 'allowed_sums' and the uniqueness of the product '4 * 13 = 52'.\n\nBy running the code or logical deduction, we can confrm that '(4, 13)' is the only pair that satisfes all the conditions.\n\nBy running the code or logical deduction, we can confrm that '(4, 13)' is the only pair that satisfes all the conditions.\n\nBy running the code or logical deduction, we can confrm that '(4, 13)' is the only pair that satisfes all the conditions.\n\nTherefore, the input that produced the output 'True' is:\n\"'input\n(4, 13)\n\nTherefore, the input that produced the output 'True' is:\n\nTherefore, the input that produced the output 'True' is:\n\n\"'input\n\n\"'input\n\n(4, 13)\n\n(4, 13)\n\n\"'\n\n\"'\n\n\"'\n\nFigure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\nabduction task (predict input). 48\nFigure 41. Vibe Check 2: Sum Product Game. We cast the Sum-Product Game as an abduction task, where when the constraints of the\ngame are satisfied, function 'f' returns True, return False otherwise. AZR-Coder-14b was able to first analyze the complicated constraints,\nidentify candidate solution and verify. Generation parameters: temperature=0.6, top_p=0.95.\n\nFigure 41. Vibe Check 2: Sum Product Game. We cast the Sum-Product Game as an abduction task, where when the constraints of the\n\nWe cast the\n\nSum-Product Game\n\nas an abduction task, where when the constraints of the\n\nVibe Check 2: Sum Product Game.\n\nFigure 41.\n\nFigure 41. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\n\nFigure 41.\n\nA more detail\n\ned version of the exam\n\nple shown in main Figure\n\n7. The model is solving an\n\nSolve Abduction Task Example.\n\ngame are satisfied, function 'f' returns True, return False otherwise. AZR-Coder-14b was able to first analyze the complicated constraints,\n\ngame are satisfied, function 'f' returns True, return False otherwise.\n\nwas able to first analyze the complicated constraints,\n\nAZR-Coder-14b\n\nabduction task (predict input). 48\n\nabduction task (predict input).\n\n48\n\nidentify candidate solution and verify. Generation parameters: temperature=0.6, top_p=0.95.\n\nidentify candidate solution and verify. Generation parameters: temperature=0.6, top_p=0.95.\n\n48\n\n48\n\n48",
        "tables": []
      },
      {
        "page_num": 48,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nD. Alternative Approaches Considered\n\nD. Alternative Approaches Considered\n\nD. Alternative Approaches Considered\n\nIn this section, we share many of the approaches we tried that did not prove to be particularly helpful for Absolute Zero Reasoner.\nHowever, we believe it is especially valuable to share these findings with the community, as they are crucial for guiding future research.\nBelow, we outline each of the additional methods we explored during the development of our project.\n\nIn this section, we share many of the approaches we tried that did not prove to be particularly helpful for Absolute Zero Reasoner.\n\nIn this section, we share many of the approaches we tried that did not prove to be particularly helpful for Absolute Zero Reasoner.\n\nHowever, we believe it is especially valuable to share these findings with the community, as they are crucial for guiding future research.\n\nHowever, we believe it is especially valuable to share these findings with the community, as they are crucial for guiding future research.\n\nBelow, we outline each of the additional methods we explored during the development of our project.\n\nBelow, we outline each of the additional methods we explored during the development of our project.\n\nD.1. Error Deduction Task\n\nD.1. Error Deduction Task\n\nD.1. Error Deduction Task\n\nSince programming languages often have error messages, and these messages contain a lot of information about how someone might\nexpect a program to run, we also came up with another task domain: allowing the learner to propose a program that will produce an\nerror, and requiring the solver to deduce what kind of error is raised when executing this code. We experimented with this additional\ntask alongside the induction (f), deduction (o), and abduction (i) tasks. Unfortunately, we did not observe noticeable changes in\ndownstream performance with this additional task and since it requires more computational resources than our AZR setup, we decided\nnot to incorporate it into our final version. However, we believe further thorough investigation of this is well deserved.\n\nSince programming languages often have error messages, and these messages contain a lot of information about how someone might\n\nSince programming languages often have error messages, and these messages contain a lot of information about how someone might\n\nexpect a program to run, we also came up with another task domain: allowing the learner to propose a program that will produce an\n\nexpect a program to run, we also came up with another task domain: allowing the learner to propose a program\n\nthat will produce an\n\nerror, and requiring the solver to deduce what kind of error is raised when executing this code. We experimented with this additional\n\n, and requiring the solver to\n\nwhen executing this code. We experimented with this additional\n\nerror\n\ndeduce what kind of error is raised\n\ntask alongside the induction (f), deduction (o), and abduction (i) tasks. Unfortunately, we did not observe noticeable changes in\n\ntask alongside the induction (\n\n), deduction (\n\n), and abduction (\n\n) tasks. Unfortunately, we did not observe noticeable changes in\n\nf\n\no\n\ni\n\ndownstream performance with this additional task and since it requires more computational resources than our AZR setup, we decided\n\ndownstream performance with this additional task and since it requires more computational resources than our AZR setup, we decided\n\nnot to incorporate it into our final version. However, we believe further thorough investigation of this is well deserved.\n\nnot to incorporate it into our final version. However, we believe further thorough investigation of this is well deserved.\n\nD.2. Composite Functions as Curriculum Learning\n\nD.2. Composite Functions as Curriculum Learning\n\nD.2. Composite Functions as Curriculum Learning\n\nOne valuable property we can leverage from programming languages is the ability to compose functions\u2014that is, to define a function as\na composite of other functions, i.e., f(g(x)). In our setting, when generating a program, we can not only require the output to be a valid\nprogram but also constrain the LLM to utilize a predefined set of programs within its main function. For example, if the target program\nto be generated is f(\u00b7), we can sample a set of previously generated programs {g_0, . . . , gc} from D, and force a valid program to be\nf(g_0, \u00b7 \u00b7 \u00b7 , gc, i).\n\nOne valuable property we can leverage from programming languages is the ability to compose functions\u2014that is, to define a function as\n\nOne valuable property we can leverage from programming languages is the ability to compose functions\u2014that is, to define a function as\n\na composite of other functions, i.e., f(g(x)). In our setting, when generating a program, we can not only require the output to be a valid\n\na composite of other functions, i.e.,\n\n. In our setting, when generating a program, we can not only require the output to be a valid\n\n(\n\n(\n\n))\n\nf\n\ng\n\nx\n\nprogram but also constrain the LLM to utilize a predefined set of programs within its main function. For example, if the target program\n\nprogram but also constrain the LLM to utilize a predefined set of programs within its main function. For example, if the target program\n\nto be generated is f(\u00b7), we can sample a set of previously generated programs {g_0, . . . , gc} from D, and force a valid program to be\n\nto be generated is\n\n, we can sample a set of previously generated programs\n\nfrom\n\n, and force a valid program to be\n\n_\n\n(\n\n)\n\n0\n\nf\n\ng\n\n, . . . , g\n\n\u00b7\n\n{\n\n}\n\nD\n\nc\n\nf(g_0, \u00b7 \u00b7 \u00b7 , gc, i).\n\n_\n\n.\n\n(\n\n0\n\n)\n\nf\n\ng\n\n,\n\n, g\n\n, i\n\n\u00b7 \u00b7 \u00b7\n\nc\n\nSince all programs are generated by the LLM itself, this setup allows the model to bootstrap from its earlier generations, automatically\nincreasing the complexity of the generated programs. We interpret this mechanism as a form of curriculum learning: earlier programs\nin the AZR self-play loop tend to be simpler, and as the loop progresses, they become increasingly complex. By composing newer\nprograms from progressively more difficult earlier ones, the resulting programs naturally inherit this growing difficulty, which in turn\nchallenges the solver step.\n\nSince all programs are generated by the LLM itself, this setup allows the model to bootstrap from its earlier generations, automatically\n\nSince all programs are generated by the LLM itself, this setup allows the model to bootstrap from its earlier generations, automatically\n\nincreasing the complexity of the generated programs. We interpret this mechanism as a form of curriculum learning: earlier programs\n\nincreasing the complexity of the generated programs. We interpret this mechanism as a form of curriculum learning: earlier programs\n\nin the AZR self-play loop tend to be simpler, and as the loop progresses, they become increasingly complex. By composing newer\n\nin the AZR self-play loop tend to be simpler, and as the loop progresses, they become increasingly complex. By composing newer\n\nprograms from progressively more difficult earlier ones, the resulting programs naturally inherit this growing difficulty, which in turn\n\nprograms from progressively more difficult earlier ones, the resulting programs naturally inherit this growing difficulty, which in turn\n\nchallenges the solver step.\n\nchallenges the solver step.\n\nFor implementation, in generating tasks for abduction and deduction, we begin by sampling a binary decision from a binomial distribution\nwith p = 0.5. This determines whether the generated program should be a simple program or a composite one. If the sample is 0, we\nprompt the LLM to generate a standard program along with a corresponding input. If the sample is 1, we prompt the LLM to generate a\ncomposite program. To construct the composite, we first sample an integer c \u223c U(1, 3), then uniformly select c programs from the\ndataset D that are not themselves composite programs. Finally, we prompt the LLM to generate a valid program that incorporates\n{g_0, . . . , gc} as subcomponents, ensuring it composes these selected programs meaningfully. We additionally filter programs that did\nnot utilize all the c programs.\n\nFor implementation, in generating tasks for abduction and deduction, we begin by sampling a binary decision from a binomial distribution\n\nFor implementation, in generating tasks for abduction and deduction, we begin by sampling a binary decision from a binomial distribution\n\nwith p = 0.5. This determines whether the generated program should be a simple program or a composite one. If the sample is 0, we\n\nwith\n\n. This determines whether the generated program should be a simple program or a composite one. If the sample is 0, we\n\n= 0\n\n5\n\np\n\n.\n\nprompt the LLM to generate a standard program along with a corresponding input. If the sample is 1, we prompt the LLM to generate a\n\nprompt the LLM to generate a standard program along with a corresponding input. If the sample is 1, we prompt the LLM to generate a\n\ncomposite program. To construct the composite, we first sample an integer c \u223c U(1, 3), then uniformly select c programs from the\n\ncomposite program. To construct the composite, we first sample an integer\n\n, then uniformly select\n\nprograms from the\n\n(1\n\n3)\n\nc\n\n,\n\nc\n\n\u223c U\n\ndataset D that are not themselves composite programs. Finally, we prompt the LLM to generate a valid program that incorporates\n\ndataset\n\nthat are not themselves composite programs. Finally, we prompt the LLM to generate a valid program that incorporates\n\nD\n\n{g_0, . . . , gc} as subcomponents, ensuring it composes these selected programs meaningfully. We additionally filter programs that did\n\n_\n\nas subcomponents, ensuring it composes these selected programs meaningfully. We additionally filter programs that did\n\n0\n\ng\n\n, . . . , g\n\n{\n\n}\n\nc\n\nnot utilize all the c programs.\n\nnot utilize all the\n\nprograms.\n\nc\n\nHowever, we did not observe a significant difference when using this more complex curriculum compared to our simpler and more\neffective approach. One failure mode we encountered was that the model often defaulted to simply returning \"g(x)\", effectively learning\nf(g(x)) = g(x), which failed to introduce any additional difficulty. This trivial behavior undermined the intended challenge, leading us\nto deprioritize further exploration in this direction. While it may be possible to design a stricter reward mechanism\u2014such as enforcing\nf(g(x)) \u0338= g(x) by executing the code via a Python interpreter and penalizing such shortcuts\u2014we leave this to future work.\n\nHowever, we did not observe a significant difference when using this more complex curriculum compared to our simpler and more\n\nHowever, we did not observe a significant difference when using this more complex curriculum compared to our simpler and more\n\neffective approach. One failure mode we encountered was that the model often defaulted to simply returning \"g(x)\", effectively learning\n\neffective approach. One failure mode we encountered was that the model often defaulted to simply returning \"g(x)\", effectively learning\n\nf(g(x)) = g(x), which failed to introduce any additional difficulty. This trivial behavior undermined the intended challenge, leading us\n\n, which failed to introduce any additional difficulty. This trivial behavior undermined the intended challenge, leading us\n\n(\n\n(\n\n)) =\n\n(\n\n)\n\nf\n\ng\n\nx\n\ng\n\nx\n\nto deprioritize further exploration in this direction. While it may be possible to design a stricter reward mechanism\u2014such as enforcing\n\nto deprioritize further exploration in this direction. While it may be possible to design a stricter reward mechanism\u2014such as enforcing\n\nf(g(x)) \u0338= g(x) by executing the code via a Python interpreter and penalizing such shortcuts\u2014we leave this to future work.\n\nby executing the code via a Python interpreter and penalizing such shortcuts\u2014we leave this to future work.\n\n(\n\n(\n\n))\n\n=\n\n(\n\n)\n\nf\n\ng\n\nx\n\ng\n\nx\n\n\u0338\n\nD.3. Toying with the Initial p(z)\n\nD.3. Toying with the Initial p(z)\n\n(\n\n)\n\nD.3. Toying with the Initial\n\np\n\nz\n\nWe investigated a setting where the initial seed buffer (see Section 3.3.1 on how we generated these), i.e. p(z) in Equation (3), is not\nself-generated by the base model, but instead sourced from the LeetCode Dataset. We only modified this component and ran AZR\nusing the same procedure as before, continuing to add new valid programs to the initialized buffer. We observed an increase in initial\nperformance on coding benchmarks; however, the performance plateaued at roughly the same level after additional training steps,\ncompared to our official AZR setup. Interestingly, math performance was lower than in the official AZR setup, pointing towards that\non-policy data may be more beneficial to the learner to bootstrap from for mathematical reasoning. We believe that exploring different\nstrategies for initializing and updating p(z) is an important and exciting direction for future research. We briefly explored different\nstrategies for sampling reference code, ultimately settling on uniform sampling for its simplicity, though we also experimented with\nrecency-based sampling and observed potential collapse.\n\nWe investigated a setting where the initial seed buffer (see Section 3.3.1 on how we generated these), i.e. p(z) in Equation (3), is not\n\nWe investigated a setting where the initial seed buffer (see Section\n\n3.3.1\n\non how we generated these),\n\n.\n\nin Equation\n\n(3)\n\n, is not\n\n(\n\n)\n\ni.e\n\np\n\nz\n\nself-generated by the base model, but instead sourced from the LeetCode Dataset. We only modified this component and ran AZR\n\nself-generated by the base model, but instead sourced from the\n\nLeetCode Dataset.\n\nWe only modified this component and ran AZR\n\nusing the same procedure as before, continuing to add new valid programs to the initialized buffer. We observed an increase in initial\n\nusing the same procedure as before, continuing to add new valid programs to the initialized buffer. We observed an increase in initial\n\nperformance on coding benchmarks; however, the performance plateaued at roughly the same level after additional training steps,\n\nperformance on coding benchmarks; however, the performance plateaued at roughly the same level after additional training steps,\n\ncompared to our official AZR setup. Interestingly, math performance was lower than in the official AZR setup, pointing towards that\n\ncompared to our official AZR setup. Interestingly, math performance was lower than in the official AZR setup, pointing towards that\n\non-policy data may be more beneficial to the learner to bootstrap from for mathematical reasoning. We believe that exploring different\n\non-policy data may be more beneficial to the learner to bootstrap from for mathematical reasoning. We believe that exploring different\n\nstrategies for initializing and updating p(z) is an important and exciting direction for future research. We briefly explored different\n\nstrategies for initializing and updating\n\nis an important and exciting direction for future research. We briefly explored different\n\n(\n\n)\n\np\n\nz\n\nstrategies for sampling reference code, ultimately settling on uniform sampling for its simplicity, though we also experimented with\n\nstrategies for sampling reference code, ultimately settling on uniform sampling for its simplicity, though we also experimented with\n\nrecency-based sampling and observed potential collapse.\n\nrecency-based sampling and observed potential collapse.\n\nD.4. Extra Rewards\n\nD.4. Extra Rewards\n\nD.4. Extra Rewards\n\nComplexity Rewards. Code complexity is well studied in software science and could potentially be a good proxy for measuring\nhow hard it is to infer the properties of a piece of code for our reasoning learner. Therefore, for the problem proposer, we can add various\nmeasures of complexity\u2014such as Cyclomatic Complexity (Ebert et al., 2016), maintainability, etc.\u2014to the reward function to incentivize\nthe proposer to produce more complex programs. For illustration purposes, we tried using the Maintainability measure and the Halstead\n\nComplexity Rewards. Code complexity is well studied in software science and could potentially be a good proxy for measuring\n\nCode complexity is well studied in software science and could potentially be a good proxy for measuring\n\nComplexity Rewards.\n\nhow hard it is to infer the properties of a piece of code for our reasoning learner. Therefore, for the problem proposer, we can add various\n\nhow hard it is to infer the properties of a piece of code for our reasoning learner. Therefore, for the problem proposer, we can add various\n\nmeasures of complexity\u2014such as Cyclomatic Complexity (Ebert et al., 2016), maintainability, etc.\u2014to the reward function to incentivize\n\nmeasures of complexity\u2014such as Cyclomatic Complexity\n\n(Ebert et al.,\n\n, maintainability, etc.\u2014to the reward function to incentivize\n\n2016)\n\nthe proposer to produce more complex programs. For illustration purposes, we tried using the Maintainability measure and the Halstead\n\nthe proposer to produce more complex programs. For illustration purposes, we tried using the Maintainability measure and the Halstead\n\n49\n\n49\n\n49",
        "tables": []
      },
      {
        "page_num": 49,
        "text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\ncomplexity measure (Halstead, 1977) as intrinsic rewards. Concretely, we used the complexipy and Radon packages (Lopez, 2025;\nCanal, 2023) to implement the respective metrics. These are then served as intrinsic rewards during the AZR self-play phase.\n\ncomplexity measure (Halstead, 1977) as intrinsic rewards. Concretely, we used the complexipy and Radon packages (Lopez, 2025;\n\ncomplexity measure\n\n(Halstead,\n\n1977)\n\nas intrinsic rewards. Concretely, we used the\n\nand\n\npackages\n\n(Lopez,\n\n2025;\n\ncomplexipy\n\nRadon\n\nCanal, 2023) to implement the respective metrics. These are then served as intrinsic rewards during the AZR self-play phase.\n\nCanal,\n\n2023)\n\nto implement the respective metrics. These are then served as intrinsic rewards during the AZR self-play phase.\n\nDiversity Rewards. We also attempted using diversity rewards to . Inspired by DiveR-CT (Zhao et al., 2025a), we incorporate\ncode edit distance as an intrinsic reward. Specifically, we treat the reference programs shown in the prompt as anchors and compute the\naverage code edit distance between the generated program and these anchors. This serves as a measure of diversity in the generated\noutput. Additionally, we explored another diversity-based reward inspired by the notion of surprise (Zhao et al., 2022). In this approach,\nwe construct a probability distribution over previously encountered input/output pairs that the solver has answered. The reward is then\ndefined as 1 \u2212 p(input/output), where p denotes the empirical probability of a particular input or output. While both strategies were\nevaluated in our experiments, we did not observe a significant difference in performance. However, we believe this aspect warrants\ndeeper investigation, as diversity rewards remain a promising avenue for strengthening AZR further.\n\nDiversity Rewards. We also attempted using diversity rewards to . Inspired by DiveR-CT (Zhao et al., 2025a), we incorporate\n\nWe also attempted using diversity rewards to . Inspired by DiveR-CT\n\n(Zhao et al.,\n\n2025a)\n\n, we incorporate\n\nDiversity Rewards.\n\ncode edit distance as an intrinsic reward. Specifically, we treat the reference programs shown in the prompt as anchors and compute the\n\nas an intrinsic reward. Specifically, we treat the reference programs shown in the prompt as anchors and compute the\n\ncode edit distance\n\naverage code edit distance between the generated program and these anchors. This serves as a measure of diversity in the generated\n\naverage code edit distance between the generated program and these anchors. This serves as a measure of diversity in the generated\n\noutput. Additionally, we explored another diversity-based reward inspired by the notion of surprise (Zhao et al., 2022). In this approach,\n\noutput. Additionally, we explored another diversity-based reward inspired by the notion of\n\n(Zhao et al.,\n\n. In this approach,\n\n2022)\n\nsurprise\n\nwe construct a probability distribution over previously encountered input/output pairs that the solver has answered. The reward is then\n\nwe construct a probability distribution over previously encountered input/output pairs that the solver has answered. The reward is then\n\ndefined as 1 \u2212 p(input/output), where p denotes the empirical probability of a particular input or output. While both strategies were\n\ndefined as\n\n, where\n\ndenotes the empirical probability of a particular input or output. While both strategies were\n\ninput/output\n\n1\n\n(\n\n)\n\np\n\np\n\n\u2212\n\nevaluated in our experiments, we did not observe a significant difference in performance. However, we believe this aspect warrants\n\nevaluated in our experiments, we did not observe a significant difference in performance. However, we believe this aspect warrants\n\ndeeper investigation, as diversity rewards remain a promising avenue for strengthening AZR further.\n\ndeeper investigation, as diversity rewards remain a promising avenue for strengthening AZR further.\n\nReward Aggregation. We tested several ways on how to combine rewards for the proposer and discriminator. First, we separate\nthe reward into extrinsic reward rextrinsic and a set of intrinsic reward(s) I = {ri}, and tested the following strategies to combine them\ninto a single reward,\n\nReward Aggregation. We tested several ways on how to combine rewards for the proposer and discriminator. First, we separate\n\nWe tested several ways on how to combine rewards for the proposer and discriminator. First, we separate\n\nReward Aggregation.\n\nthe reward into extrinsic reward rextrinsic and a set of intrinsic reward(s) I = {ri}, and tested the following strategies to combine them\n\nthe reward into extrinsic reward\n\nand a set of intrinsic reward(s)\n\n, and tested the following strategies to combine them\n\n=\n\nr\n\nI\n\nr\n\nextrinsic\n\n{\n\n}\n\ni\n\ninto a single reward,\n\ninto a single reward,\n\nr = rextrinsic +\nX\n|I|\ni\nri, (11)\n\n|I|\n\nI\n\n|\n\n|\n\nX\n\nX\n\nri, (11)\n\n(11)\n\nr = rextrinsic +\n\n=\n\n+\n\nr\n\nr\n\nr\n\n,\n\nextrinsic\n\ni\n\ni\n\ni\n\nr = rextrinsic \u00b7\nX\n|I|\ni\nri, (12)\n\n|I|\n\nI\n\n|\n\n|\n\nX\n\nX\n\nri, (12)\n\n(12)\n\nr = rextrinsic \u00b7\n\n=\n\nr\n\nr\n\nr\n\n,\n\nextrinsic\n\n\u00b7\n\ni\n\ni\n\ni\n\nr = rextrinsic \u00b7\nY\n|I|\ni\nri, (13)\n\n|I|\n\nI\n\n|\n\n|\n\nY\n\nY\n\nri, (13)\n\n(13)\n\nr = rextrinsic \u00b7\n\n=\n\nr\n\nr\n\nr\n\n,\n\nextrinsic\n\n\u00b7\n\ni\n\ni\n\ni\n\nr = rextrinsic +\nY\n|I|\ni\nri. (14)\n\n|I|\n\nI\n\n|\n\n|\n\nY\n\nY\n\nri. (14)\n\n(14)\n\nr = rextrinsic +\n\n=\n\n+\n\nr\n\nr\n\nr\n\n.\n\nextrinsic\n\ni\n\ni\n\ni\n\nWe found that the simple additive way of combining rewards, a.k.a Equation (11), produced the most stable runs, possibly due to less\nvariance.\n\nWe found that the simple additive way of combining rewards, a.k.a Equation (11), produced the most stable runs, possibly due to less\n\nWe found that the simple additive way of combining rewards, a.k.a Equation\n\n(11)\n\n, produced the most stable runs, possibly due to less\n\nvariance.\n\nvariance.\n\nD.5. Environment Transition\n\nD.5. Environment Transition\n\nD.5. Environment Transition\n\nWe investigated how the transition function in our coding environment for the proposer. Specifically, after generating a piece of code, we\ncan apply a transformation function on it before giving it making it an valid tuple in our dataset. We investigated two\n\nWe investigated how the transition function in our coding environment for the proposer. Specifically, after generating a piece of code, we\n\nWe investigated how the transition function in our coding environment for the proposer. Specifically, after generating a piece of code, we\n\ncan apply a transformation function on it before giving it making it an valid tuple in our dataset. We investigated two\n\ncan apply a transformation function on it before giving it making it an valid tuple in our dataset. We investigated two\n\nRemoving Comments and Docstrings In early iterations of our experiments, we noticed that comments and docstrings\nwere sometimes used to explicitly outline what the function was doing, or even served as a partial \"note-taking\" interleaved \"ReAct\"\nprocess (Yao et al., 2023) of generating code\u2014that is, the model could interleave think and action at the same time, and to make the\ngenerated code valid, it used comments to encase its thoughts (Appendix C.3), similarly observed in DeepSeek-Prover-V2: (Ren et al.,\n2025). We then thought that to make the task harder for the solver, we should occlude this information from it. However, we observed\na significant performance drop after removing all comments and docstrings. One explanation for this phenomenon is that the only\n\"communication\" channel between the proposer and the solver is restricted to the code itself, rather than some kind of \"message\" along\nwith the code. These messages can potentially provide hints to the solver, thus making some otherwise impossible tasks solvable. As a\nresult, the solver is able to learn from its experience and self-bootstrap out of certain unsolvable tasks.\n\nRemoving Comments and Docstrings In early iterations of our experiments, we noticed that comments and docstrings\n\nIn early iterations of our experiments, we noticed that comments and docstrings\n\nRemoving Comments and Docstrings\n\nwere sometimes used to explicitly outline what the function was doing, or even served as a partial \"note-taking\" interleaved \"ReAct\"\n\nwere sometimes used to explicitly outline what the function was doing, or even served as a partial \"note-taking\" interleaved \"ReAct\"\n\nprocess (Yao et al., 2023) of generating code\u2014that is, the model could interleave think and action at the same time, and to make the\n\nprocess\n\n(Yao et al.,\n\n2023)\n\nof generating code\u2014that is, the model could interleave think and action at the same time, and to make the\n\ngenerated code valid, it used comments to encase its thoughts (Appendix C.3), similarly observed in DeepSeek-Prover-V2: (Ren et al.,\n\ngenerated code valid, it used comments to encase its thoughts (Appendix\n\nC.3)\n\n, similarly observed in DeepSeek-Prover-V2:\n\n(Ren et al.,\n\n2025). We then thought that to make the task harder for the solver, we should occlude this information from it. However, we observed\n\n2025)\n\n. We then thought that to make the task harder for the solver, we should occlude this information from it. However, we observed\n\na significant performance drop after removing all comments and docstrings. One explanation for this phenomenon is that the only\n\na significant performance drop after removing all comments and docstrings. One explanation for this phenomenon is that the only\n\n\"communication\" channel between the proposer and the solver is restricted to the code itself, rather than some kind of \"message\" along\n\n\"communication\" channel between the proposer and the solver is restricted to the code itself, rather than some kind of \"message\" along\n\nwith the code. These messages can potentially provide hints to the solver, thus making some otherwise impossible tasks solvable. As a\n\nwith the code. These messages can potentially provide hints to the solver, thus making some otherwise impossible tasks solvable. As a\n\nresult, the solver is able to learn from its experience and self-bootstrap out of certain unsolvable tasks.\n\nresult, the solver is able to learn from its experience and self-bootstrap out of certain unsolvable tasks.\n\nRemoving Global Variables. We observed that some programs contain globally declared variables that may inadvertently leak\ninformation about the correct answer\u2014this issue is particularly prevalent in the input induction task generation and solving. Initially, we\nwere concerned that such leakage might lead to wasted computation on trivial or compromised examples. To address this, we developed\na systematic procedure to remove globally declared variables from the generated programs.\n\nRemoving Global Variables. We observed that some programs contain globally declared variables that may inadvertently leak\n\nWe observed that some programs contain globally declared variables that may inadvertently leak\n\nRemoving Global Variables.\n\ninformation about the correct answer\u2014this issue is particularly prevalent in the input induction task generation and solving. Initially, we\n\ninformation about the correct answer\u2014this issue is particularly prevalent in the input induction task generation and solving. Initially, we\n\nwere concerned that such leakage might lead to wasted computation on trivial or compromised examples. To address this, we developed\n\nwere concerned that such leakage might lead to wasted computation on trivial or compromised examples. To address this, we developed\n\na systematic procedure to remove globally declared variables from the generated programs.\n\na systematic procedure to remove globally declared variables from the generated programs.\n\nHowever, after applying this cleaning step, we observed a noticeable drop in performance on our self-play reasoning tasks. One possible\nexplanation is that the generation step is unaware of this post-processing modification; since the reward is assigned after the transition\nfunction (which includes variable removal), the model may not learn effectively from this mismatch.\n\nHowever, after applying this cleaning step, we observed a noticeable drop in performance on our self-play reasoning tasks. One possible\n\nHowever, after applying this cleaning step, we observed a noticeable drop in performance on our self-play reasoning tasks. One possible\n\nexplanation is that the generation step is unaware of this post-processing modification; since the reward is assigned after the transition\n\nexplanation is that the generation step is unaware of this post-processing modification; since the reward is assigned after the transition\n\nfunction (which includes variable removal), the model may not learn effectively from this mismatch.\n\nfunction (which includes variable removal), the model may not learn effectively from this mismatch.\n\nMoreover, we believe that even when answers are present, the solver still engages in nontrivial reasoning to reach a solution, potentially\nbenefiting from this exposure. This aligns with the idea of rationalization as proposed in STaR (Zelikman et al., 2022), where the model\npretends to not see the answer but still performs reasoning during learning. Therefore, in our final experiments, we choose not to remove\nglobally declared variables, allowing the self-play loop to naturally incorporate and adapt to such cases.\n\nMoreover, we believe that even when answers are present, the solver still engages in nontrivial reasoning to reach a solution, potentially\n\nMoreover, we believe that even when answers are present, the solver still engages in nontrivial reasoning to reach a solution, potentially\n\nbenefiting from this exposure. This aligns with the idea of rationalization as proposed in STaR (Zelikman et al., 2022), where the model\n\nbenefiting from this exposure. This aligns with the idea of rationalization as proposed in STaR\n\n(Zelikman et al.,\n\n, where the model\n\n2022)\n\npretends to not see the answer but still performs reasoning during learning. Therefore, in our final experiments, we choose not to remove\n\npretends to not see the answer but still performs reasoning during learning. Therefore, in our final experiments, we choose not to remove\n\nglobally declared variables, allowing the self-play loop to naturally incorporate and adapt to such cases.\n\nglobally declared variables, allowing the self-play loop to naturally incorporate and adapt to such cases.\n\n50\n\n50\n\n50",
        "tables": []
      }
    ],
    "total_pages": 50
  }
}