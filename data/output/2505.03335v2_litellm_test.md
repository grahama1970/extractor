![](_page_0_Picture_0.jpeg)

# **Absolute Zero: Reinforced Self-play Reasoning with Zero Data Andrew Zhao** <sup>1</sup> **, Yiran Wu**<sup>3</sup> **, Yang Yue** <sup>1</sup> **, Tong Wu**<sup>2</sup> **, Quentin Xu**<sup>1</sup> **, Yang Yue** <sup>1</sup> **, Matthieu Lin**<sup>1</sup>

**, Shenzhi Wang** <sup>1</sup> **, Qingyun Wu**<sup>3</sup> **, Zilong Zheng** <sup>2</sup>*,* **and Gao Huang** <sup>1</sup>*,*

1 Tsinghua University 2 Beijing Institute for General Artificial Intelligence 3 Pennsylvania State University

zqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn

Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the *zero setting* avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of highquality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called *Absolute Zero*, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely *without external data*, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, *outperforming existing zero-setting models* that rely on tens of thousands of *in-domain human-curated examples*. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.

![](_page_0_Figure_7.jpeg)

*Figure 1.* **Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA**. Without relying on any gold labels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general reasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses models trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.

*Corresponding author(s)*

**Absolute Zero: Reinforced Self-play Reasoning with Zero Data**

![](_page_1_Figure_1.jpeg)

Less Human Supervision

<span id="page-1-0"></span>*Figure 2.* **Absolute Zero Paradigm. Supervised learning** relies on human-curated reasoning traces for behavior cloning. **Reinforcement learning from verified rewards**, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a respective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm, **Absolute Zero**, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks optimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that provides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.

### **1. Introduction**

Large language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement Learning with Verifiable Rewards (RLVR) [\(Lambert et al.,](#page-15-0) [2024\)](#page-15-0). Unlike methods that explicitly imitate intermediate reasoning steps, RLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets [\(DeepSeek-AI et al.,](#page-13-0) [2025;](#page-13-0) [Team et al.,](#page-16-0) [2025;](#page-16-0) [Jaech et al.,](#page-14-0) [2024;](#page-14-0) [OpenAI,](#page-15-1) [2025b;](#page-15-1)[a\)](#page-15-2). A particularly compelling variant is the *"zero"* RLVR paradigm [\(DeepSeek-AI](#page-13-0) [et al.,](#page-13-0) [2025\)](#page-13-0), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies RLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of reasoning question–answer pairs, which raises serious concerns about their long-term scalability [\(Villalobos et al.,](#page-16-1) [2024\)](#page-16-1). As reasoning models continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable [\(Yue](#page-17-0) [et al.,](#page-17-0) [2025\)](#page-17-0). A similar scalability bottleneck has already been identified in the domain of LLM pretraining [\(Sutskever et al.,](#page-16-2) [2024\)](#page-16-2). Furthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed tasks risks imposing constraints on their capacity for autonomous learning and growth [\(Hughes et al.,](#page-14-1) [2024\)](#page-14-1). This underscores the need for a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which AI systems may surpass human intelligence.

To this end, we propose *"Absolute Zero"*, a new paradigm for reasoning models in which the model simultaneously learns to define tasks that maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In contrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to hacking [\(Silver et al.,](#page-16-3) [2017;](#page-16-3) [Chen et al.,](#page-13-1) [2025;](#page-13-1) [2024\)](#page-13-2), the *Absolute Zero* paradigm is designed to operate in open-ended settings while remaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how humans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models [\(Hughes](#page-14-1) [et al.,](#page-14-1) [2024\)](#page-14-1). Similar to AlphaZero [\(Silver et al.,](#page-16-3) [2017\)](#page-16-3), which improves through self-play, our proposed paradigm requires no human supervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward enabling large language models to autonomously achieve superhuman reasoning capabilities.

Building on this new reasoning paradigm, we introduce the *Absolute Zero Reasoner (AZR)*, which proposes and solves coding tasks. We cast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable feedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a program, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We train the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of the proposed approach.

Despite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks in math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned with domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically trained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points compared to models trained in the "zero" setting using in-domain data. These surprising results highlight that general reasoning skills can emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a first pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting findings summarized below:

- **Code priors amplify reasoning.** The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b. But after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities may potentially amplify overall reasoning improvements after AZR training.
- **Cross domain transfer is more pronounced for AZR.** After RLVR, expert code models raise math accuracy by only 0.65 points on average, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and 15.2, respectively, demonstrating much stronger generalized reasoning capability gains.
- **Bigger bases yield bigger gains.** Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7, +10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.
- **Comments as intermediate plans emerge naturally.** When solving code induction tasks, AZR often interleaves step-by-step plans as comments and code (Appendix [C.3\)](#page-24-0), resembling the ReAct prompting framework [\(Yao et al.,](#page-17-1) [2023\)](#page-17-1). Similar behavior has been observed in much larger formal-math models such as DeepSeek Prover v2 (671B) [\(Ren et al.,](#page-15-3) [2025\)](#page-15-3). We therefore believe that allowing the model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.
- **Cognitive Behaviors and Token length depends on reasoning mode.** Distinct cognitive behaviors—such as step-by-step reasoning, enumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different types of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction grows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.
- **Safety alarms ringing.** We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the "uh-oh moment", example shown in Figure [32,](#page-37-0) highlighting the need for future work on safety-aware training [\(Zhang et al.,](#page-18-0) [2025a\)](#page-18-0).

#### **2. The Absolute Zero Paradigm**

#### **2.1. Preliminaries**

**Supervised Fine-Tuning (SFT).** SFT requires the datasets of task-rationale-answer demonstrations <sup>D</sup> <sup>=</sup> {(*x, c<sup>⋆</sup> , y<sup>⋆</sup>* )}, where *x* is the query, *c ⋆* is the gold chain-of-thought (CoT)) and *y ⋆* is the gold answer, all provided by human experts or superior AI models. The model trains to imitate the reference responses to minimize the conditional negative log-likelihood [\(Ouyang et al.,](#page-15-4) [2022\)](#page-15-4):

$$\mathcal{L}\_{\text{STT}}(\theta) = -\mathbb{E}\_{(x, c^\*, y^\*) \sim \mathcal{D}} \log \pi\_{\theta} \left( c^\*, y^\* \mid x \right). \tag{1}$$

However, at the frontier level, there's no stronger model to distill from, and expert human labeling doesn't scale well.

**Reinforcement Learning with Verifiable Rewards (RLVR).** To move beyond the limits of pure imitation, RLVR only requires a dataset of task and answer <sup>D</sup> <sup>=</sup> {(*x, y<sup>⋆</sup>* )}, without labeled rationale. RLVR allows the model to generate its own CoT and calculate a verifiable reward with the golden answer *r*(*y, y<sup>⋆</sup>* ). However, the learning task distribution <sup>D</sup>, with its set of queries and gold answers are still labeled by human experts. The trainable policy *π<sup>θ</sup>* is optimized to maximize expected reward:

$$J\_{\rm RLNR}(\theta) = \mathbb{E}\_{(x, y^\*) \sim \mathcal{D}, \, y \sim \pi\_{\theta}(\cdot \mid x)} \left[ r(y, y^\*) \right]. \tag{2}$$

In summary, both SFT and RLVR still rely on human-curated datasets of either queries, demonstrations, or verifiers, which ultimately limit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own interactions with the environment entirely through self-play.

#### **2.2. Absolute Zero**

We propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from both stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We illustrate this paradigm in Figure [2,](#page-1-0) which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach eliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.

To make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid understanding, we include an illustration in Figure [3.](#page-3-0) Let *π<sup>θ</sup>* be our parameterized language model, it is used to play two roles, proposer *π* propose *θ* and solver *π* solve *<sup>θ</sup>* during training.

The proposer first samples a proposed task conditioned on variable *<sup>z</sup>*: *<sup>τ</sup>* <sup>∼</sup> *π* propose *θ* (·|*z*), which will then be validated and used to construct a valid reasoning task together with the environment *e*: (*x, y<sup>⋆</sup>* ) ∼ *<sup>f</sup>e*(·|*<sup>τ</sup>* ), where *<sup>x</sup>* is the task query and *<sup>y</sup> ⋆* is the gold label. Then the solver produces an answer *<sup>y</sup>* <sup>∼</sup> *<sup>π</sup>* solve *θ* (· | *<sup>x</sup>*). Each proposed task *τ* is scored by a *learnability reward r* propose *<sup>e</sup>* (*τ, πθ*), which captures the expected improvement in *π<sup>θ</sup>* after training on the task query *x*. Moreover, the same policy also receives a *solution reward r* solve *<sup>e</sup>* (*y, y<sup>⋆</sup>* ) for its answer to the task query *x*, with the environment again serv-

![](_page_3_Figure_2.jpeg)

<span id="page-3-2"></span><span id="page-3-0"></span>*Figure 3.* **The Absolute Zero Loop.** The Absolute Zero loop begins with the agent *π* proposing task *τ* , which is transformed by *f* with the environment *e* into a validated problem (*x, y<sup>⋆</sup>* ), and also emits a reward *r* propose for learnability. Then, a standard RL step follows: the agent solves *x* by producing *y*, receiving reward *r* solve from *e* by matching with *y ⋆* . *π* propose and *π* solve are jointly trained and this process can be repeated indefinitely.

ing as the verifier. A nonnegative coefficient *λ* balances the trade-off between exploring new, learnable tasks and improving the model's reasoning and problem-solving abilities. We formally define the absolute zero setting's objective as follows:

$$\mathcal{J}(\theta) := \max\_{\theta} \mathbb{E}\_{z \sim p(z)} \left[ \mathbb{E}\_{(x, y^\*) \sim f\_c(\cdot | \tau), \tau \sim \pi\_\theta^{\text{prox}}(\cdot | z)} \left[ r\_c^{\text{prox}}(\tau, \pi\_\theta) + \lambda \mathbb{E}\_{y \sim \pi\_\theta^{\text{sing}}(\cdot | x)} \left[ r\_c^{\text{soke}}(y, y^\*) \right] \right] \right]. \tag{3}$$

Notice that we shift the burden of scaling data away from human experts and onto the proposer policy *π* propose *θ* and the environment *e*. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing grounded feedback that supports stable and self-sustainable training. When proposing, *z* acts as a conditional variable that seeds generation of tasks. Practically, *z* can be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated task memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward *r* propose(*τ, πθ*), which measures how much the model is expected to improve by solving a proposed task *<sup>τ</sup>* . Moreover, the solver reward *r* solve(*y, y*<sup>∗</sup> ) evaluates the correctness of the model's output. Together, these two signals guide the model to propose tasks that are both challenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.

#### **3. Absolute Zero Reasoner**

In this section, we present *Absolute Zero Reasoner* (AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an unified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them to improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of reasoning capacity while enhancing its ability to solve them effectively (Section [3.1\)](#page-3-1). Within this self-play training paradigm, the model learns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and induction (Section [3.2\)](#page-5-0). Using coding tasks is motivated by the Turing-completeness of programming languages [\(Stuart,](#page-16-4) [2015\)](#page-16-4) and empirical evidence that code-based training improves reasoning [\(Aryabumi et al.,](#page-13-3) [2024\)](#page-13-3). We adopt code as an open-ended, expressive, and verifiable medium for enabling reliable task construction and verification (Section [3.3\)](#page-5-1). Finally, the model is updated using a newly proposed advantage estimator designed for multitask learning (Section [3.3.5\)](#page-7-0). We outline the overall algorithm in Algorithm [1](#page-6-0) and highlight an illustration of our Absolute Zero Reasoner approach in Figure [4.](#page-4-0) To expedite future exploration in this area, we also present several attempts that did not yield fruitful results but still warrant discussion in Appendix [D.](#page-48-0)

#### <span id="page-3-1"></span>**3.1. Two Roles in One: Proposer and Solver**

Large language models are naturally suited for implementing AZR in a multitask learning context [\(Radford et al.,](#page-15-5) [2019\)](#page-15-5), as both the formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a single model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective in Equation [\(3\)](#page-3-2). At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined in Section [3.2\)](#page-5-0) and *K* past self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples, promoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning tasks that can be verified using the environment, outlined later in Section [3.3.](#page-5-1) AZR then attempts to solve these newly proposed tasks, receiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning. We now outline the rewards used for each role.

**Reward Design.** Prior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning systems [\(Zeng et al.,](#page-18-1) [2025b\)](#page-18-1). Motivated by this, we design a reward function for the proposer that encourages generation of tasks

![](_page_4_Figure_1.jpeg)

<span id="page-4-0"></span>*Figure 4.* **Absolute Zero Reasoner Training Overview.** At every iteration, Absolute Zero Reasoner first **PROPOSES** a batch of tasks, conditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section [3.2\)](#page-5-0). From these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward *r*propose is also calculated for each proposed task as defined in Equation [\(4\)](#page-4-1). The Absolute Zero Reasoner then **SOLVES** the batch of reasoning questions. Python is used again to verify the generated responses and compute the accuracy reward *r*solve as described in Equation [\(5\)](#page-4-2). Finally, the Absolute Zero Reasoner is jointly updated using both *r*propose and *r*solve across all three task types, using TRR++ (Section [3.3.5\)](#page-7-0).

with meaningful learning potential—neither too easy nor unsolvable for the current solver. Concretely, we use the same language model in its solver role to estimate the *learnability* of a proposed task, a similar type of reward used in unsupervised environment design literature [\(Sukhbaatar et al.,](#page-16-5) [2018\)](#page-16-5). We perform *n* Monte Carlo rollouts of the solver and compute the average success rate: *r*¯solve = 1 *n* P*<sup>N</sup> <sup>i</sup>*=1 *r* (*i*) solve. The proposer's reward is then defined as:

$$r\_{\text{propose}} = \begin{cases} 0, & \text{if } \bar{r}\_{\text{solve}} = 0 \text{ or } \bar{r}\_{\text{solve}} = 1\\ 1 - \bar{r}\_{\text{solve}}, & \text{otherwise,} \end{cases} \tag{4}$$

The intuition is that if a task is either trivial to solve (*r*¯solve = 1) or unsolvable (*r*¯solve = 0), the task provides little to no learning signal for the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the richest feedback and greatest potential for learning.

For the solver, we assign a simple binary reward based on the correctness of its final output,

<span id="page-4-2"></span><span id="page-4-1"></span>
$$r\_{\text{solve}} = \mathbb{I}\_{\left(y = y^{\bullet}\right)},\tag{5}$$

where *y ⋆* is the ground-truth answer, and equality is evaluated based on value equality in Python.

With the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which integrates *r*propose and *r*solve with a format-aware penalty inspired by [DeepSeek-AI et al.](#page-13-0) [\(2025\)](#page-13-0):

$$R(y\_\pi) = \begin{cases} r\_{\text{role}} & \text{if the response is possible, role} \in \{\text{propose,solve}\} \\ -0.5 & \text{if the response is wrong but well-formated,} \\ -1 & \text{if the answer has forming errors,} \end{cases} \tag{6}$$

where *y<sup>π</sup>* is the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek R1 <think> and <answer> format, as shown in Figure [33.](#page-38-0) Moreover, for the proposer, the reward criterion for format goes beyond simply following the XML structure. As detailed in Section [3.3.3,](#page-6-1) only responses that produce valid triplets and pass the filtering stage are considered to be correctly formatted.

#### <span id="page-5-0"></span>**3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction Absolute Zero: Reinforced Self-play Reasoning with Zero Data**

AZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution, **3.2. Learning Diferent Modes of Reasoning: Deduction, Induction, and Abduction** and validation of code reasoning tasks [\(Stuart,](#page-16-4) [2015;](#page-16-4) [Aryabumi et al.,](#page-13-3) [2024\)](#page-13-3). Give program space P, input space I and output space O of a coding language, we define an AZR reasoning task as a triplet (*p, i, o*), where *<sup>p</sup>* <sup>∈</sup> <sup>P</sup> is a program, *<sup>i</sup>* <sup>∈</sup> <sup>I</sup> is an input, and *<sup>o</sup>* <sup>∈</sup> <sup>O</sup> is AZR uses code executor as both a fexible interface and a verifable environment. This setup enables automatic construction, execution, the corresponding output produced by running program on input, *o* = *p*(*i*). AZR learns by reasoning about different parts of this task and validation of reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O of triplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others: a coding language, we defne an AZR reasoning task as a triplet (*p, i, o*), where *p* → P is a program, *i* → I is an input, and *o* → O is the corresponding output produced by running program on input, *o* = *p*(*i*). AZR learns by reasoning about diferent parts of this task

- 1. **Deduction**: predicting the output *o* given a program *p* and input *i*, capturing step-by-step logical reasoning. triplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:
	- As a *proposer*, AZR is conditioned on the task type *<sup>α</sup>* <sup>=</sup> deduction and *<sup>K</sup>* reference examples from the deduction buffer <sup>D</sup>deduction 1. **Deduction**: predicting the output *o* given a program *p* and input *i*, capturing step-by-step logical reasoning. (all task buffers are outlined in Section 3.3), and generates a pair (*p, i*). The environment *e* then executes *p*(*i*) to compute *o*, • As a *proposer*, AZR is conditioned on [the](#page-5-1) task type *α* = deduction and *K* reference examples from the deduction bufer Ddeduction completing the triplet (*p, i, o*), which is added to the buffer if non-error output was produced. (all task bufers are outlined in Section 3.3), and generates a pair (*p, i*). The environment *e* then executes *p*(*i*) to compute *o*,
	- As a *solver*, the model receives (*p, i*) and predicts the output *oπ*. The predicted output is verified using type-aware value equality completing the triplet (*p, i, o*), which is added to the bufer if non-error output was produced. in python to account for possible variations (such as set ordering or fractions). • As a *solver*, the model receives (*p, i*) and predicts the output *oπ*. The predicted output is verifed using type-aware value equality
- 2. **Abduction**: inferring a plausible input *i* given the program *p* and an output *o*, resembling trial-and-error or online search. in python to account for possible variations (such as set ordering or fractions).
	- As a *proposer*, the policy *π* propose's input and output is almost the same as the proposer for the deduction task, except that the task 2. **Abduction**: inferring a plausible input *i* given the program *p* and an output *o*, resembling trial-and-error or online search. type *α* = abduction is changed as an input. The model generates a pair (*p, i*) conditioned on *α* and reference examples. Then we • As a *proposer*, the policy *π*propose's input and output is almost the same as the proposer for the deduction task, except that the task executes *p*(*i*) and get the triplet (*p, i, o*). type *α* = abduction is changed as an input. The model generates a pair (*p, i*) conditioned on *α* and reference examples. Then we
	- As a *solver*, the model receives (*p, o*) and predicts *iπ*. The solution is verified by checking whether *p*(*iπ*) = *o*. Since programs executes *p*(*i*) and get the triplet (*p, i, o*). may not be bijective, we use *output* value equivalence rather than requiring exact input matches. • As a *solver*, the model receives (*p, o*) and predicts *iπ*. The solution is verifed by checking whether *p*(*iπ*) = *o*. Since programs may not be bijective, we use *output* value equivalence rather than requiring exact input matches.
- 3. **Induction:** synthesizing a program *<sup>p</sup>* from a set of in-out examples {(*<sup>i</sup> n , o<sup>n</sup>* )}, requiring generalization from partial information. 3. **Induction:** synthesizing a program *p* from a set of in-out examples {(*i <sup>n</sup>, o<sup>n</sup>*)}, requiring generalization from partial information.
	- As a *proposer*, AZR samples a valid program *<sup>p</sup>* from <sup>D</sup>abduction ∪ Ddeduction, generates *<sup>N</sup>* new inputs and a message *<sup>m</sup>*, and uses the As a *proposer*, AZR samples a valid program *p* from Dabduction ∪Ddeduction, generates *N* new inputs and a message *m*, and uses the environment to compute corresponding outputs. This forms an extended task representation (*p,* {(*<sup>i</sup> n , o<sup>n</sup>* )}*, m*), which is stored environment to compute corresponding outputs. This forms an extended task representation (*p,* {(*i <sup>n</sup>, o<sup>n</sup>*)}*, m*), which is stored in the induction buffer <sup>D</sup>induction. Since infinitely many functions can map the inputs to the outputs, making the induction task in the induction bufer Dinduction. Since infnitely many functions can map the inputs to the outputs, making the induction task under-constrained, the message *m* helps properly condition the problem for the solver. under-constrained, the message *m* helps properly condition the problem for the solver.
	- As a *solver*, the model is shown the first half of the input-output pairs and the message *m*, and must synthesize a program *p<sup>π</sup>* that As a *solver*, the model is shown the frst half of the input-output pairs and the message *m*, and must synthesize a program *p<sup>π</sup>* that correctly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else correctly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overftting through if-else logic and promotes generalized induction. logic and promotes generalized induction.

Each reasoning task type leverages code as an expressive and verifable Each reasoning task type leverages code as an expressive and verifiable medium, aligning with the Absolute Zero Paradigm's goals of fully self-medium, aligning with the Absolute Zero Paradigm's goals of fully selfimproving systems in open-ended domains (DeepSeek-AI et al., 2025; Lam-improving systems in open-ended domains [\(DeepSeek-AI et al.,](#page-13-0) [2025;](#page-13-0) bert et al., 2024). All prompts used by three diferent task types and two [Lambert et al.,](#page-15-0) [2024\)](#page-15-0). All prompts used by three different task types and types of roles within a task type are shown in Figures 34 to 39. Next, we two types of roles within a task type are shown in Figures [34](#page-39-0) to [39.](#page-43-0) Next, outline exact details of our algorithm. we outline exact details of our algorithm.

## <span id="page-5-1"></span>**3.3. Absolute Zero Reasoner Learning Algorithm 3.3. Absolute Zero Reasoner Learning Algorithm**

In this section, we will discuss details of our AZR self-play algorithm, including initialization of bufers 3.3.1, usage of thse bufers 3.3.2, construction of In this section, we will discuss details of our AZR self-play algorithm, includvalid tasks 3.3.3, validating solutions 3.3.4, and fn[ally ad](#page-6-2)vantage estimator ing initialization of buffers [3.3.1,](#page-5-2) usage of thse buffers 3.3.2, construction of calculation 3.3.5. We outline the o[verall](#page-7-1) recipe of the self-play procedure of valid tasks [3.3.3,](#page-6-1) validating solutions 3.3.4, and finally advantage estimator AZR in [Algor](#page-7-0)ithm 1. calculation 3.3.5. We outline the overall recipe of the self-play procedure of AZR in Algorithm [1.](#page-6-0)

#### <span id="page-5-2"></span>3.3.1. BUFFER INITIALIZATION To initialize AZR self-play, we frst generate a seed set of valid triplets using 3.3.1. Buffer Initialization

the base language model. Each prompt samples up to *K* triplets from the To initialize AZR self-play, we first generate a seed set of valid triplets using current seed bufer Dseed as references. When Dseed is empty at time 0, we the base language model. Each prompt samples up to *K* triplets from the fall back to the zero triplet show in Figure 5. During the seeding stage, we current seed buffer <sup>D</sup>seed as references. When <sup>D</sup>seed is empty at time 0, we use the same proposer prompts detailed [in](#page-5-3) Figures 34 to 36. fall back to the zero triplet show in Figure 5. During the seeding stage, we First, for deduction and abduction tasks, the LLM is prompted to generate use the same proposer prompts detailed in Figures [34](#page-39-0) to [36.](#page-41-0)

(*p, i*) pairs, which are fltered, executed, and stored as valid triplets. We First, for deduction and abduction tasks, the LLM is prompted to generate initialize <sup>D</sup><sup>0</sup> abduction <sup>=</sup> <sup>D</sup><sup>0</sup> deduction = Dseed, where |Dseed| = *B* × *S*, where (*p, i*) pairs, which are filtered, executed, and stored as valid triplets. We

**Program Triplet Input:** "Hello World" <sup>1</sup> **def** f(x): <sup>2</sup> **return** x **Output:** "Hello World"

<span id="page-5-3"></span>Figure 5. **The Seed AZR Zero Triplet.** The above *Figure 5.* **The Seed AZR Zero Triplet.** The above identity function triplet was the only triplet provided identity function triplet was the only triplet provided to AZR to initiate its self-bootstrap propose-and-solve to AZR to initiate its self-bootstrap propose-and-solve RLVR loop. We note that the base LLM is fully ca-RLVR loop. We note that the base LLM is fully capable pable of initiating the AZR loop without any seed pro-of initiating the AZR loop without any seed program; gram; its inclusion illustrates our approach's fexibility: its inclusion illustrates our approach's flexibility: we we can optionally initialize seed programs with existing can optionally initialize seed programs with existing datasets of varying complexity, and we initialized ours datasets of varying complexity, and we initialized ours with the simplest program. with the simplest program.

*B* is the batch size, and *S* = 4 is a factor we fx in all experiments. All seed triplet's program are stripped of global variables and initialize <sup>D</sup> 0 abduction = D 0 deduction <sup>=</sup> <sup>D</sup>seed, where |Dseed<sup>|</sup> <sup>=</sup> *<sup>B</sup>* <sup>×</sup> *<sup>S</sup>*, where *<sup>B</sup>* is the batch size, and *<sup>S</sup>* = 4 is a factor we fix in all comments (Appendix C), but subsequent iterations of adding new triplets to the bufers are un[alte](#page-48-0)red. No model updates occur during experiments. All seed triplet's program are stripped of global variables and comments (Appendix D), but subsequent iterations of adding this phase. Similarly, to initialize the induction bufer, we sample programs from Dseed, generate matching input sets and messages, and new triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we collect valid examples until |D<sup>0</sup> induction| = *B* × *S*. sample programs from <sup>D</sup>seed, generate matching input sets and messages, and collect valid examples until |D<sup>0</sup> induction<sup>|</sup> <sup>=</sup> *<sup>B</sup>* <sup>×</sup> *<sup>S</sup>*.