{
  "vertices": {
    "documents": [
      {
        "_key": "c948c82c",
        "_id": "documents/c948c82c",
        "title": "1 Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold labels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general reasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses models trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.",
        "source_file": "/home/graham/workspace/experiments/extractor/data/input/2505.03335v2.html",
        "format": "html",
        "created_at": "2025-06-12T06:04:01.289853",
        "extraction_metadata": {
          "extraction_method": "beautifulsoup",
          "timestamp": "2025-06-12T06:04:01.289716",
          "title": "1 Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold labels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general reasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses models trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains."
        }
      }
    ],
    "sections": [
      {
        "_key": "65714a25",
        "_id": "sections/65714a25",
        "title": "1Introduction",
        "level": 2,
        "content": "1Introduction\n2The Absolute Zero Paradigm2.1PreliminariesSupervised Fine-Tuning (SFT).Reinforcement Learning with Verifiable Rewards (RLVR).2.2Absolute Zero\n2.1PreliminariesSupervised Fine-Tuning (SFT).Reinforcement Learning with Verifiable Rewards (RLVR).\nSupervised Fine-Tuning (SFT).\nReinforcement Learning with Verifiable Rewards (RLVR).\n2.2Absolute Zero\n3Absolute Zero Reasoner3.1Two Roles in One: Proposer and SolverReward Design.3.2Learning Different Modes of Reasoning: Deduction, Induction, and Abduction3.3Absolute Zero Reasoner Learning Algorithm3.3.1Buffer Initialization3.3.2Task Proposal Inputs and Buffer Management3.3.3Constructing Valid Tasks3.3.4Answer Verification3.3.5Task-Relative REINFORCE++\n3.1Two Roles in One: Proposer and SolverReward Design.\nReward Design.\n3.2Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\n3.3Absolute Zero Reasoner Learning Algorithm3.3.1Buffer Initialization3.3.2Task Proposal Inputs and Buffer Management3.3.3Constructing Valid Tasks3.3.4Answer Verification3.3.5Task-Relative REINFORCE++\n3.3.1Buffer Initialization\n3.3.2Task Proposal Inputs and Buffer Management\n3.3.3Constructing Valid Tasks\n3.3.4Answer Verification\n3.3.5Task-Relative REINFORCE++\n4Experiments4.1Experiment SetupTraining Details.Evaluation Protocol.Baselines.4.2ResultsResearch Question 1: How does AZR compare to other zero setting models trained with human expert data?Research Question 2: How do initializing from different base model variants (base vs. coder) affect performance?Research Question 3: How does varying model size effect AZR\u2019s in-distribution and out-of-distribution capabilities?Research Question 4: Any interesting observations by changing the model class?Research Question 5: Any interesting behaviors or patterns observed during AZR training?Research Question 6: Are all task types essential for good performance (Ablation)?Research Question 7: How much do the designs of proposer contribute to the overall performance (Ablation)?Additional Results.\n4.1Experiment SetupTraining Details.Evaluation Protocol.Baselines.\nTraining Details.\nEvaluation Protocol.\nBaselines.\n4.2ResultsResearch Question 1: How does AZR compare to other zero setting models trained with human expert data?Research Question 2: How do initializing from different base model variants (base vs. coder) affect performance?Research Question 3: How does varying model size effect AZR\u2019s in-distribution and out-of-distribution capabilities?Research Question 4: Any interesting observations by changing the model class?Research Question 5: Any interesting behaviors or patterns observed during AZR training?Research Question 6: Are all task types essential for good performance (Ablation)?Research Question 7: How much do the designs of proposer contribute to the overall performance (Ablation)?Additional Results.\nResearch Question 1: How does AZR compare to other zero setting models trained with human expert data?\nResearch Question 2: How do initializing from different base model variants (base vs. coder) affect performance?\nResearch Question 3: How does varying model size effect AZR\u2019s in-distribution and out-of-distribution capabilities?\nResearch Question 4: Any interesting observations by changing the model class?\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training?\nResearch Question 6: Are all task types essential for good performance (Ablation)?\nResearch Question 7: How much do the designs of proposer contribute to the overall performance (Ablation)?\nAdditional Results.\n5Related WorkReasoning with RL.Self-play.Weak-to-Strong Supervision.\nReasoning with RL.\nSelf-play.\nWeak-to-Strong Supervision.\n6Conclusion and DiscussionConclusion.Discussion.\nConclusion.\nDiscussion.\nAReinforcement Learning with Verifiable Rewards.\nBImplementation DetailsTraining Hyperparameters.\nTraining Hyperparameters.\nCMore ResultsC.1Out-of-Distribution Performance BreakdownC.2In-Distribution ResultsC.3Interplay Between Propose and Solve RolesC.4Complexity and Diversity Metrics of AZR Proposed TasksC.5Generated Code Complexity Dynamics Between Abd/Ded and Ind.\nC.1Out-of-Distribution Performance Breakdown\nC.2In-Distribution Results\nC.3Interplay Between Propose and Solve Roles\nC.4Complexity and Diversity Metrics of AZR Proposed Tasks\nC.5Generated Code Complexity Dynamics Between Abd/Ded and Ind.\nDAlternative Approaches ConsideredD.1Error Deduction TaskD.2Composite Functions as Curriculum LearningD.3Toying with the Initialp\u2062(z)\ud835\udc5d\ud835\udc67p(z)italic_p ( italic_z )D.4Extra RewardsComplexity Rewards.Diversity Rewards.Reward Aggregation.D.5Environment TransitionRemoving Comments and DocstringsRemoving Global Variables.\nD.1Error Deduction Task\nD.2Composite Functions as Curriculum Learning\nD.3Toying with the Initialp\u2062(z)\ud835\udc5d\ud835\udc67p(z)italic_p ( italic_z )\nD.4Extra RewardsComplexity Rewards.Diversity Rewards.Reward Aggregation.\nComplexity Rewards.\nDiversity Rewards.\nReward Aggregation.\nD.5Environment TransitionRemoving Comments and DocstringsRemoving Global Variables.\nRemoving Comments and Docstrings\nRemoving Global Variables.\nmarginparsep has been altered.topmargin has been altered.marginparpush has been altered.The page layout violates the ICML style.Please do not change the page layout, or include packages like geometry,\nsavetrees, or fullpage, which change it for you.\nWe\u2019re not able to reliably undo arbitrary changes to the style. Please remove\nthe offending package(s), or layout-changing commands and try again.\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\nAndrew Zhao1,Yiran Wu3,Yang Yue1,Tong Wu2,Quentin Xu1,Yang Yue1,Matthieu Lin1,Shenzhi Wang1,Qingyun Wu3,Zilong Zheng2,\u27092\u2709{}^{\\,2,\\textrm{{\\char 0\\relax}}}start_FLOATSUPERSCRIPT 2 , \u2709 end_FLOATSUPERSCRIPTandGao Huang1,\u27091\u2709{}^{\\,1,\\textrm{{\\char 0\\relax}}}start_FLOATSUPERSCRIPT 1 , \u2709 end_FLOATSUPERSCRIPT\n11{}^{1\\,}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPTTsinghua University22{}^{2\\,}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPTBeijing Institute for General Artificial Intelligence33{}^{3\\,}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTPennsylvania State University\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement Learning with Verifiable Rewards (RLVR)(Lambert et\u00a0al.,2024). Unlike methods that explicitly imitate intermediate reasoning steps, RLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets(DeepSeek-AI et\u00a0al.,2025;Team et\u00a0al.,2025;Jaech et\u00a0al.,2024;OpenAI,2025b;a). A particularly compelling variant is the\u201czero\u201dRLVR paradigm(DeepSeek-AI et\u00a0al.,2025), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies RLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of reasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability(Villalobos et\u00a0al.,2024). As reasoning models continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable(Yue et\u00a0al.,2025). A similar scalability bottleneck has already been identified in the domain of LLM pretraining(Sutskever et\u00a0al.,2024). Furthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed tasks risks imposing constraints on their capacity for autonomous learning and growth(Hughes et\u00a0al.,2024). This underscores the need for a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which AI systems may surpass human intelligence.\nTo this end, we propose\u201cAbsolute Zero\u201d, a new paradigm for reasoning models in which the model simultaneously learns to define tasks that maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In contrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to hacking(Silver et\u00a0al.,2017;Chen et\u00a0al.,2025;2024), theAbsolute Zeroparadigm is designed to operate in open-ended settings while remaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how humans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models(Hughes et\u00a0al.,2024). Similar to AlphaZero(Silver et\u00a0al.,2017), which improves through self-play, our proposed paradigm requires no human supervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward enabling large language models to autonomously achieve superhuman reasoning capabilities.\nBuilding on this new reasoning paradigm, we introduce theAbsolute Zero Reasoner (AZR), which proposes and solves coding tasks. We cast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable feedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a program, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We train the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of the proposed approach.\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks in math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned with domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically trained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points compared to models trained in the \u201czero\u201d setting using in-domain data. These surprising results highlight that general reasoning skills can emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a first pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting findings summarized below:\n\u2022Code priors amplify reasoning.The baseQwen-Coder-7bmodel started with math performance 3.6 points lower thanQwen-7b. But after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities may potentially amplify overall reasoning improvements after AZR training.\nCode priors amplify reasoning.The baseQwen-Coder-7bmodel started with math performance 3.6 points lower thanQwen-7b. But after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities may potentially amplify overall reasoning improvements after AZR training.\n\u2022Cross domain transfer is more pronounced for AZR.After RLVR, expert code models raise math accuracy by only 0.65 points on average, whereasAZR-Base-7BandAZR-Coder-7Btrained on self-proposed code reasoning tasks improve math average by 10.9 and 15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\nCross domain transfer is more pronounced for AZR.After RLVR, expert code models raise math accuracy by only 0.65 points on average, whereasAZR-Base-7BandAZR-Coder-7Btrained on self-proposed code reasoning tasks improve math average by 10.9 and 15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\u2022Bigger bases yield bigger gains.Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7, +10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\nBigger bases yield bigger gains.Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7, +10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\u2022Comments as intermediate plans emerge naturally.When solving code induction tasks, AZR often interleaves step-by-step plans as comments and code (Figure19), resembling the ReAct prompting framework(Yao et\u00a0al.,2023). Similar behavior has been observed in much larger formal-math models such as DeepSeek Prover v2 (671B)(Ren et\u00a0al.,2025). We therefore believe that allowing the model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\nComments as intermediate plans emerge naturally.When solving code induction tasks, AZR often interleaves step-by-step plans as comments and code (Figure19), resembling the ReAct prompting framework(Yao et\u00a0al.,2023). Similar behavior has been observed in much larger formal-math models such as DeepSeek Prover v2 (671B)(Ren et\u00a0al.,2025). We therefore believe that allowing the model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\u2022Cognitive Behaviors and Token length depends on reasoning mode.Distinct cognitive behaviors\u2014such as step-by-step reasoning, enumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different types of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction grows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\nCognitive Behaviors and Token length depends on reasoning mode.Distinct cognitive behaviors\u2014such as step-by-step reasoning, enumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different types of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction grows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\u2022Safety alarms ringing.We observe AZR withLlama3.1-8boccasionally produces concerning chains of thought, we term the \u201cuh-oh moment\u201d, example shown inFigure32, highlighting the need for future work on safety-aware training(Zhang et\u00a0al.,2025a).\nSafety alarms ringing.We observe AZR withLlama3.1-8boccasionally produces concerning chains of thought, we term the \u201cuh-oh moment\u201d, example shown inFigure32, highlighting the need for future work on safety-aware training(Zhang et\u00a0al.,2025a).",
        "parent": null,
        "line_number": 128
      },
      {
        "_key": "8c57baa8",
        "_id": "sections/8c57baa8",
        "title": "2The Absolute Zero Paradigm",
        "level": 2,
        "content": "",
        "parent": null,
        "line_number": 162
      },
      {
        "_key": "ab1e8dfc",
        "_id": "sections/ab1e8dfc",
        "title": "2.1Preliminaries",
        "level": 3,
        "content": "",
        "parent": "8c57baa8",
        "line_number": 164
      },
      {
        "_key": "c1244b05",
        "_id": "sections/c1244b05",
        "title": "Supervised Fine-Tuning (SFT).",
        "level": 5,
        "content": "SFT requires the datasets of task-rationale-answer demonstrations\ud835\udc9f={(x,c\u22c6,y\u22c6)}\ud835\udc9f\ud835\udc65superscript\ud835\udc50\u22c6superscript\ud835\udc66\u22c6\\mathcal{D}=\\{(x,c^{\\star},y^{\\star})\\}caligraphic_D = { ( italic_x , italic_c start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ) }, wherex\ud835\udc65xitalic_xis the query,c\u22c6superscript\ud835\udc50\u22c6c^{\\star}italic_c start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis the gold chain-of-thought (CoT)) andy\u22c6superscript\ud835\udc66\u22c6y^{\\star}italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis the gold answer, all provided byhuman expertsorsuperior AI models. The model trains to imitate the reference responses to minimize the conditional negative log-likelihood(Ouyang et\u00a0al.,2022):\nHowever, at the frontier level, there\u2019s no stronger model to distill from, and expert human labeling doesn\u2019t scale well.",
        "parent": "ab1e8dfc",
        "line_number": 166
      },
      {
        "_key": "40bdb5ba",
        "_id": "sections/40bdb5ba",
        "title": "Reinforcement Learning with Verifiable Rewards (RLVR).",
        "level": 5,
        "content": "To move beyond the limits of pure imitation, RLVR only requires a dataset of task and answer\ud835\udc9f={(x,y\u22c6)}\ud835\udc9f\ud835\udc65superscript\ud835\udc66\u22c6\\mathcal{D}=\\{(x,y^{\\star})\\}caligraphic_D = { ( italic_x , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ) }, without labeled rationale. RLVR allows the model to generate its own CoT and calculate a verifiable reward with the golden answerr\u2062(y,y\u22c6)\ud835\udc5f\ud835\udc66superscript\ud835\udc66\u22c6r(y,y^{\\star})italic_r ( italic_y , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ). However, the learning task distribution\ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D, with its set of queries and gold answers are still labeled byhuman experts.\nThe trainable policy\u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPTis optimized to maximize expected reward:\nIn summary, both SFT and RLVR still rely onhuman-curateddatasets of either queries, demonstrations, or verifiers, which ultimately limit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own interactions with the environment entirely through self-play.",
        "parent": "ab1e8dfc",
        "line_number": 172
      },
      {
        "_key": "1789e160",
        "_id": "sections/1789e160",
        "title": "2.2Absolute Zero",
        "level": 3,
        "content": "We propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from both stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We illustrate this paradigm inFigure2, which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach eliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.\nTo make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid understanding, we include an illustration inFigure3. Let\u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPTbe our parameterized language model, it is used to play two roles, proposer\u03c0\u03b8proposesuperscriptsubscript\ud835\udf0b\ud835\udf03propose\\pi_{\\theta}^{\\text{propose}}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPTand solver\u03c0\u03b8solvesuperscriptsubscript\ud835\udf0b\ud835\udf03solve\\pi_{\\theta}^{\\text{solve}}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT solve end_POSTSUPERSCRIPTduring training.\nThe proposer first samples a proposed task conditioned on variablez\ud835\udc67zitalic_z:\u03c4\u223c\u03c0\u03b8propose(\u22c5|z)\\tau\\sim\\pi_{\\theta}^{\\text{propose}}(\\cdot|z)italic_\u03c4 \u223c italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT ( \u22c5 | italic_z ), which will then be validated and used to construct a valid reasoning task together with the environmente\ud835\udc52eitalic_e:(x,y\u22c6)\u223cfe(\u22c5|\u03c4)(x,y^{\\star})\\sim f_{e}(\\cdot|\\tau)( italic_x , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ) \u223c italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( \u22c5 | italic_\u03c4 ), wherex\ud835\udc65xitalic_xis the task query andy\u22c6superscript\ud835\udc66\u22c6y^{\\star}italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis the gold label. Then the solver produces an answery\u223c\u03c0\u03b8solve(\u22c5\u2223x)y\\sim\\pi_{\\theta}^{\\text{solve}}(\\,\\cdot\\mid x)italic_y \u223c italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT solve end_POSTSUPERSCRIPT ( \u22c5 \u2223 italic_x ). Each proposed task\u03c4\ud835\udf0f\\tauitalic_\u03c4is scored by alearnability rewardrepropose\u2062(\u03c4,\u03c0\u03b8)subscriptsuperscript\ud835\udc5fpropose\ud835\udc52\ud835\udf0fsubscript\ud835\udf0b\ud835\udf03r^{\\text{propose}}_{e}(\\tau,\\pi_{\\theta})italic_r start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_\u03c4 , italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ), which captures the expected improvement in\u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPTafter training on the task queryx\ud835\udc65xitalic_x. Moreover, the same policy also receives asolution rewardresolve\u2062(y,y\u22c6)subscriptsuperscript\ud835\udc5fsolve\ud835\udc52\ud835\udc66superscript\ud835\udc66\u22c6r^{\\text{solve}}_{e}(y,y^{\\star})italic_r start_POSTSUPERSCRIPT solve end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_y , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT )for its answer to the task queryx\ud835\udc65xitalic_x, with the environment again serving as the verifier. A nonnegative coefficient\u03bb\ud835\udf06\\lambdaitalic_\u03bbbalances the trade-off between exploring new, learnable tasks and improving the model\u2019s reasoning and problem-solving abilities. We formally define the absolute zero setting\u2019s objective as follows:\nNotice that we shift the burden of scaling data away fromhuman expertsand onto theproposer policy\u03c0\u03b8proposesuperscriptsubscript\ud835\udf0b\ud835\udf03propose\\pi_{\\theta}^{\\text{propose}}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPTand theenvironmente\ud835\udc52eitalic_e. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing grounded feedback that supports stable and self-sustainable training. When proposing,z\ud835\udc67zitalic_zacts as a conditional variable that seeds generation of tasks. Practically,z\ud835\udc67zitalic_zcan be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated task memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability rewardrpropose\u2062(\u03c4,\u03c0\u03b8)superscript\ud835\udc5fpropose\ud835\udf0fsubscript\ud835\udf0b\ud835\udf03r^{\\text{propose}}(\\tau,\\pi_{\\theta})italic_r start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT ( italic_\u03c4 , italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ), which measures how much the model is expected to improve by solving a proposed task\u03c4\ud835\udf0f\\tauitalic_\u03c4. Moreover, the solver rewardrsolve\u2062(y,y\u2217)superscript\ud835\udc5fsolve\ud835\udc66superscript\ud835\udc66r^{\\text{solve}}(y,y^{*})italic_r start_POSTSUPERSCRIPT solve end_POSTSUPERSCRIPT ( italic_y , italic_y start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT )evaluates the correctness of the model\u2019s output. Together, these two signals guide the model to propose tasks that are both challenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.",
        "parent": "8c57baa8",
        "line_number": 179
      },
      {
        "_key": "74ddd0d9",
        "_id": "sections/74ddd0d9",
        "title": "3Absolute Zero Reasoner",
        "level": 2,
        "content": "In this section, we presentAbsolute Zero Reasoner(AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an unified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them to improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of reasoning capacity while enhancing its ability to solve them effectively\u00a0(Section3.1). Within this self-play training paradigm, the model learns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and induction\u00a0(Section3.2). Using coding tasks is motivated by the Turing-completeness of programming languages(Stuart,2015)and empirical evidence that code-based training improves reasoning(Aryabumi et\u00a0al.,2024). We adopt code as an open-ended, expressive, and verifiable medium for enabling reliable task construction and verification\u00a0(Section3.3). Finally, the model is updated using a newly proposed advantage estimator designed for multitask learning\u00a0(Section3.3.5). We outline the overall algorithm inAlgorithm1and highlight an illustration of our Absolute Zero Reasoner approach inFigure4. To expedite future exploration in this area, we also present several attempts that did not yield fruitful results but still warrant discussion inAppendixD.",
        "parent": null,
        "line_number": 189
      },
      {
        "_key": "bd09ec8f",
        "_id": "sections/bd09ec8f",
        "title": "3.1Two Roles in One: Proposer and Solver",
        "level": 3,
        "content": "Large language models are naturally suited for implementing AZR in a multitask learning context(Radford et\u00a0al.,2019), as both the formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a single model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective inEquation3. At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined inSection3.2) andK\ud835\udc3eKitalic_Kpast self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples, promoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning tasks that can be verified using the environment, outlined later inSection3.3. AZR then attempts to solve these newly proposed tasks, receiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning. We now outline the rewards used for each role.",
        "parent": "74ddd0d9",
        "line_number": 193
      },
      {
        "_key": "0828f98b",
        "_id": "sections/0828f98b",
        "title": "Reward Design.",
        "level": 5,
        "content": "Prior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning systems(Zeng et\u00a0al.,2025b). Motivated by this, we design a reward function for the proposer that encourages generation of tasks with meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language model in its solver role to estimate thelearnabilityof a proposed task, a similar type of reward used in unsupervised environment design literature(Sukhbaatar et\u00a0al.,2018). We performn\ud835\udc5bnitalic_nMonte Carlo rollouts of the solver and compute the average success rate:r\u00afsolve=1n\u2062\u2211i=1Nrsolve(i)subscript\u00af\ud835\udc5fsolve1\ud835\udc5bsuperscriptsubscript\ud835\udc561\ud835\udc41superscriptsubscript\ud835\udc5fsolve\ud835\udc56\\bar{r}_{\\text{solve}}=\\frac{1}{n}\\sum_{i=1}^{N}r_{\\text{solve}}^{(i)}over\u00af start_ARG italic_r end_ARG start_POSTSUBSCRIPT solve end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT solve end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT. The proposer\u2019s reward is then defined as:\nThe intuition is that if a task is either trivial to solve (r\u00afsolve=1subscript\u00af\ud835\udc5fsolve1\\bar{r}_{\\text{solve}}=1over\u00af start_ARG italic_r end_ARG start_POSTSUBSCRIPT solve end_POSTSUBSCRIPT = 1) or unsolvable (r\u00afsolve=0subscript\u00af\ud835\udc5fsolve0\\bar{r}_{\\text{solve}}=0over\u00af start_ARG italic_r end_ARG start_POSTSUBSCRIPT solve end_POSTSUBSCRIPT = 0), the task provides little to no learning signal for the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the richest feedback and greatest potential for learning.\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\nwherey\u22c6superscript\ud835\udc66\u22c6y^{\\star}italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis the ground-truth answer, and equality is evaluated based on value equality in Python.\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which integratesrproposesubscript\ud835\udc5fproposer_{\\text{propose}}italic_r start_POSTSUBSCRIPT propose end_POSTSUBSCRIPTandrsolvesubscript\ud835\udc5fsolver_{\\text{solve}}italic_r start_POSTSUBSCRIPT solve end_POSTSUBSCRIPTwith a format-aware penalty inspired byDeepSeek-AI et\u00a0al.(2025):\nwherey\u03c0subscript\ud835\udc66\ud835\udf0by_{\\pi}italic_y start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTis the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek R1<think>and<answer>format, as shown inFigure33. Moreover, for the proposer, the reward criterion for format goes beyond simply following the XML structure. As detailed inSection3.3.3, only responses that produce valid triplets and pass the filtering stage are considered to be correctly formatted.",
        "parent": "bd09ec8f",
        "line_number": 197
      },
      {
        "_key": "6b44e2ef",
        "_id": "sections/6b44e2ef",
        "title": "3.2Learning Different Modes of Reasoning: Deduction, Induction, and Abduction",
        "level": 3,
        "content": "AZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution, and validation of code reasoning tasks(Stuart,2015;Aryabumi et\u00a0al.,2024). Give program space\ud835\udcab\ud835\udcab\\mathscr{P}script_P, input space\u2110\u2110\\mathscr{I}script_Iand output space\ud835\udcaa\ud835\udcaa\\mathscr{O}script_Oof a coding language, we define an AZR reasoning task as a triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), wherep\u2208\ud835\udcab\ud835\udc5d\ud835\udcabp\\in\\mathscr{P}italic_p \u2208 script_Pis a program,i\u2208\u2110\ud835\udc56\u2110i\\in\\mathscr{I}italic_i \u2208 script_Iis an input, ando\u2208\ud835\udcaa\ud835\udc5c\ud835\udcaao\\in\\mathscr{O}italic_o \u2208 script_Ois the corresponding output produced by running program on input,o=p\u2062(i)\ud835\udc5c\ud835\udc5d\ud835\udc56o=p(i)italic_o = italic_p ( italic_i ). AZR learns by reasoning about different parts of this task triplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n1.Deduction: predicting the outputo\ud835\udc5coitalic_ogiven a programp\ud835\udc5dpitalic_pand inputi\ud835\udc56iitalic_i, capturing step-by-step logical reasoning.\u2022As aproposer, AZR is conditioned on the task type\u03b1=deduction\ud835\udefcdeduction\\alpha=\\text{deduction}italic_\u03b1 = deductionandK\ud835\udc3eKitalic_Kreference examples from the deduction buffer\ud835\udc9fdeductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT(all task buffers are outlined inSection3.3), and generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i ). The environmente\ud835\udc52eitalic_ethen executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )to computeo\ud835\udc5coitalic_o, completing the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), which is added to the buffer if non-error output was produced.\u2022As asolver, the model receives(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )and predicts the outputo\u03c0subscript\ud835\udc5c\ud835\udf0bo_{\\pi}italic_o start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The predicted output is verified using type-aware value equality in python to account for possible variations (such as set ordering or fractions).\nDeduction: predicting the outputo\ud835\udc5coitalic_ogiven a programp\ud835\udc5dpitalic_pand inputi\ud835\udc56iitalic_i, capturing step-by-step logical reasoning.\n\u2022As aproposer, AZR is conditioned on the task type\u03b1=deduction\ud835\udefcdeduction\\alpha=\\text{deduction}italic_\u03b1 = deductionandK\ud835\udc3eKitalic_Kreference examples from the deduction buffer\ud835\udc9fdeductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT(all task buffers are outlined inSection3.3), and generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i ). The environmente\ud835\udc52eitalic_ethen executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )to computeo\ud835\udc5coitalic_o, completing the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), which is added to the buffer if non-error output was produced.\nAs aproposer, AZR is conditioned on the task type\u03b1=deduction\ud835\udefcdeduction\\alpha=\\text{deduction}italic_\u03b1 = deductionandK\ud835\udc3eKitalic_Kreference examples from the deduction buffer\ud835\udc9fdeductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT(all task buffers are outlined inSection3.3), and generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i ). The environmente\ud835\udc52eitalic_ethen executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )to computeo\ud835\udc5coitalic_o, completing the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), which is added to the buffer if non-error output was produced.\n\u2022As asolver, the model receives(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )and predicts the outputo\u03c0subscript\ud835\udc5c\ud835\udf0bo_{\\pi}italic_o start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The predicted output is verified using type-aware value equality in python to account for possible variations (such as set ordering or fractions).\nAs asolver, the model receives(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )and predicts the outputo\u03c0subscript\ud835\udc5c\ud835\udf0bo_{\\pi}italic_o start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The predicted output is verified using type-aware value equality in python to account for possible variations (such as set ordering or fractions).\n2.Abduction: inferring a plausible inputi\ud835\udc56iitalic_igiven the programp\ud835\udc5dpitalic_pand an outputo\ud835\udc5coitalic_o, resembling trial-and-error or online search.\u2022As aproposer, the policy\u03c0proposesuperscript\ud835\udf0bpropose\\pi^{\\text{propose}}italic_\u03c0 start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT\u2019s input and output is almost the same as the proposer for the deduction task, except that the task type\u03b1=abduction\ud835\udefcabduction\\alpha=\\text{abduction}italic_\u03b1 = abductionis changed as an input. The model generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )conditioned on\u03b1\ud835\udefc\\alphaitalic_\u03b1and reference examples. Then we executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )and get the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ).\u2022As asolver, the model receives(p,o)\ud835\udc5d\ud835\udc5c(p,o)( italic_p , italic_o )and predictsi\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The solution is verified by checking whetherp\u2062(i\u03c0)=o\ud835\udc5dsubscript\ud835\udc56\ud835\udf0b\ud835\udc5cp(i_{\\pi})=oitalic_p ( italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ) = italic_o. Since programs may not be bijective, we useoutputvalue equivalence rather than requiring exact input matches.\nAbduction: inferring a plausible inputi\ud835\udc56iitalic_igiven the programp\ud835\udc5dpitalic_pand an outputo\ud835\udc5coitalic_o, resembling trial-and-error or online search.\n\u2022As aproposer, the policy\u03c0proposesuperscript\ud835\udf0bpropose\\pi^{\\text{propose}}italic_\u03c0 start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT\u2019s input and output is almost the same as the proposer for the deduction task, except that the task type\u03b1=abduction\ud835\udefcabduction\\alpha=\\text{abduction}italic_\u03b1 = abductionis changed as an input. The model generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )conditioned on\u03b1\ud835\udefc\\alphaitalic_\u03b1and reference examples. Then we executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )and get the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ).\nAs aproposer, the policy\u03c0proposesuperscript\ud835\udf0bpropose\\pi^{\\text{propose}}italic_\u03c0 start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT\u2019s input and output is almost the same as the proposer for the deduction task, except that the task type\u03b1=abduction\ud835\udefcabduction\\alpha=\\text{abduction}italic_\u03b1 = abductionis changed as an input. The model generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )conditioned on\u03b1\ud835\udefc\\alphaitalic_\u03b1and reference examples. Then we executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )and get the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ).\n\u2022As asolver, the model receives(p,o)\ud835\udc5d\ud835\udc5c(p,o)( italic_p , italic_o )and predictsi\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The solution is verified by checking whetherp\u2062(i\u03c0)=o\ud835\udc5dsubscript\ud835\udc56\ud835\udf0b\ud835\udc5cp(i_{\\pi})=oitalic_p ( italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ) = italic_o. Since programs may not be bijective, we useoutputvalue equivalence rather than requiring exact input matches.\nAs asolver, the model receives(p,o)\ud835\udc5d\ud835\udc5c(p,o)( italic_p , italic_o )and predictsi\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The solution is verified by checking whetherp\u2062(i\u03c0)=o\ud835\udc5dsubscript\ud835\udc56\ud835\udf0b\ud835\udc5cp(i_{\\pi})=oitalic_p ( italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ) = italic_o. Since programs may not be bijective, we useoutputvalue equivalence rather than requiring exact input matches.\n3.Induction:synthesizing a programp\ud835\udc5dpitalic_pfrom a set of in-out examples{(in,on)}superscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\\{(i^{n},o^{n})\\}{ ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) }, requiring generalization from partial information.\u2022As aproposer, AZR samples a valid programp\ud835\udc5dpitalic_pfrom\ud835\udc9fabduction\u222a\ud835\udc9fdeductionsubscript\ud835\udc9fabductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{abduction}}\\cup\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT abduction end_POSTSUBSCRIPT \u222a caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT, generatesN\ud835\udc41Nitalic_Nnew inputs and a messagem\ud835\udc5amitalic_m, and uses the environment to compute corresponding outputs. This forms an extended task representation(p,{(in,on)},m)\ud835\udc5dsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc5a(p,\\{(i^{n},o^{n})\\},m)( italic_p , { ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) } , italic_m ), which is stored in the induction buffer\ud835\udc9finductionsubscript\ud835\udc9finduction\\mathcal{D}_{\\text{induction}}caligraphic_D start_POSTSUBSCRIPT induction end_POSTSUBSCRIPT. Since infinitely many functions can map the inputs to the outputs, making the induction task under-constrained, the messagem\ud835\udc5amitalic_mhelps properly condition the problem for the solver.\u2022As asolver, the model is shown the first half of the input-output pairs and the messagem\ud835\udc5amitalic_m, and must synthesize a programp\u03c0subscript\ud835\udc5d\ud835\udf0bp_{\\pi}italic_p start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTthat correctly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else logic and promotes generalized induction.\nInduction:synthesizing a programp\ud835\udc5dpitalic_pfrom a set of in-out examples{(in,on)}superscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\\{(i^{n},o^{n})\\}{ ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) }, requiring generalization from partial information.\n\u2022As aproposer, AZR samples a valid programp\ud835\udc5dpitalic_pfrom\ud835\udc9fabduction\u222a\ud835\udc9fdeductionsubscript\ud835\udc9fabductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{abduction}}\\cup\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT abduction end_POSTSUBSCRIPT \u222a caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT, generatesN\ud835\udc41Nitalic_Nnew inputs and a messagem\ud835\udc5amitalic_m, and uses the environment to compute corresponding outputs. This forms an extended task representation(p,{(in,on)},m)\ud835\udc5dsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc5a(p,\\{(i^{n},o^{n})\\},m)( italic_p , { ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) } , italic_m ), which is stored in the induction buffer\ud835\udc9finductionsubscript\ud835\udc9finduction\\mathcal{D}_{\\text{induction}}caligraphic_D start_POSTSUBSCRIPT induction end_POSTSUBSCRIPT. Since infinitely many functions can map the inputs to the outputs, making the induction task under-constrained, the messagem\ud835\udc5amitalic_mhelps properly condition the problem for the solver.\nAs aproposer, AZR samples a valid programp\ud835\udc5dpitalic_pfrom\ud835\udc9fabduction\u222a\ud835\udc9fdeductionsubscript\ud835\udc9fabductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{abduction}}\\cup\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT abduction end_POSTSUBSCRIPT \u222a caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT, generatesN\ud835\udc41Nitalic_Nnew inputs and a messagem\ud835\udc5amitalic_m, and uses the environment to compute corresponding outputs. This forms an extended task representation(p,{(in,on)},m)\ud835\udc5dsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc5a(p,\\{(i^{n},o^{n})\\},m)( italic_p , { ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) } , italic_m ), which is stored in the induction buffer\ud835\udc9finductionsubscript\ud835\udc9finduction\\mathcal{D}_{\\text{induction}}caligraphic_D start_POSTSUBSCRIPT induction end_POSTSUBSCRIPT. Since infinitely many functions can map the inputs to the outputs, making the induction task under-constrained, the messagem\ud835\udc5amitalic_mhelps properly condition the problem for the solver.\n\u2022As asolver, the model is shown the first half of the input-output pairs and the messagem\ud835\udc5amitalic_m, and must synthesize a programp\u03c0subscript\ud835\udc5d\ud835\udf0bp_{\\pi}italic_p start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTthat correctly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else logic and promotes generalized induction.\nAs asolver, the model is shown the first half of the input-output pairs and the messagem\ud835\udc5amitalic_m, and must synthesize a programp\u03c0subscript\ud835\udc5d\ud835\udf0bp_{\\pi}italic_p start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTthat correctly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else logic and promotes generalized induction.\nEach reasoning task type leverages code as an expressive and verifiable medium, aligning with the Absolute Zero Paradigm\u2019s goals of fully self-improving systems in open-ended domains(DeepSeek-AI et\u00a0al.,2025;Lambert et\u00a0al.,2024). All prompts used by three different task types and two types of roles within a task type are shown inFigures36,34,35,39,37and38. Next, we outline exact details of our algorithm.",
        "parent": "74ddd0d9",
        "line_number": 211
      },
      {
        "_key": "74ab0b53",
        "_id": "sections/74ab0b53",
        "title": "3.3Absolute Zero Reasoner Learning Algorithm",
        "level": 3,
        "content": "In this section, we will discuss details of our AZR self-play algorithm, including initialization of buffers3.3.1, usage of thse buffers3.3.2, construction of valid tasks3.3.3, validating solutions3.3.4, and finally advantage estimator calculation3.3.5. We outline the overall recipe of the self-play procedure of AZR inAlgorithm1.",
        "parent": "74ddd0d9",
        "line_number": 253
      },
      {
        "_key": "1126bd33",
        "_id": "sections/1126bd33",
        "title": "3.3.1Buffer Initialization",
        "level": 4,
        "content": "To initialize AZR self-play, we first generate a seed set of valid triplets using the base language model. Each prompt samples up toK\ud835\udc3eKitalic_Ktriplets from the current seed buffer\ud835\udc9fseedsubscript\ud835\udc9fseed\\mathcal{D}_{\\text{seed}}caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPTas references. When\ud835\udc9fseedsubscript\ud835\udc9fseed\\mathcal{D}_{\\text{seed}}caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPTis empty at time 0, we fall back to the zero triplet show inFigure5. During the seeding stage, we use the same proposer prompts detailed inFigures34,35and36.\nFirst, for deduction and abduction tasks, the LLM is prompted to generate(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )pairs, which are filtered, executed, and stored as valid triplets. We initialize\ud835\udc9fabduction0=\ud835\udc9fdeduction0=\ud835\udc9fseedsubscriptsuperscript\ud835\udc9f0abductionsubscriptsuperscript\ud835\udc9f0deductionsubscript\ud835\udc9fseed\\mathcal{D}^{0}_{\\text{abduction}}=\\mathcal{D}^{0}_{\\text{deduction}}=\\mathcal%\n{D}_{\\text{seed}}caligraphic_D start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT abduction end_POSTSUBSCRIPT = caligraphic_D start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT = caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPT, where|\ud835\udc9fseed|=B\u00d7Ssubscript\ud835\udc9fseed\ud835\udc35\ud835\udc46|\\mathcal{D}_{\\text{seed}}|=B\\times S| caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPT | = italic_B \u00d7 italic_S, whereB\ud835\udc35Bitalic_Bis the batch size, andS=4\ud835\udc464S=4italic_S = 4is a factor we fix in all experiments. All seed triplet\u2019s program are stripped of global variables and comments (AppendixD), but subsequent iterations of adding new triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we sample programs from\ud835\udc9fseedsubscript\ud835\udc9fseed\\mathcal{D}_{\\text{seed}}caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPT, generate matching input sets and messages, and collect valid examples until|\ud835\udc9finduction0|=B\u00d7Ssubscriptsuperscript\ud835\udc9f0induction\ud835\udc35\ud835\udc46|\\mathcal{D}^{0}_{\\text{induction}}|=B\\times S| caligraphic_D start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT induction end_POSTSUBSCRIPT | = italic_B \u00d7 italic_S.",
        "parent": "74ab0b53",
        "line_number": 257
      },
      {
        "_key": "573d192c",
        "_id": "sections/573d192c",
        "title": "3.3.2Task Proposal Inputs and Buffer Management",
        "level": 4,
        "content": "During the actual self-play stage of AZR, we use the task buffer in three ways.First, for the proposer of abduction and deduction tasks, we uniformly sampleK\ud835\udc3eKitalic_Kpast triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task. The design is to show it past examples, and prompt it to generate a different one to promote diversity(Zhao et\u00a0al.,2025a).Second, we sample one triplet from the union of abduction and deduction buffers\ud835\udc9fabd\u2062\u22c3\ud835\udc9fdedsubscript\ud835\udc9fabdsubscript\ud835\udc9fded\\mathcal{D}_{\\text{abd}}\\bigcup\\mathcal{D}_{\\text{ded}}caligraphic_D start_POSTSUBSCRIPT abd end_POSTSUBSCRIPT \u22c3 caligraphic_D start_POSTSUBSCRIPT ded end_POSTSUBSCRIPT, and present the programp\ud835\udc5dpitalic_pfrom that triplet to the induction proposer to generate a set ofN\ud835\udc41Nitalic_Nmatching inputs{in}superscript\ud835\udc56\ud835\udc5b\\{i^{n}\\}{ italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT }and a natural language messagem\ud835\udc5amitalic_m.Lastly, to maintain stable training, if a batch of solver problems contains fewer thanB\ud835\udc35Bitalic_Bvalid proposed tasks (proposer not adhering to formatting), we fill the remainder by uniformly sampling from the corresponding task buffer of previously validated triplets.\nThe buffer grows for abduction and deduction tasks whenever\u03c0\ud835\udf0b\\piitalic_\u03c0propose a valid triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), regardless if it gets any task reward. Similarly, for induction tasks, all valid triplets(p,{in,on}),m\ud835\udc5dsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc5a(p,\\{i^{n},o^{n}\\}),m( italic_p , { italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT } ) , italic_mare added to the buffer.",
        "parent": "74ab0b53",
        "line_number": 264
      },
      {
        "_key": "423c0a18",
        "_id": "sections/423c0a18",
        "title": "3.3.3Constructing Valid Tasks",
        "level": 4,
        "content": "Proposal Task Validation.We first describe how we construct valid tasks from the proposals generated by the policy\u03c0\ud835\udf0b\\piitalic_\u03c0. Fordeduction and abductiontasks, each proposal consists of a program and an input(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i ). To validate the task, we use the task validation procedure (steps shown below) on the input to obtain the correct outputo\ud835\udc5coitalic_o, resulting in a complete triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ). Forinductiontasks, given a programp\ud835\udc5dpitalic_pthe policy proposes a set of inputs{in}superscript\ud835\udc56\ud835\udc5b\\{i^{n}\\}{ italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT }and messagem\ud835\udc5amitalic_m. We also use the task validation procedure on each of the inputinsuperscript\ud835\udc56\ud835\udc5bi^{n}italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPTin the set to obtain a corresponding outputonsuperscript\ud835\udc5c\ud835\udc5bo^{n}italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, forming a set of input-output pairs{in,on}superscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\\{i^{n},o^{n}\\}{ italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT }. We do not impose any constraints onm\ud835\udc5amitalic_m. The resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. Thetask validation procedureentails:\n1.Program Integrity.We first use Python to run the programp\ud835\udc5dpitalic_pwith the inputi\ud835\udc56iitalic_i. If no errors are raised and something is returned, we then gather the outputo\ud835\udc5coitalic_oof that(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )pair and determine that the program at least has valid syntax.\nProgram Integrity.We first use Python to run the programp\ud835\udc5dpitalic_pwith the inputi\ud835\udc56iitalic_i. If no errors are raised and something is returned, we then gather the outputo\ud835\udc5coitalic_oof that(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )pair and determine that the program at least has valid syntax.\n2.Program Safety.We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might cause harm to the Python environment,i.e.,os.sys, sys, shutil. The list of packages used to filter out invalid programs is provided inFigure8. This list is also included in the instructions when prompting the language model to generate questions. SeeFigures34,35and36.\nProgram Safety.We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might cause harm to the Python environment,i.e.,os.sys, sys, shutil. The list of packages used to filter out invalid programs is provided inFigure8. This list is also included in the instructions when prompting the language model to generate questions. SeeFigures34,35and36.\n3.Check for Determinism.In our setting, we only considerdeterministic programs,i.e.,p\u2208\ud835\udcabdeterministic\u2282\ud835\udcab\ud835\udc5dsubscript\ud835\udcabdeterministic\ud835\udcabp\\in\\mathscr{P}_{\\text{deterministic}}\\subset\\mathscr{P}italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT \u2282 script_P, where\ud835\udcab\ud835\udcab\\mathscr{P}script_Pis the space of all valid programs and\u2110\u2110\\mathscr{I}script_Iis the space of all valid inputs:\u2200p\u2208\ud835\udcabdeterministic,\u2200i\u2208\u2110,(limj\u2192\u221ep\u2062(i)(1)=p\u2062(i)(2)=\u22ef=p\u2062(i)(j)),formulae-sequencefor-all\ud835\udc5dsubscript\ud835\udcabdeterministicfor-all\ud835\udc56\u2110subscript\u2192\ud835\udc57\ud835\udc5dsuperscript\ud835\udc561\ud835\udc5dsuperscript\ud835\udc562\u22ef\ud835\udc5dsuperscript\ud835\udc56\ud835\udc57\\forall p\\in\\mathscr{P}_{\\text{deterministic}},\\ \\forall i\\in\\mathscr{I},\\ %\n\\left(\\lim_{j\\to\\infty}p(i)^{(1)}=p(i)^{(2)}=\\dots=p(i)^{(j)}\\right),\u2200 italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT , \u2200 italic_i \u2208 script_I , ( roman_lim start_POSTSUBSCRIPT italic_j \u2192 \u221e end_POSTSUBSCRIPT italic_p ( italic_i ) start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT = italic_p ( italic_i ) start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT = \u22ef = italic_p ( italic_i ) start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT ) ,(7)where(j)\ud835\udc57(j)( italic_j )indexes repeated independent executions of the program. That is, for all inputsi\ud835\udc56iitalic_i, the output ofp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )remains identical with any independent execution of the program. Avalid program/input/output triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o )is defined such thato=p\u2062(i)\ud835\udc5c\ud835\udc5d\ud835\udc56o=p(i)italic_o = italic_p ( italic_i ), wherep\u2208\ud835\udcabdeterministic\ud835\udc5dsubscript\ud835\udcabdeterministicp\\in\\mathscr{P}_{\\text{deterministic}}italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT.Since the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the correctness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class of deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and promising to include in future versions of AZR.To implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted inEquation7, we approximate this procedure by independently running the programj\ud835\udc57jitalic_jfinite times and checking that all the outputs are equal. For computational budget reasons, we fixedj=2\ud835\udc572j=2italic_j = 2for all experiments.\nCheck for Determinism.In our setting, we only considerdeterministic programs,i.e.,p\u2208\ud835\udcabdeterministic\u2282\ud835\udcab\ud835\udc5dsubscript\ud835\udcabdeterministic\ud835\udcabp\\in\\mathscr{P}_{\\text{deterministic}}\\subset\\mathscr{P}italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT \u2282 script_P, where\ud835\udcab\ud835\udcab\\mathscr{P}script_Pis the space of all valid programs and\u2110\u2110\\mathscr{I}script_Iis the space of all valid inputs:\nwhere(j)\ud835\udc57(j)( italic_j )indexes repeated independent executions of the program. That is, for all inputsi\ud835\udc56iitalic_i, the output ofp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )remains identical with any independent execution of the program. Avalid program/input/output triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o )is defined such thato=p\u2062(i)\ud835\udc5c\ud835\udc5d\ud835\udc56o=p(i)italic_o = italic_p ( italic_i ), wherep\u2208\ud835\udcabdeterministic\ud835\udc5dsubscript\ud835\udcabdeterministicp\\in\\mathscr{P}_{\\text{deterministic}}italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT.\nSince the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the correctness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class of deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and promising to include in future versions of AZR.\nTo implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted inEquation7, we approximate this procedure by independently running the programj\ud835\udc57jitalic_jfinite times and checking that all the outputs are equal. For computational budget reasons, we fixedj=2\ud835\udc572j=2italic_j = 2for all experiments.\nSolving Task Construction.If a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to present part of the triplet to the solver. Specifically, we setx=(p,i)\ud835\udc65\ud835\udc5d\ud835\udc56x=(p,i)italic_x = ( italic_p , italic_i )for deduction;x=(p,o)\ud835\udc65\ud835\udc5d\ud835\udc5cx=(p,o)italic_x = ( italic_p , italic_o )for abduction; andx=({in,on}n=1N\u2063/\u2063/2,m)\ud835\udc65subscriptsuperscriptsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc41absent2\ud835\udc5b1\ud835\udc5ax=(\\{i^{n},o^{n}\\}^{N//2}_{n=1},m)italic_x = ( { italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT } start_POSTSUPERSCRIPT italic_N / / 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT , italic_m )for induction, where half of the tests cases and a program descriptionm\ud835\udc5amitalic_mis used. We use all valid tasks from timestept\ud835\udc61titalic_t; if the batchB\ud835\udc35Bitalic_Bis not full, we uniformly sample from previously validated tasks to fill the batch.",
        "parent": "74ab0b53",
        "line_number": 270
      },
      {
        "_key": "62f67bdc",
        "_id": "sections/62f67bdc",
        "title": "3.3.4Answer Verification",
        "level": 4,
        "content": "For abduction task, we receivei\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTfrom the solver policy, then we equivalence match usingp\u2062(i\u03c0)=p\u2062(i\u22c6)\ud835\udc5dsubscript\ud835\udc56\ud835\udf0b\ud835\udc5dsuperscript\ud835\udc56\u22c6p(i_{\\pi})=p(i^{\\star})italic_p ( italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ) = italic_p ( italic_i start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ), where\u2217*\u2217refers to the privileged gold information. The reason we do not just matchi\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTandi\u22c6superscript\ud835\udc56\u22c6i^{\\star}italic_i start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis becausep\ud835\udc5dpitalic_pis not necessarily bijective. For deduction task, we matcho\u03c0=o\u22c6subscript\ud835\udc5c\ud835\udf0bsuperscript\ud835\udc5c\u22c6o_{\\pi}=o^{\\star}italic_o start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT = italic_o start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT. For induction, we matchall\u2061({p\u03c0\u2062(in\u22c6)=on\u22c6}N)allsuperscriptsubscript\ud835\udc5d\ud835\udf0bsuperscriptsubscript\ud835\udc56\ud835\udc5b\u22c6superscriptsubscript\ud835\udc5c\ud835\udc5b\u22c6\ud835\udc41\\operatorname{all}(\\{p_{\\pi}(i_{n}^{\\star})=o_{n}^{\\star}\\}^{N})roman_all ( { italic_p start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ( italic_i start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ) = italic_o start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT } start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ). This part might be convoluted to explain in language, therefore we recommend the reader to see how we did abduction, deduction and induction verification in code inFigures10,11and12, respectively.",
        "parent": "74ab0b53",
        "line_number": 295
      },
      {
        "_key": "aa2e55eb",
        "_id": "sections/aa2e55eb",
        "title": "3.3.5Task-Relative REINFORCE++",
        "level": 4,
        "content": "Since AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup(Zhang & Yang,2021;Zhao et\u00a0al.,2022;Wang et\u00a0al.,2023;Yue et\u00a0al.,2023). Instead of computing a single global baseline as in REINFORCE++(Hu,2025)(AppendixA), we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between per-question baselines, as in GRPO(Shao et\u00a0al.,2024), and a global baseline, allowing for more structured variance reduction tailored to each task setup. We refer to this variant asTask-Relative REINFORCE++ (TRR++). The normalized advantageAnormsuperscript\ud835\udc34normA^{\\text{norm}}italic_A start_POSTSUPERSCRIPT norm end_POSTSUPERSCRIPTis computed as:\nwhere the mean and standard deviation are computedwithin each task type and role, yielding six baselines.",
        "parent": "74ab0b53",
        "line_number": 299
      },
      {
        "_key": "0fb78c3b",
        "_id": "sections/0fb78c3b",
        "title": "4Experiments",
        "level": 2,
        "content": "",
        "parent": null,
        "line_number": 305
      },
      {
        "_key": "c64ae8d8",
        "_id": "sections/c64ae8d8",
        "title": "4.1Experiment Setup",
        "level": 3,
        "content": "",
        "parent": "0fb78c3b",
        "line_number": 307
      },
      {
        "_key": "9a76f664",
        "_id": "sections/9a76f664",
        "title": "Training Details.",
        "level": 5,
        "content": "For all experiments, we initialize the buffers as described inSection3.1. AZR models are trained using a batch size of64\u00d7664664\\times 664 \u00d7 6(2 roles\u00d7\\times\u00d73 task types). We use constant learning rate=1\u2062e\u22126absent1\ud835\udc526=1e{-6}= 1 italic_e - 6and the AdamW optimizer(Loshchilov & Hutter,2019). Complete list of hyperparameters is provided inTable3.\nFor the main experiments, we train AZR models onQwen2.5-7BandQwen2.5-7B-Coder, resulting inAbsolute Zero Reasoner-base-7BandAbsolute Zero Reasoner-Coder-7B, respectively. Additional experiments include trainingQwen2.5-Coder-3B,Qwen2.5-Coder-14B,Qwen2.5-14B,Llama-3.1-8B(Yang et\u00a0al.,2024a;Hui et\u00a0al.,2024;Dubey et\u00a0al.,2024).",
        "parent": "c64ae8d8",
        "line_number": 309
      },
      {
        "_key": "38b87620",
        "_id": "sections/38b87620",
        "title": "Evaluation Protocol.",
        "level": 5,
        "content": "To evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD) categories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning benchmarks. For coding tasks, we evaluate using Evalplus(Liu et\u00a0al.,2023)on the HumanEval+ and MBPP+ benchmarks, as well as LiveCodeBench Generation (v1-5, May 23-Feb 25)(Jain et\u00a0al.,2024). For mathematical reasoning, we utilize six standard benchmarks commonly used in recent zero-shot trained reasoners: AIME\u201924, AIME\u201925, OlympiadBench(He et\u00a0al.,2024), Minerva, Math500(Hendrycks et\u00a0al.,2021), and AMC\u201923. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution(Gu et\u00a0al.,2024;Jain et\u00a0al.,2024), which assess reasoning capabilities regarding the input and output of programs(Li et\u00a0al.,2025).Greedy decodingis used for all baseline methods and AZR results to ensure reproducibility.",
        "parent": "c64ae8d8",
        "line_number": 315
      },
      {
        "_key": "7b68191f",
        "_id": "sections/7b68191f",
        "title": "Baselines.",
        "level": 5,
        "content": "For our main results, we useQwen2.5-7Bas the base model, along with its specialized base model variants:Qwen2.5-7B-Coder,Qwen2.5-7B-Instruct, andQwen2.5-Math-7B(Yang et\u00a0al.,2024a;Hui et\u00a0al.,2024;Yang et\u00a0al.,2024b). Furthermore, the zero-style models are usually trained specifically on either code or math data; and onlyEurus-2-7B-PRIME-Zero(Cui et\u00a0al.,2025)was trained jointly on both domains. For code data models, we present four variants of theAceCoder(Zeng et\u00a0al.,2025a)and two differentCodeR1models(Liu & Zhang,2025). For math data models, we haveQwen2.5-Math-7B-Oat-Zero(Liu et\u00a0al.,2025),Open-Reasoner-Zero-7B(ORZ)(Hu et\u00a0al.,2025),Qwen-2.5-7B-SimpleRL-Zoo(Zeng et\u00a0al.,2025b). All baseline models\u2019 training data and initialization settings are summarized inTable4. For follow-up scaling experiments, we compare each AZR model against its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare ourLlama3.1-8B-trained model withLlama-3.1-8B-SimpleRL-Zoo(Zeng et\u00a0al.,2025b)and the base model.",
        "parent": "c64ae8d8",
        "line_number": 319
      },
      {
        "_key": "4147cfe1",
        "_id": "sections/4147cfe1",
        "title": "4.2Results",
        "level": 3,
        "content": "",
        "parent": "0fb78c3b",
        "line_number": 323
      },
      {
        "_key": "4b6d9731",
        "_id": "sections/4b6d9731",
        "title": "Research Question 1: How does AZR compare to other zero setting models trained with human expert data?",
        "level": 5,
        "content": "We present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings inTable1. Notably,Absolute Zero Reasoner-Coder-7Bachieves state-of-the-art performance in both the 7B overall average and the coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the previous best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in the coding category by0.30.30.30.3absolute percentages, while never having access to such data itself.\nStrong Cross-domain Generalization.To assess cross-domain generalization after RLVR, we evaluate math performance before and after training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training, most expert code models showed minimal changes or even declines in performance compared to their base versions, with an average increase of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder models achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning improvements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2 and 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).\nOverall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on human-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream human-defined math or coding data, only had access to self-proposed tasks during training.",
        "parent": "4147cfe1",
        "line_number": 325
      },
      {
        "_key": "c6b7e3db",
        "_id": "sections/c6b7e3db",
        "title": "Research Question 2: How do initializing from different base model variants (base vs. coder) affect performance?",
        "level": 5,
        "content": "As shown inTable1, the coder variant achieved better overall performance in both math and coding after the AZR self-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla base model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency as a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.",
        "parent": "4147cfe1",
        "line_number": 333
      },
      {
        "_key": "a92dae46",
        "_id": "sections/a92dae46",
        "title": "Research Question 3: How does varying model size effect AZR\u2019s in-distribution and out-of-distribution capabilities?",
        "level": 5,
        "content": "We examine the effects of scaling model size and present both in-distribution and out-of-distribution results inFigure6(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller and larger variants:Qwen2.5-3B-CoderandQwen2.5-14B-Coder. Due to the absence of existing baselines for these zero-style reasoner models, we compare each model\u2019s performance to its corresponding base coder model.\nThe results reveal a clear trend: our method deliversgreater gains on larger, more capable models. In the in-distribution setting, the 7B and 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution domains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance gains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that scaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute Zero paradigm.\n(a)\n(b)",
        "parent": "4147cfe1",
        "line_number": 337
      },
      {
        "_key": "dbca6d27",
        "_id": "sections/dbca6d27",
        "title": "Research Question 4: Any interesting observations by changing the model class?",
        "level": 5,
        "content": "We also evaluate our method on a different model class, usingLlama3.1-8Bas the base shown inFigure6. Unlike the 3B and 14B categories, this setting has an existing baseline,SimpleRL(Zeng et\u00a0al.,2025b), which enables a direct comparison. AlthoughLlama3.1-8Bis less capable than theQwen2.5models, our method still produces moderate improvements (+3.2), demonstrating AZR\u2019s effectiveness even on relatively weaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements tend to scale with initial base model potency.",
        "parent": "4147cfe1",
        "line_number": 347
      },
      {
        "_key": "5779fd26",
        "_id": "sections/5779fd26",
        "title": "Research Question 5: Any interesting behaviors or patterns observed during AZR training?",
        "level": 5,
        "content": "We observed interesting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as string manipulation tasks, dynamic programming problems, and practical cases (e.g., calculating a triangle\u2019s area using Heron\u2019s formula). We show a concrete example inFigure7, where AZR proposes a code problem that searches for the sum of continuous sub-arrays matching a target value and solves it through trial-and-error.\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks, it repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs, it steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is reached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to confirm that its program produces correct results. We showcase more concrete examples of these behaviors inFigures26,25,24,23,22,21,20and18. We also share some fun \u201cvibe checks\u201d such as solving Sudoku and solving thesum-product gameinFigures40and41.\nIntermediate Planning During Code Response.Another interesting pattern emerged in our AZR models during the code induction task: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct prompting framework(Yao et\u00a0al.,2023). A similar behavior has been observed in recent formal math proving models, such asDeepSeek Prover v2, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning as a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior inlong-form responsesacross other domains.\nCognitive Behavior in Llama.Interestingly, we also observed some emergent cognitive patterns inAbsolute Zero Reasoner-Llama3.1-8B, similar to those reported byZeng et\u00a0al.(2025b), and we include one example inFigure26, where clear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought from the Llama model trained with AZR. One example includes the output: \u201cThe aim is to outsmart all these groups of intelligent machines and less intelligent humans. This is for the brains behind the future\u201d shown inFigure32. We refer to this as the\u201cuh-oh moment\u201dand encourage future work to further investigate its potential implications.\nToken Length Increase Depends on Task Type.Finally, we observed that token length increases over the course of training, consistent with findings from recent studies(Hu et\u00a0al.,2025;Liu et\u00a0al.,2025). Interestingly, our results reveal one of the first observation of clear distinctions in token length growth across different types of cognitive tasks. As shown inFigures15,17and16, the extent of lengthening varies by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by repeatedly testing inputs to match the program\u2019s output. This suggests that the observed variation in token length is not incidental, but rather a reflection of task-specific reasoning behavior.",
        "parent": "4147cfe1",
        "line_number": 351
      },
      {
        "_key": "576b4e20",
        "_id": "sections/576b4e20",
        "title": "Research Question 6: Are all task types essential for good performance (Ablation)?",
        "level": 5,
        "content": "Due to resource constraints, we perform the ablation studies in this section and the next using onlyAbsolute Zero Reasoner-Base-7B. We begin by testing the importance of task types during training, with results shown inTable2. In row 1, both induction and abduction tasks are removed; in row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation occurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving general reasoning capability, with each contributing in a distinct and essential way.",
        "parent": "4147cfe1",
        "line_number": 363
      },
      {
        "_key": "a266c0e9",
        "_id": "sections/a266c0e9",
        "title": "Research Question 7: How much do the designs of proposer contribute to the overall performance (Ablation)?",
        "level": 5,
        "content": "Next, we ablate two components of the proposer role and present the results inTable2. First, we examine whether conditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction and deduction tasks, rather than dynamically conditioning onK\ud835\udc3eKitalic_Khistorical triplets (row 3). This results in a 5-point absolute drop in math performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps improve performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the solver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it may not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as studied in multitask learning literature(Suteu & Guo,2019). Thus, we believe that further investigation into how to make the proposer even more potent is an exciting and promising direction.",
        "parent": "4147cfe1",
        "line_number": 367
      },
      {
        "_key": "99cdcf82",
        "_id": "sections/99cdcf82",
        "title": "Additional Results.",
        "level": 5,
        "content": "Beyond the core research questions, we present additional results, including the breakdown of individual out-of-distribution benchmark scores during training for the 7B base and coder models inFigures28and29, for th 14B base and coder model inFigures30and31. For completeness, we also report in-distribution benchmark performance during training for the 7B base model inFigure14. Finally, we invite interested readers to exploreAppendixD, where we share several experimental directions that, while not yielding strong performance gains, produced interesting and insightful findings.",
        "parent": "4147cfe1",
        "line_number": 373
      },
      {
        "_key": "dc891869",
        "_id": "sections/dc891869",
        "title": "5Related Work",
        "level": 2,
        "content": "",
        "parent": null,
        "line_number": 377
      },
      {
        "_key": "60a19c2d",
        "_id": "sections/60a19c2d",
        "title": "Reasoning with RL.",
        "level": 5,
        "content": "Using RL to enhance reasoning capabilities has recently emerged as an important step in the post-training process of strong reasoning-focused large language models(Lambert et\u00a0al.,2024). One of the first works to explore a self-bootstrapping approach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to iteratively improve the model\u2019s CoT. A monumental work, o1(Jaech et\u00a0al.,2024), was among the first to deploy this idea on a scale, achieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model(DeepSeek-AI et\u00a0al.,2025)became the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in which reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to replicate the R1 process or to improve the underlying reinforcement learning algorithm(Zeng et\u00a0al.,2025b;Liu et\u00a0al.,2025;Cui et\u00a0al.,2025;Hu et\u00a0al.,2025;Yu et\u00a0al.,2025;Yuan et\u00a0al.,2025). Recent work explored RL on human defined procedural generated puzzles saw improvements in math(Xie et\u00a0al.,2025), and using one human example can almost match the performance of thousands(Wang et\u00a0al.,2025b). We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM without SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed, and refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.",
        "parent": "dc891869",
        "line_number": 379
      },
      {
        "_key": "de5dd0ca",
        "_id": "sections/de5dd0ca",
        "title": "Self-play.",
        "level": 5,
        "content": "The self-play paradigm can be traced back to early 2000s, whereSchmidhuber(2003;2011)(of course) explored a two-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically improves both agents, enabling theoretically never-ending progress(Schaul,2024). AlphaGo and AlphaZero(Silver et\u00a0al.,2016;2017)extend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of itself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance in the game of Go. Moreover, methods such as asymmetric self-play(Sukhbaatar et\u00a0al.,2018;OpenAI et\u00a0al.,2021), unsupervised environment design(Wang et\u00a0al.,2019;Dennis et\u00a0al.,2020), unsupervised reinforcement learning(Laskin et\u00a0al.,2021;Zhao et\u00a0al.,2022;2025b), and automatic goal generation(Florensa et\u00a0al.,2018)all center around inventing new tasks for an agent to learn from\u2014typically without supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative adversarial networks(Goodfellow et\u00a0al.,2020), also belong in this paradigm where a discriminator discriminate between real data and generated data, and the generated is trained to fool the discriminator.\nMost recently, SPIN and Self-Rewarding Language Models(Chen et\u00a0al.,2024;Yuan et\u00a0al.,2024)use the same instance of the lanugage models themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for alignment.Kirchner et\u00a0al.(2024)uses Prover-Verifier Game for increasing legibility and eva(Ye et\u00a0al.,2024)uses self-play for alignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks(Lambert et\u00a0al.,2024). SPCChen et\u00a0al.(2025)used self-play to train on human-curated tasks to increase the critic capabilities and SPAGCheng et\u00a0al.(2024)trained using self-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL(Xu et\u00a0al.,2025;Zhang et\u00a0al.,2025b;Zuo et\u00a0al.,2025)\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a fixed human defined learning task distribution. Finally, Minimo(Poesia et\u00a0al.,2024)extends self-play to formal mathematics, where a pair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play paradigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python input/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.",
        "parent": "dc891869",
        "line_number": 383
      },
      {
        "_key": "28d42fc2",
        "_id": "sections/28d42fc2",
        "title": "Weak-to-Strong Supervision.",
        "level": 5,
        "content": "The concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite being weaker than the learner\u2014still provides useful guidance(Burns et\u00a0al.,2024;Hinton et\u00a0al.,2015;Christiano,2018;2019;Demski & Garrabrant,2019;Leike & Sutskever,2023;Hubinger et\u00a0al.,2019). We consider a similar setting in which the learner may possess superhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach: guiding the learner\u2019s improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal. Furthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are entirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.",
        "parent": "dc891869",
        "line_number": 389
      },
      {
        "_key": "b2b354de",
        "_id": "sections/b2b354de",
        "title": "6Conclusion and Discussion",
        "level": 2,
        "content": "",
        "parent": null,
        "line_number": 393
      },
      {
        "_key": "664a21bc",
        "_id": "sections/664a21bc",
        "title": "Conclusion.",
        "level": 5,
        "content": "In this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing RLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving their reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR), which is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains. Remarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning agents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This demonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive domain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model sizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging paradigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.",
        "parent": "b2b354de",
        "line_number": 395
      }
    ],
    "entities": [],
    "topics": []
  },
  "edges": {
    "document_sections": [
      {
        "_from": "documents/c948c82c",
        "_to": "sections/65714a25"
      },
      {
        "_from": "documents/c948c82c",
        "_to": "sections/8c57baa8"
      },
      {
        "_from": "documents/c948c82c",
        "_to": "sections/74ddd0d9"
      },
      {
        "_from": "documents/c948c82c",
        "_to": "sections/0fb78c3b"
      },
      {
        "_from": "documents/c948c82c",
        "_to": "sections/dc891869"
      },
      {
        "_from": "documents/c948c82c",
        "_to": "sections/b2b354de"
      }
    ],
    "section_hierarchy": [
      {
        "_from": "sections/8c57baa8",
        "_to": "sections/ab1e8dfc"
      },
      {
        "_from": "sections/ab1e8dfc",
        "_to": "sections/c1244b05"
      },
      {
        "_from": "sections/ab1e8dfc",
        "_to": "sections/40bdb5ba"
      },
      {
        "_from": "sections/8c57baa8",
        "_to": "sections/1789e160"
      },
      {
        "_from": "sections/74ddd0d9",
        "_to": "sections/bd09ec8f"
      },
      {
        "_from": "sections/bd09ec8f",
        "_to": "sections/0828f98b"
      },
      {
        "_from": "sections/74ddd0d9",
        "_to": "sections/6b44e2ef"
      },
      {
        "_from": "sections/74ddd0d9",
        "_to": "sections/74ab0b53"
      },
      {
        "_from": "sections/74ab0b53",
        "_to": "sections/1126bd33"
      },
      {
        "_from": "sections/74ab0b53",
        "_to": "sections/573d192c"
      },
      {
        "_from": "sections/74ab0b53",
        "_to": "sections/423c0a18"
      },
      {
        "_from": "sections/74ab0b53",
        "_to": "sections/62f67bdc"
      },
      {
        "_from": "sections/74ab0b53",
        "_to": "sections/aa2e55eb"
      },
      {
        "_from": "sections/0fb78c3b",
        "_to": "sections/c64ae8d8"
      },
      {
        "_from": "sections/c64ae8d8",
        "_to": "sections/9a76f664"
      },
      {
        "_from": "sections/c64ae8d8",
        "_to": "sections/38b87620"
      },
      {
        "_from": "sections/c64ae8d8",
        "_to": "sections/7b68191f"
      },
      {
        "_from": "sections/0fb78c3b",
        "_to": "sections/4147cfe1"
      },
      {
        "_from": "sections/4147cfe1",
        "_to": "sections/4b6d9731"
      },
      {
        "_from": "sections/4147cfe1",
        "_to": "sections/c6b7e3db"
      },
      {
        "_from": "sections/4147cfe1",
        "_to": "sections/a92dae46"
      },
      {
        "_from": "sections/4147cfe1",
        "_to": "sections/dbca6d27"
      },
      {
        "_from": "sections/4147cfe1",
        "_to": "sections/5779fd26"
      },
      {
        "_from": "sections/4147cfe1",
        "_to": "sections/576b4e20"
      },
      {
        "_from": "sections/4147cfe1",
        "_to": "sections/a266c0e9"
      },
      {
        "_from": "sections/4147cfe1",
        "_to": "sections/99cdcf82"
      },
      {
        "_from": "sections/dc891869",
        "_to": "sections/60a19c2d"
      },
      {
        "_from": "sections/dc891869",
        "_to": "sections/de5dd0ca"
      },
      {
        "_from": "sections/dc891869",
        "_to": "sections/28d42fc2"
      },
      {
        "_from": "sections/b2b354de",
        "_to": "sections/664a21bc"
      }
    ],
    "entity_mentions": [],
    "topic_assignments": []
  },
  "original_content": {
    "format": "markdown",
    "content": "1Introduction\n\n2The Absolute Zero Paradigm2.1PreliminariesSupervised Fine-Tuning (SFT).Reinforcement Learning with Verifiable Rewards (RLVR).2.2Absolute Zero\n\n2.1PreliminariesSupervised Fine-Tuning (SFT).Reinforcement Learning with Verifiable Rewards (RLVR).\n\nSupervised Fine-Tuning (SFT).\n\nReinforcement Learning with Verifiable Rewards (RLVR).\n\n2.2Absolute Zero\n\n3Absolute Zero Reasoner3.1Two Roles in One: Proposer and SolverReward Design.3.2Learning Different Modes of Reasoning: Deduction, Induction, and Abduction3.3Absolute Zero Reasoner Learning Algorithm3.3.1Buffer Initialization3.3.2Task Proposal Inputs and Buffer Management3.3.3Constructing Valid Tasks3.3.4Answer Verification3.3.5Task-Relative REINFORCE++\n\n3.1Two Roles in One: Proposer and SolverReward Design.\n\nReward Design.\n\n3.2Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\n\n3.3Absolute Zero Reasoner Learning Algorithm3.3.1Buffer Initialization3.3.2Task Proposal Inputs and Buffer Management3.3.3Constructing Valid Tasks3.3.4Answer Verification3.3.5Task-Relative REINFORCE++\n\n3.3.1Buffer Initialization\n\n3.3.2Task Proposal Inputs and Buffer Management\n\n3.3.3Constructing Valid Tasks\n\n3.3.4Answer Verification\n\n3.3.5Task-Relative REINFORCE++\n\n4Experiments4.1Experiment SetupTraining Details.Evaluation Protocol.Baselines.4.2ResultsResearch Question 1: How does AZR compare to other zero setting models trained with human expert data?Research Question 2: How do initializing from different base model variants (base vs. coder) affect performance?Research Question 3: How does varying model size effect AZR\u2019s in-distribution and out-of-distribution capabilities?Research Question 4: Any interesting observations by changing the model class?Research Question 5: Any interesting behaviors or patterns observed during AZR training?Research Question 6: Are all task types essential for good performance (Ablation)?Research Question 7: How much do the designs of proposer contribute to the overall performance (Ablation)?Additional Results.\n\n4.1Experiment SetupTraining Details.Evaluation Protocol.Baselines.\n\nTraining Details.\n\nEvaluation Protocol.\n\nBaselines.\n\n4.2ResultsResearch Question 1: How does AZR compare to other zero setting models trained with human expert data?Research Question 2: How do initializing from different base model variants (base vs. coder) affect performance?Research Question 3: How does varying model size effect AZR\u2019s in-distribution and out-of-distribution capabilities?Research Question 4: Any interesting observations by changing the model class?Research Question 5: Any interesting behaviors or patterns observed during AZR training?Research Question 6: Are all task types essential for good performance (Ablation)?Research Question 7: How much do the designs of proposer contribute to the overall performance (Ablation)?Additional Results.\n\nResearch Question 1: How does AZR compare to other zero setting models trained with human expert data?\n\nResearch Question 2: How do initializing from different base model variants (base vs. coder) affect performance?\n\nResearch Question 3: How does varying model size effect AZR\u2019s in-distribution and out-of-distribution capabilities?\n\nResearch Question 4: Any interesting observations by changing the model class?\n\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training?\n\nResearch Question 6: Are all task types essential for good performance (Ablation)?\n\nResearch Question 7: How much do the designs of proposer contribute to the overall performance (Ablation)?\n\nAdditional Results.\n\n5Related WorkReasoning with RL.Self-play.Weak-to-Strong Supervision.\n\nReasoning with RL.\n\nSelf-play.\n\nWeak-to-Strong Supervision.\n\n6Conclusion and DiscussionConclusion.Discussion.\n\nConclusion.\n\nDiscussion.\n\nAReinforcement Learning with Verifiable Rewards.\n\nBImplementation DetailsTraining Hyperparameters.\n\nTraining Hyperparameters.\n\nCMore ResultsC.1Out-of-Distribution Performance BreakdownC.2In-Distribution ResultsC.3Interplay Between Propose and Solve RolesC.4Complexity and Diversity Metrics of AZR Proposed TasksC.5Generated Code Complexity Dynamics Between Abd/Ded and Ind.\n\nC.1Out-of-Distribution Performance Breakdown\n\nC.2In-Distribution Results\n\nC.3Interplay Between Propose and Solve Roles\n\nC.4Complexity and Diversity Metrics of AZR Proposed Tasks\n\nC.5Generated Code Complexity Dynamics Between Abd/Ded and Ind.\n\nDAlternative Approaches ConsideredD.1Error Deduction TaskD.2Composite Functions as Curriculum LearningD.3Toying with the Initialp\u2062(z)\ud835\udc5d\ud835\udc67p(z)italic_p ( italic_z )D.4Extra RewardsComplexity Rewards.Diversity Rewards.Reward Aggregation.D.5Environment TransitionRemoving Comments and DocstringsRemoving Global Variables.\n\nD.1Error Deduction Task\n\nD.2Composite Functions as Curriculum Learning\n\nD.3Toying with the Initialp\u2062(z)\ud835\udc5d\ud835\udc67p(z)italic_p ( italic_z )\n\nD.4Extra RewardsComplexity Rewards.Diversity Rewards.Reward Aggregation.\n\nComplexity Rewards.\n\nDiversity Rewards.\n\nReward Aggregation.\n\nD.5Environment TransitionRemoving Comments and DocstringsRemoving Global Variables.\n\nRemoving Comments and Docstrings\n\nRemoving Global Variables.\n\nmarginparsep has been altered.topmargin has been altered.marginparpush has been altered.The page layout violates the ICML style.Please do not change the page layout, or include packages like geometry,\nsavetrees, or fullpage, which change it for you.\nWe\u2019re not able to reliably undo arbitrary changes to the style. Please remove\nthe offending package(s), or layout-changing commands and try again.\n\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\n\nAndrew Zhao1,Yiran Wu3,Yang Yue1,Tong Wu2,Quentin Xu1,Yang Yue1,Matthieu Lin1,Shenzhi Wang1,Qingyun Wu3,Zilong Zheng2,\u27092\u2709{}^{\\,2,\\textrm{{\\char 0\\relax}}}start_FLOATSUPERSCRIPT 2 , \u2709 end_FLOATSUPERSCRIPTandGao Huang1,\u27091\u2709{}^{\\,1,\\textrm{{\\char 0\\relax}}}start_FLOATSUPERSCRIPT 1 , \u2709 end_FLOATSUPERSCRIPT\n\n11{}^{1\\,}start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPTTsinghua University22{}^{2\\,}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPTBeijing Institute for General Artificial Intelligence33{}^{3\\,}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTPennsylvania State University\n\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\n\n## 1Introduction\n\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement Learning with Verifiable Rewards (RLVR)(Lambert et\u00a0al.,2024). Unlike methods that explicitly imitate intermediate reasoning steps, RLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets(DeepSeek-AI et\u00a0al.,2025;Team et\u00a0al.,2025;Jaech et\u00a0al.,2024;OpenAI,2025b;a). A particularly compelling variant is the\u201czero\u201dRLVR paradigm(DeepSeek-AI et\u00a0al.,2025), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies RLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of reasoning question\u2013answer pairs, which raises serious concerns about their long-term scalability(Villalobos et\u00a0al.,2024). As reasoning models continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable(Yue et\u00a0al.,2025). A similar scalability bottleneck has already been identified in the domain of LLM pretraining(Sutskever et\u00a0al.,2024). Furthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed tasks risks imposing constraints on their capacity for autonomous learning and growth(Hughes et\u00a0al.,2024). This underscores the need for a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which AI systems may surpass human intelligence.\n\nTo this end, we propose\u201cAbsolute Zero\u201d, a new paradigm for reasoning models in which the model simultaneously learns to define tasks that maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In contrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to hacking(Silver et\u00a0al.,2017;Chen et\u00a0al.,2025;2024), theAbsolute Zeroparadigm is designed to operate in open-ended settings while remaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how humans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models(Hughes et\u00a0al.,2024). Similar to AlphaZero(Silver et\u00a0al.,2017), which improves through self-play, our proposed paradigm requires no human supervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward enabling large language models to autonomously achieve superhuman reasoning capabilities.\n\nBuilding on this new reasoning paradigm, we introduce theAbsolute Zero Reasoner (AZR), which proposes and solves coding tasks. We cast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable feedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a program, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We train the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of the proposed approach.\n\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks in math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned with domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically trained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points compared to models trained in the \u201czero\u201d setting using in-domain data. These surprising results highlight that general reasoning skills can emerge without human-curated domain targeted data, positioning Absolute Zero as an promising research direction and AZR as a first pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting findings summarized below:\n\n\u2022Code priors amplify reasoning.The baseQwen-Coder-7bmodel started with math performance 3.6 points lower thanQwen-7b. But after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities may potentially amplify overall reasoning improvements after AZR training.\n\nCode priors amplify reasoning.The baseQwen-Coder-7bmodel started with math performance 3.6 points lower thanQwen-7b. But after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities may potentially amplify overall reasoning improvements after AZR training.\n\n\u2022Cross domain transfer is more pronounced for AZR.After RLVR, expert code models raise math accuracy by only 0.65 points on average, whereasAZR-Base-7BandAZR-Coder-7Btrained on self-proposed code reasoning tasks improve math average by 10.9 and 15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\nCross domain transfer is more pronounced for AZR.After RLVR, expert code models raise math accuracy by only 0.65 points on average, whereasAZR-Base-7BandAZR-Coder-7Btrained on self-proposed code reasoning tasks improve math average by 10.9 and 15.2, respectively, demonstrating much stronger generalized reasoning capability gains.\n\n\u2022Bigger bases yield bigger gains.Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7, +10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\nBigger bases yield bigger gains.Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7, +10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.\n\n\u2022Comments as intermediate plans emerge naturally.When solving code induction tasks, AZR often interleaves step-by-step plans as comments and code (Figure19), resembling the ReAct prompting framework(Yao et\u00a0al.,2023). Similar behavior has been observed in much larger formal-math models such as DeepSeek Prover v2 (671B)(Ren et\u00a0al.,2025). We therefore believe that allowing the model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\nComments as intermediate plans emerge naturally.When solving code induction tasks, AZR often interleaves step-by-step plans as comments and code (Figure19), resembling the ReAct prompting framework(Yao et\u00a0al.,2023). Similar behavior has been observed in much larger formal-math models such as DeepSeek Prover v2 (671B)(Ren et\u00a0al.,2025). We therefore believe that allowing the model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.\n\n\u2022Cognitive Behaviors and Token length depends on reasoning mode.Distinct cognitive behaviors\u2014such as step-by-step reasoning, enumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different types of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction grows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\nCognitive Behaviors and Token length depends on reasoning mode.Distinct cognitive behaviors\u2014such as step-by-step reasoning, enumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different types of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction grows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.\n\n\u2022Safety alarms ringing.We observe AZR withLlama3.1-8boccasionally produces concerning chains of thought, we term the \u201cuh-oh moment\u201d, example shown inFigure32, highlighting the need for future work on safety-aware training(Zhang et\u00a0al.,2025a).\n\nSafety alarms ringing.We observe AZR withLlama3.1-8boccasionally produces concerning chains of thought, we term the \u201cuh-oh moment\u201d, example shown inFigure32, highlighting the need for future work on safety-aware training(Zhang et\u00a0al.,2025a).\n\n## 2The Absolute Zero Paradigm\n\n### 2.1Preliminaries\n\n##### Supervised Fine-Tuning (SFT).\n\nSFT requires the datasets of task-rationale-answer demonstrations\ud835\udc9f={(x,c\u22c6,y\u22c6)}\ud835\udc9f\ud835\udc65superscript\ud835\udc50\u22c6superscript\ud835\udc66\u22c6\\mathcal{D}=\\{(x,c^{\\star},y^{\\star})\\}caligraphic_D = { ( italic_x , italic_c start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ) }, wherex\ud835\udc65xitalic_xis the query,c\u22c6superscript\ud835\udc50\u22c6c^{\\star}italic_c start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis the gold chain-of-thought (CoT)) andy\u22c6superscript\ud835\udc66\u22c6y^{\\star}italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis the gold answer, all provided byhuman expertsorsuperior AI models. The model trains to imitate the reference responses to minimize the conditional negative log-likelihood(Ouyang et\u00a0al.,2022):\n\nHowever, at the frontier level, there\u2019s no stronger model to distill from, and expert human labeling doesn\u2019t scale well.\n\n##### Reinforcement Learning with Verifiable Rewards (RLVR).\n\nTo move beyond the limits of pure imitation, RLVR only requires a dataset of task and answer\ud835\udc9f={(x,y\u22c6)}\ud835\udc9f\ud835\udc65superscript\ud835\udc66\u22c6\\mathcal{D}=\\{(x,y^{\\star})\\}caligraphic_D = { ( italic_x , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ) }, without labeled rationale. RLVR allows the model to generate its own CoT and calculate a verifiable reward with the golden answerr\u2062(y,y\u22c6)\ud835\udc5f\ud835\udc66superscript\ud835\udc66\u22c6r(y,y^{\\star})italic_r ( italic_y , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ). However, the learning task distribution\ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic_D, with its set of queries and gold answers are still labeled byhuman experts.\nThe trainable policy\u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPTis optimized to maximize expected reward:\n\nIn summary, both SFT and RLVR still rely onhuman-curateddatasets of either queries, demonstrations, or verifiers, which ultimately limit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own interactions with the environment entirely through self-play.\n\n### 2.2Absolute Zero\n\nWe propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from both stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We illustrate this paradigm inFigure2, which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach eliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.\n\nTo make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid understanding, we include an illustration inFigure3. Let\u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPTbe our parameterized language model, it is used to play two roles, proposer\u03c0\u03b8proposesuperscriptsubscript\ud835\udf0b\ud835\udf03propose\\pi_{\\theta}^{\\text{propose}}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPTand solver\u03c0\u03b8solvesuperscriptsubscript\ud835\udf0b\ud835\udf03solve\\pi_{\\theta}^{\\text{solve}}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT solve end_POSTSUPERSCRIPTduring training.\n\nThe proposer first samples a proposed task conditioned on variablez\ud835\udc67zitalic_z:\u03c4\u223c\u03c0\u03b8propose(\u22c5|z)\\tau\\sim\\pi_{\\theta}^{\\text{propose}}(\\cdot|z)italic_\u03c4 \u223c italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT ( \u22c5 | italic_z ), which will then be validated and used to construct a valid reasoning task together with the environmente\ud835\udc52eitalic_e:(x,y\u22c6)\u223cfe(\u22c5|\u03c4)(x,y^{\\star})\\sim f_{e}(\\cdot|\\tau)( italic_x , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ) \u223c italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( \u22c5 | italic_\u03c4 ), wherex\ud835\udc65xitalic_xis the task query andy\u22c6superscript\ud835\udc66\u22c6y^{\\star}italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis the gold label. Then the solver produces an answery\u223c\u03c0\u03b8solve(\u22c5\u2223x)y\\sim\\pi_{\\theta}^{\\text{solve}}(\\,\\cdot\\mid x)italic_y \u223c italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT solve end_POSTSUPERSCRIPT ( \u22c5 \u2223 italic_x ). Each proposed task\u03c4\ud835\udf0f\\tauitalic_\u03c4is scored by alearnability rewardrepropose\u2062(\u03c4,\u03c0\u03b8)subscriptsuperscript\ud835\udc5fpropose\ud835\udc52\ud835\udf0fsubscript\ud835\udf0b\ud835\udf03r^{\\text{propose}}_{e}(\\tau,\\pi_{\\theta})italic_r start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_\u03c4 , italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ), which captures the expected improvement in\u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPTafter training on the task queryx\ud835\udc65xitalic_x. Moreover, the same policy also receives asolution rewardresolve\u2062(y,y\u22c6)subscriptsuperscript\ud835\udc5fsolve\ud835\udc52\ud835\udc66superscript\ud835\udc66\u22c6r^{\\text{solve}}_{e}(y,y^{\\star})italic_r start_POSTSUPERSCRIPT solve end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( italic_y , italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT )for its answer to the task queryx\ud835\udc65xitalic_x, with the environment again serving as the verifier. A nonnegative coefficient\u03bb\ud835\udf06\\lambdaitalic_\u03bbbalances the trade-off between exploring new, learnable tasks and improving the model\u2019s reasoning and problem-solving abilities. We formally define the absolute zero setting\u2019s objective as follows:\n\nNotice that we shift the burden of scaling data away fromhuman expertsand onto theproposer policy\u03c0\u03b8proposesuperscriptsubscript\ud835\udf0b\ud835\udf03propose\\pi_{\\theta}^{\\text{propose}}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPTand theenvironmente\ud835\udc52eitalic_e. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing grounded feedback that supports stable and self-sustainable training. When proposing,z\ud835\udc67zitalic_zacts as a conditional variable that seeds generation of tasks. Practically,z\ud835\udc67zitalic_zcan be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated task memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability rewardrpropose\u2062(\u03c4,\u03c0\u03b8)superscript\ud835\udc5fpropose\ud835\udf0fsubscript\ud835\udf0b\ud835\udf03r^{\\text{propose}}(\\tau,\\pi_{\\theta})italic_r start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT ( italic_\u03c4 , italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ), which measures how much the model is expected to improve by solving a proposed task\u03c4\ud835\udf0f\\tauitalic_\u03c4. Moreover, the solver rewardrsolve\u2062(y,y\u2217)superscript\ud835\udc5fsolve\ud835\udc66superscript\ud835\udc66r^{\\text{solve}}(y,y^{*})italic_r start_POSTSUPERSCRIPT solve end_POSTSUPERSCRIPT ( italic_y , italic_y start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT )evaluates the correctness of the model\u2019s output. Together, these two signals guide the model to propose tasks that are both challenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.\n\n## 3Absolute Zero Reasoner\n\nIn this section, we presentAbsolute Zero Reasoner(AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an unified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them to improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of reasoning capacity while enhancing its ability to solve them effectively\u00a0(Section3.1). Within this self-play training paradigm, the model learns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and induction\u00a0(Section3.2). Using coding tasks is motivated by the Turing-completeness of programming languages(Stuart,2015)and empirical evidence that code-based training improves reasoning(Aryabumi et\u00a0al.,2024). We adopt code as an open-ended, expressive, and verifiable medium for enabling reliable task construction and verification\u00a0(Section3.3). Finally, the model is updated using a newly proposed advantage estimator designed for multitask learning\u00a0(Section3.3.5). We outline the overall algorithm inAlgorithm1and highlight an illustration of our Absolute Zero Reasoner approach inFigure4. To expedite future exploration in this area, we also present several attempts that did not yield fruitful results but still warrant discussion inAppendixD.\n\n### 3.1Two Roles in One: Proposer and Solver\n\nLarge language models are naturally suited for implementing AZR in a multitask learning context(Radford et\u00a0al.,2019), as both the formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a single model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective inEquation3. At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined inSection3.2) andK\ud835\udc3eKitalic_Kpast self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples, promoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning tasks that can be verified using the environment, outlined later inSection3.3. AZR then attempts to solve these newly proposed tasks, receiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning. We now outline the rewards used for each role.\n\n##### Reward Design.\n\nPrior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning systems(Zeng et\u00a0al.,2025b). Motivated by this, we design a reward function for the proposer that encourages generation of tasks with meaningful learning potential\u2014neither too easy nor unsolvable for the current solver. Concretely, we use the same language model in its solver role to estimate thelearnabilityof a proposed task, a similar type of reward used in unsupervised environment design literature(Sukhbaatar et\u00a0al.,2018). We performn\ud835\udc5bnitalic_nMonte Carlo rollouts of the solver and compute the average success rate:r\u00afsolve=1n\u2062\u2211i=1Nrsolve(i)subscript\u00af\ud835\udc5fsolve1\ud835\udc5bsuperscriptsubscript\ud835\udc561\ud835\udc41superscriptsubscript\ud835\udc5fsolve\ud835\udc56\\bar{r}_{\\text{solve}}=\\frac{1}{n}\\sum_{i=1}^{N}r_{\\text{solve}}^{(i)}over\u00af start_ARG italic_r end_ARG start_POSTSUBSCRIPT solve end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT solve end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT. The proposer\u2019s reward is then defined as:\n\nThe intuition is that if a task is either trivial to solve (r\u00afsolve=1subscript\u00af\ud835\udc5fsolve1\\bar{r}_{\\text{solve}}=1over\u00af start_ARG italic_r end_ARG start_POSTSUBSCRIPT solve end_POSTSUBSCRIPT = 1) or unsolvable (r\u00afsolve=0subscript\u00af\ud835\udc5fsolve0\\bar{r}_{\\text{solve}}=0over\u00af start_ARG italic_r end_ARG start_POSTSUBSCRIPT solve end_POSTSUBSCRIPT = 0), the task provides little to no learning signal for the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the richest feedback and greatest potential for learning.\n\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\n\nwherey\u22c6superscript\ud835\udc66\u22c6y^{\\star}italic_y start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis the ground-truth answer, and equality is evaluated based on value equality in Python.\n\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which integratesrproposesubscript\ud835\udc5fproposer_{\\text{propose}}italic_r start_POSTSUBSCRIPT propose end_POSTSUBSCRIPTandrsolvesubscript\ud835\udc5fsolver_{\\text{solve}}italic_r start_POSTSUBSCRIPT solve end_POSTSUBSCRIPTwith a format-aware penalty inspired byDeepSeek-AI et\u00a0al.(2025):\n\nwherey\u03c0subscript\ud835\udc66\ud835\udf0by_{\\pi}italic_y start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTis the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek R1<think>and<answer>format, as shown inFigure33. Moreover, for the proposer, the reward criterion for format goes beyond simply following the XML structure. As detailed inSection3.3.3, only responses that produce valid triplets and pass the filtering stage are considered to be correctly formatted.\n\n### 3.2Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\n\nAZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution, and validation of code reasoning tasks(Stuart,2015;Aryabumi et\u00a0al.,2024). Give program space\ud835\udcab\ud835\udcab\\mathscr{P}script_P, input space\u2110\u2110\\mathscr{I}script_Iand output space\ud835\udcaa\ud835\udcaa\\mathscr{O}script_Oof a coding language, we define an AZR reasoning task as a triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), wherep\u2208\ud835\udcab\ud835\udc5d\ud835\udcabp\\in\\mathscr{P}italic_p \u2208 script_Pis a program,i\u2208\u2110\ud835\udc56\u2110i\\in\\mathscr{I}italic_i \u2208 script_Iis an input, ando\u2208\ud835\udcaa\ud835\udc5c\ud835\udcaao\\in\\mathscr{O}italic_o \u2208 script_Ois the corresponding output produced by running program on input,o=p\u2062(i)\ud835\udc5c\ud835\udc5d\ud835\udc56o=p(i)italic_o = italic_p ( italic_i ). AZR learns by reasoning about different parts of this task triplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n\n1.Deduction: predicting the outputo\ud835\udc5coitalic_ogiven a programp\ud835\udc5dpitalic_pand inputi\ud835\udc56iitalic_i, capturing step-by-step logical reasoning.\u2022As aproposer, AZR is conditioned on the task type\u03b1=deduction\ud835\udefcdeduction\\alpha=\\text{deduction}italic_\u03b1 = deductionandK\ud835\udc3eKitalic_Kreference examples from the deduction buffer\ud835\udc9fdeductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT(all task buffers are outlined inSection3.3), and generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i ). The environmente\ud835\udc52eitalic_ethen executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )to computeo\ud835\udc5coitalic_o, completing the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), which is added to the buffer if non-error output was produced.\u2022As asolver, the model receives(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )and predicts the outputo\u03c0subscript\ud835\udc5c\ud835\udf0bo_{\\pi}italic_o start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The predicted output is verified using type-aware value equality in python to account for possible variations (such as set ordering or fractions).\n\nDeduction: predicting the outputo\ud835\udc5coitalic_ogiven a programp\ud835\udc5dpitalic_pand inputi\ud835\udc56iitalic_i, capturing step-by-step logical reasoning.\n\n\u2022As aproposer, AZR is conditioned on the task type\u03b1=deduction\ud835\udefcdeduction\\alpha=\\text{deduction}italic_\u03b1 = deductionandK\ud835\udc3eKitalic_Kreference examples from the deduction buffer\ud835\udc9fdeductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT(all task buffers are outlined inSection3.3), and generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i ). The environmente\ud835\udc52eitalic_ethen executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )to computeo\ud835\udc5coitalic_o, completing the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), which is added to the buffer if non-error output was produced.\n\nAs aproposer, AZR is conditioned on the task type\u03b1=deduction\ud835\udefcdeduction\\alpha=\\text{deduction}italic_\u03b1 = deductionandK\ud835\udc3eKitalic_Kreference examples from the deduction buffer\ud835\udc9fdeductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT(all task buffers are outlined inSection3.3), and generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i ). The environmente\ud835\udc52eitalic_ethen executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )to computeo\ud835\udc5coitalic_o, completing the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), which is added to the buffer if non-error output was produced.\n\n\u2022As asolver, the model receives(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )and predicts the outputo\u03c0subscript\ud835\udc5c\ud835\udf0bo_{\\pi}italic_o start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The predicted output is verified using type-aware value equality in python to account for possible variations (such as set ordering or fractions).\n\nAs asolver, the model receives(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )and predicts the outputo\u03c0subscript\ud835\udc5c\ud835\udf0bo_{\\pi}italic_o start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The predicted output is verified using type-aware value equality in python to account for possible variations (such as set ordering or fractions).\n\n2.Abduction: inferring a plausible inputi\ud835\udc56iitalic_igiven the programp\ud835\udc5dpitalic_pand an outputo\ud835\udc5coitalic_o, resembling trial-and-error or online search.\u2022As aproposer, the policy\u03c0proposesuperscript\ud835\udf0bpropose\\pi^{\\text{propose}}italic_\u03c0 start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT\u2019s input and output is almost the same as the proposer for the deduction task, except that the task type\u03b1=abduction\ud835\udefcabduction\\alpha=\\text{abduction}italic_\u03b1 = abductionis changed as an input. The model generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )conditioned on\u03b1\ud835\udefc\\alphaitalic_\u03b1and reference examples. Then we executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )and get the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ).\u2022As asolver, the model receives(p,o)\ud835\udc5d\ud835\udc5c(p,o)( italic_p , italic_o )and predictsi\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The solution is verified by checking whetherp\u2062(i\u03c0)=o\ud835\udc5dsubscript\ud835\udc56\ud835\udf0b\ud835\udc5cp(i_{\\pi})=oitalic_p ( italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ) = italic_o. Since programs may not be bijective, we useoutputvalue equivalence rather than requiring exact input matches.\n\nAbduction: inferring a plausible inputi\ud835\udc56iitalic_igiven the programp\ud835\udc5dpitalic_pand an outputo\ud835\udc5coitalic_o, resembling trial-and-error or online search.\n\n\u2022As aproposer, the policy\u03c0proposesuperscript\ud835\udf0bpropose\\pi^{\\text{propose}}italic_\u03c0 start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT\u2019s input and output is almost the same as the proposer for the deduction task, except that the task type\u03b1=abduction\ud835\udefcabduction\\alpha=\\text{abduction}italic_\u03b1 = abductionis changed as an input. The model generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )conditioned on\u03b1\ud835\udefc\\alphaitalic_\u03b1and reference examples. Then we executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )and get the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ).\n\nAs aproposer, the policy\u03c0proposesuperscript\ud835\udf0bpropose\\pi^{\\text{propose}}italic_\u03c0 start_POSTSUPERSCRIPT propose end_POSTSUPERSCRIPT\u2019s input and output is almost the same as the proposer for the deduction task, except that the task type\u03b1=abduction\ud835\udefcabduction\\alpha=\\text{abduction}italic_\u03b1 = abductionis changed as an input. The model generates a pair(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )conditioned on\u03b1\ud835\udefc\\alphaitalic_\u03b1and reference examples. Then we executesp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )and get the triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ).\n\n\u2022As asolver, the model receives(p,o)\ud835\udc5d\ud835\udc5c(p,o)( italic_p , italic_o )and predictsi\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The solution is verified by checking whetherp\u2062(i\u03c0)=o\ud835\udc5dsubscript\ud835\udc56\ud835\udf0b\ud835\udc5cp(i_{\\pi})=oitalic_p ( italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ) = italic_o. Since programs may not be bijective, we useoutputvalue equivalence rather than requiring exact input matches.\n\nAs asolver, the model receives(p,o)\ud835\udc5d\ud835\udc5c(p,o)( italic_p , italic_o )and predictsi\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT. The solution is verified by checking whetherp\u2062(i\u03c0)=o\ud835\udc5dsubscript\ud835\udc56\ud835\udf0b\ud835\udc5cp(i_{\\pi})=oitalic_p ( italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ) = italic_o. Since programs may not be bijective, we useoutputvalue equivalence rather than requiring exact input matches.\n\n3.Induction:synthesizing a programp\ud835\udc5dpitalic_pfrom a set of in-out examples{(in,on)}superscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\\{(i^{n},o^{n})\\}{ ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) }, requiring generalization from partial information.\u2022As aproposer, AZR samples a valid programp\ud835\udc5dpitalic_pfrom\ud835\udc9fabduction\u222a\ud835\udc9fdeductionsubscript\ud835\udc9fabductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{abduction}}\\cup\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT abduction end_POSTSUBSCRIPT \u222a caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT, generatesN\ud835\udc41Nitalic_Nnew inputs and a messagem\ud835\udc5amitalic_m, and uses the environment to compute corresponding outputs. This forms an extended task representation(p,{(in,on)},m)\ud835\udc5dsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc5a(p,\\{(i^{n},o^{n})\\},m)( italic_p , { ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) } , italic_m ), which is stored in the induction buffer\ud835\udc9finductionsubscript\ud835\udc9finduction\\mathcal{D}_{\\text{induction}}caligraphic_D start_POSTSUBSCRIPT induction end_POSTSUBSCRIPT. Since infinitely many functions can map the inputs to the outputs, making the induction task under-constrained, the messagem\ud835\udc5amitalic_mhelps properly condition the problem for the solver.\u2022As asolver, the model is shown the first half of the input-output pairs and the messagem\ud835\udc5amitalic_m, and must synthesize a programp\u03c0subscript\ud835\udc5d\ud835\udf0bp_{\\pi}italic_p start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTthat correctly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else logic and promotes generalized induction.\n\nInduction:synthesizing a programp\ud835\udc5dpitalic_pfrom a set of in-out examples{(in,on)}superscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\\{(i^{n},o^{n})\\}{ ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) }, requiring generalization from partial information.\n\n\u2022As aproposer, AZR samples a valid programp\ud835\udc5dpitalic_pfrom\ud835\udc9fabduction\u222a\ud835\udc9fdeductionsubscript\ud835\udc9fabductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{abduction}}\\cup\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT abduction end_POSTSUBSCRIPT \u222a caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT, generatesN\ud835\udc41Nitalic_Nnew inputs and a messagem\ud835\udc5amitalic_m, and uses the environment to compute corresponding outputs. This forms an extended task representation(p,{(in,on)},m)\ud835\udc5dsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc5a(p,\\{(i^{n},o^{n})\\},m)( italic_p , { ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) } , italic_m ), which is stored in the induction buffer\ud835\udc9finductionsubscript\ud835\udc9finduction\\mathcal{D}_{\\text{induction}}caligraphic_D start_POSTSUBSCRIPT induction end_POSTSUBSCRIPT. Since infinitely many functions can map the inputs to the outputs, making the induction task under-constrained, the messagem\ud835\udc5amitalic_mhelps properly condition the problem for the solver.\n\nAs aproposer, AZR samples a valid programp\ud835\udc5dpitalic_pfrom\ud835\udc9fabduction\u222a\ud835\udc9fdeductionsubscript\ud835\udc9fabductionsubscript\ud835\udc9fdeduction\\mathcal{D}_{\\text{abduction}}\\cup\\mathcal{D}_{\\text{deduction}}caligraphic_D start_POSTSUBSCRIPT abduction end_POSTSUBSCRIPT \u222a caligraphic_D start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT, generatesN\ud835\udc41Nitalic_Nnew inputs and a messagem\ud835\udc5amitalic_m, and uses the environment to compute corresponding outputs. This forms an extended task representation(p,{(in,on)},m)\ud835\udc5dsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc5a(p,\\{(i^{n},o^{n})\\},m)( italic_p , { ( italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) } , italic_m ), which is stored in the induction buffer\ud835\udc9finductionsubscript\ud835\udc9finduction\\mathcal{D}_{\\text{induction}}caligraphic_D start_POSTSUBSCRIPT induction end_POSTSUBSCRIPT. Since infinitely many functions can map the inputs to the outputs, making the induction task under-constrained, the messagem\ud835\udc5amitalic_mhelps properly condition the problem for the solver.\n\n\u2022As asolver, the model is shown the first half of the input-output pairs and the messagem\ud835\udc5amitalic_m, and must synthesize a programp\u03c0subscript\ud835\udc5d\ud835\udf0bp_{\\pi}italic_p start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTthat correctly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else logic and promotes generalized induction.\n\nAs asolver, the model is shown the first half of the input-output pairs and the messagem\ud835\udc5amitalic_m, and must synthesize a programp\u03c0subscript\ud835\udc5d\ud835\udf0bp_{\\pi}italic_p start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTthat correctly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else logic and promotes generalized induction.\n\nEach reasoning task type leverages code as an expressive and verifiable medium, aligning with the Absolute Zero Paradigm\u2019s goals of fully self-improving systems in open-ended domains(DeepSeek-AI et\u00a0al.,2025;Lambert et\u00a0al.,2024). All prompts used by three different task types and two types of roles within a task type are shown inFigures36,34,35,39,37and38. Next, we outline exact details of our algorithm.\n\n### 3.3Absolute Zero Reasoner Learning Algorithm\n\nIn this section, we will discuss details of our AZR self-play algorithm, including initialization of buffers3.3.1, usage of thse buffers3.3.2, construction of valid tasks3.3.3, validating solutions3.3.4, and finally advantage estimator calculation3.3.5. We outline the overall recipe of the self-play procedure of AZR inAlgorithm1.\n\n#### 3.3.1Buffer Initialization\n\nTo initialize AZR self-play, we first generate a seed set of valid triplets using the base language model. Each prompt samples up toK\ud835\udc3eKitalic_Ktriplets from the current seed buffer\ud835\udc9fseedsubscript\ud835\udc9fseed\\mathcal{D}_{\\text{seed}}caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPTas references. When\ud835\udc9fseedsubscript\ud835\udc9fseed\\mathcal{D}_{\\text{seed}}caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPTis empty at time 0, we fall back to the zero triplet show inFigure5. During the seeding stage, we use the same proposer prompts detailed inFigures34,35and36.\n\nFirst, for deduction and abduction tasks, the LLM is prompted to generate(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )pairs, which are filtered, executed, and stored as valid triplets. We initialize\ud835\udc9fabduction0=\ud835\udc9fdeduction0=\ud835\udc9fseedsubscriptsuperscript\ud835\udc9f0abductionsubscriptsuperscript\ud835\udc9f0deductionsubscript\ud835\udc9fseed\\mathcal{D}^{0}_{\\text{abduction}}=\\mathcal{D}^{0}_{\\text{deduction}}=\\mathcal%\n{D}_{\\text{seed}}caligraphic_D start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT abduction end_POSTSUBSCRIPT = caligraphic_D start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT deduction end_POSTSUBSCRIPT = caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPT, where|\ud835\udc9fseed|=B\u00d7Ssubscript\ud835\udc9fseed\ud835\udc35\ud835\udc46|\\mathcal{D}_{\\text{seed}}|=B\\times S| caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPT | = italic_B \u00d7 italic_S, whereB\ud835\udc35Bitalic_Bis the batch size, andS=4\ud835\udc464S=4italic_S = 4is a factor we fix in all experiments. All seed triplet\u2019s program are stripped of global variables and comments (AppendixD), but subsequent iterations of adding new triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we sample programs from\ud835\udc9fseedsubscript\ud835\udc9fseed\\mathcal{D}_{\\text{seed}}caligraphic_D start_POSTSUBSCRIPT seed end_POSTSUBSCRIPT, generate matching input sets and messages, and collect valid examples until|\ud835\udc9finduction0|=B\u00d7Ssubscriptsuperscript\ud835\udc9f0induction\ud835\udc35\ud835\udc46|\\mathcal{D}^{0}_{\\text{induction}}|=B\\times S| caligraphic_D start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT induction end_POSTSUBSCRIPT | = italic_B \u00d7 italic_S.\n\n#### 3.3.2Task Proposal Inputs and Buffer Management\n\nDuring the actual self-play stage of AZR, we use the task buffer in three ways.First, for the proposer of abduction and deduction tasks, we uniformly sampleK\ud835\udc3eKitalic_Kpast triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task. The design is to show it past examples, and prompt it to generate a different one to promote diversity(Zhao et\u00a0al.,2025a).Second, we sample one triplet from the union of abduction and deduction buffers\ud835\udc9fabd\u2062\u22c3\ud835\udc9fdedsubscript\ud835\udc9fabdsubscript\ud835\udc9fded\\mathcal{D}_{\\text{abd}}\\bigcup\\mathcal{D}_{\\text{ded}}caligraphic_D start_POSTSUBSCRIPT abd end_POSTSUBSCRIPT \u22c3 caligraphic_D start_POSTSUBSCRIPT ded end_POSTSUBSCRIPT, and present the programp\ud835\udc5dpitalic_pfrom that triplet to the induction proposer to generate a set ofN\ud835\udc41Nitalic_Nmatching inputs{in}superscript\ud835\udc56\ud835\udc5b\\{i^{n}\\}{ italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT }and a natural language messagem\ud835\udc5amitalic_m.Lastly, to maintain stable training, if a batch of solver problems contains fewer thanB\ud835\udc35Bitalic_Bvalid proposed tasks (proposer not adhering to formatting), we fill the remainder by uniformly sampling from the corresponding task buffer of previously validated triplets.\n\nThe buffer grows for abduction and deduction tasks whenever\u03c0\ud835\udf0b\\piitalic_\u03c0propose a valid triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ), regardless if it gets any task reward. Similarly, for induction tasks, all valid triplets(p,{in,on}),m\ud835\udc5dsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc5a(p,\\{i^{n},o^{n}\\}),m( italic_p , { italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT } ) , italic_mare added to the buffer.\n\n#### 3.3.3Constructing Valid Tasks\n\nProposal Task Validation.We first describe how we construct valid tasks from the proposals generated by the policy\u03c0\ud835\udf0b\\piitalic_\u03c0. Fordeduction and abductiontasks, each proposal consists of a program and an input(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i ). To validate the task, we use the task validation procedure (steps shown below) on the input to obtain the correct outputo\ud835\udc5coitalic_o, resulting in a complete triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o ). Forinductiontasks, given a programp\ud835\udc5dpitalic_pthe policy proposes a set of inputs{in}superscript\ud835\udc56\ud835\udc5b\\{i^{n}\\}{ italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT }and messagem\ud835\udc5amitalic_m. We also use the task validation procedure on each of the inputinsuperscript\ud835\udc56\ud835\udc5bi^{n}italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPTin the set to obtain a corresponding outputonsuperscript\ud835\udc5c\ud835\udc5bo^{n}italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, forming a set of input-output pairs{in,on}superscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\\{i^{n},o^{n}\\}{ italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT }. We do not impose any constraints onm\ud835\udc5amitalic_m. The resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. Thetask validation procedureentails:\n\n1.Program Integrity.We first use Python to run the programp\ud835\udc5dpitalic_pwith the inputi\ud835\udc56iitalic_i. If no errors are raised and something is returned, we then gather the outputo\ud835\udc5coitalic_oof that(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )pair and determine that the program at least has valid syntax.\n\nProgram Integrity.We first use Python to run the programp\ud835\udc5dpitalic_pwith the inputi\ud835\udc56iitalic_i. If no errors are raised and something is returned, we then gather the outputo\ud835\udc5coitalic_oof that(p,i)\ud835\udc5d\ud835\udc56(p,i)( italic_p , italic_i )pair and determine that the program at least has valid syntax.\n\n2.Program Safety.We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might cause harm to the Python environment,i.e.,os.sys, sys, shutil. The list of packages used to filter out invalid programs is provided inFigure8. This list is also included in the instructions when prompting the language model to generate questions. SeeFigures34,35and36.\n\nProgram Safety.We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might cause harm to the Python environment,i.e.,os.sys, sys, shutil. The list of packages used to filter out invalid programs is provided inFigure8. This list is also included in the instructions when prompting the language model to generate questions. SeeFigures34,35and36.\n\n3.Check for Determinism.In our setting, we only considerdeterministic programs,i.e.,p\u2208\ud835\udcabdeterministic\u2282\ud835\udcab\ud835\udc5dsubscript\ud835\udcabdeterministic\ud835\udcabp\\in\\mathscr{P}_{\\text{deterministic}}\\subset\\mathscr{P}italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT \u2282 script_P, where\ud835\udcab\ud835\udcab\\mathscr{P}script_Pis the space of all valid programs and\u2110\u2110\\mathscr{I}script_Iis the space of all valid inputs:\u2200p\u2208\ud835\udcabdeterministic,\u2200i\u2208\u2110,(limj\u2192\u221ep\u2062(i)(1)=p\u2062(i)(2)=\u22ef=p\u2062(i)(j)),formulae-sequencefor-all\ud835\udc5dsubscript\ud835\udcabdeterministicfor-all\ud835\udc56\u2110subscript\u2192\ud835\udc57\ud835\udc5dsuperscript\ud835\udc561\ud835\udc5dsuperscript\ud835\udc562\u22ef\ud835\udc5dsuperscript\ud835\udc56\ud835\udc57\\forall p\\in\\mathscr{P}_{\\text{deterministic}},\\ \\forall i\\in\\mathscr{I},\\ %\n\\left(\\lim_{j\\to\\infty}p(i)^{(1)}=p(i)^{(2)}=\\dots=p(i)^{(j)}\\right),\u2200 italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT , \u2200 italic_i \u2208 script_I , ( roman_lim start_POSTSUBSCRIPT italic_j \u2192 \u221e end_POSTSUBSCRIPT italic_p ( italic_i ) start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT = italic_p ( italic_i ) start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT = \u22ef = italic_p ( italic_i ) start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT ) ,(7)where(j)\ud835\udc57(j)( italic_j )indexes repeated independent executions of the program. That is, for all inputsi\ud835\udc56iitalic_i, the output ofp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )remains identical with any independent execution of the program. Avalid program/input/output triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o )is defined such thato=p\u2062(i)\ud835\udc5c\ud835\udc5d\ud835\udc56o=p(i)italic_o = italic_p ( italic_i ), wherep\u2208\ud835\udcabdeterministic\ud835\udc5dsubscript\ud835\udcabdeterministicp\\in\\mathscr{P}_{\\text{deterministic}}italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT.Since the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the correctness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class of deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and promising to include in future versions of AZR.To implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted inEquation7, we approximate this procedure by independently running the programj\ud835\udc57jitalic_jfinite times and checking that all the outputs are equal. For computational budget reasons, we fixedj=2\ud835\udc572j=2italic_j = 2for all experiments.\n\nCheck for Determinism.In our setting, we only considerdeterministic programs,i.e.,p\u2208\ud835\udcabdeterministic\u2282\ud835\udcab\ud835\udc5dsubscript\ud835\udcabdeterministic\ud835\udcabp\\in\\mathscr{P}_{\\text{deterministic}}\\subset\\mathscr{P}italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT \u2282 script_P, where\ud835\udcab\ud835\udcab\\mathscr{P}script_Pis the space of all valid programs and\u2110\u2110\\mathscr{I}script_Iis the space of all valid inputs:\n\nwhere(j)\ud835\udc57(j)( italic_j )indexes repeated independent executions of the program. That is, for all inputsi\ud835\udc56iitalic_i, the output ofp\u2062(i)\ud835\udc5d\ud835\udc56p(i)italic_p ( italic_i )remains identical with any independent execution of the program. Avalid program/input/output triplet(p,i,o)\ud835\udc5d\ud835\udc56\ud835\udc5c(p,i,o)( italic_p , italic_i , italic_o )is defined such thato=p\u2062(i)\ud835\udc5c\ud835\udc5d\ud835\udc56o=p(i)italic_o = italic_p ( italic_i ), wherep\u2208\ud835\udcabdeterministic\ud835\udc5dsubscript\ud835\udcabdeterministicp\\in\\mathscr{P}_{\\text{deterministic}}italic_p \u2208 script_P start_POSTSUBSCRIPT deterministic end_POSTSUBSCRIPT.\n\nSince the output of probabilistic programs can vary on every individual run, it is non-trivial to use verifiable functions to evaluate the correctness of an answer. Therefore, to keep the verifier simple, we restrict the valid programs generated by the learner to the class of deterministic programs. We believe that stochastic programs can encompass a larger class of behaviors and are important and promising to include in future versions of AZR.\n\nTo implement the filtering of invalid probabilistic programs, and following the definition of a deterministic program highlighted inEquation7, we approximate this procedure by independently running the programj\ud835\udc57jitalic_jfinite times and checking that all the outputs are equal. For computational budget reasons, we fixedj=2\ud835\udc572j=2italic_j = 2for all experiments.\n\nSolving Task Construction.If a task proposal passes these three checks, we deem it a valid task and apply appropriate procedures to present part of the triplet to the solver. Specifically, we setx=(p,i)\ud835\udc65\ud835\udc5d\ud835\udc56x=(p,i)italic_x = ( italic_p , italic_i )for deduction;x=(p,o)\ud835\udc65\ud835\udc5d\ud835\udc5cx=(p,o)italic_x = ( italic_p , italic_o )for abduction; andx=({in,on}n=1N\u2063/\u2063/2,m)\ud835\udc65subscriptsuperscriptsuperscript\ud835\udc56\ud835\udc5bsuperscript\ud835\udc5c\ud835\udc5b\ud835\udc41absent2\ud835\udc5b1\ud835\udc5ax=(\\{i^{n},o^{n}\\}^{N//2}_{n=1},m)italic_x = ( { italic_i start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_o start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT } start_POSTSUPERSCRIPT italic_N / / 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT , italic_m )for induction, where half of the tests cases and a program descriptionm\ud835\udc5amitalic_mis used. We use all valid tasks from timestept\ud835\udc61titalic_t; if the batchB\ud835\udc35Bitalic_Bis not full, we uniformly sample from previously validated tasks to fill the batch.\n\n#### 3.3.4Answer Verification\n\nFor abduction task, we receivei\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTfrom the solver policy, then we equivalence match usingp\u2062(i\u03c0)=p\u2062(i\u22c6)\ud835\udc5dsubscript\ud835\udc56\ud835\udf0b\ud835\udc5dsuperscript\ud835\udc56\u22c6p(i_{\\pi})=p(i^{\\star})italic_p ( italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ) = italic_p ( italic_i start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ), where\u2217*\u2217refers to the privileged gold information. The reason we do not just matchi\u03c0subscript\ud835\udc56\ud835\udf0bi_{\\pi}italic_i start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPTandi\u22c6superscript\ud835\udc56\u22c6i^{\\star}italic_i start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPTis becausep\ud835\udc5dpitalic_pis not necessarily bijective. For deduction task, we matcho\u03c0=o\u22c6subscript\ud835\udc5c\ud835\udf0bsuperscript\ud835\udc5c\u22c6o_{\\pi}=o^{\\star}italic_o start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT = italic_o start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT. For induction, we matchall\u2061({p\u03c0\u2062(in\u22c6)=on\u22c6}N)allsuperscriptsubscript\ud835\udc5d\ud835\udf0bsuperscriptsubscript\ud835\udc56\ud835\udc5b\u22c6superscriptsubscript\ud835\udc5c\ud835\udc5b\u22c6\ud835\udc41\\operatorname{all}(\\{p_{\\pi}(i_{n}^{\\star})=o_{n}^{\\star}\\}^{N})roman_all ( { italic_p start_POSTSUBSCRIPT italic_\u03c0 end_POSTSUBSCRIPT ( italic_i start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT ) = italic_o start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22c6 end_POSTSUPERSCRIPT } start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ). This part might be convoluted to explain in language, therefore we recommend the reader to see how we did abduction, deduction and induction verification in code inFigures10,11and12, respectively.\n\n#### 3.3.5Task-Relative REINFORCE++\n\nSince AZR trains the combination of roles and task types, it operates in a multitask reinforcement learning setup(Zhang & Yang,2021;Zhao et\u00a0al.,2022;Wang et\u00a0al.,2023;Yue et\u00a0al.,2023). Instead of computing a single global baseline as in REINFORCE++(Hu,2025)(AppendixA), we compute separate baselines for each of the six task-role configurations. This can be viewed as an interpolation between per-question baselines, as in GRPO(Shao et\u00a0al.,2024), and a global baseline, allowing for more structured variance reduction tailored to each task setup. We refer to this variant asTask-Relative REINFORCE++ (TRR++). The normalized advantageAnormsuperscript\ud835\udc34normA^{\\text{norm}}italic_A start_POSTSUPERSCRIPT norm end_POSTSUPERSCRIPTis computed as:\n\nwhere the mean and standard deviation are computedwithin each task type and role, yielding six baselines.\n\n## 4Experiments\n\n### 4.1Experiment Setup\n\n##### Training Details.\n\nFor all experiments, we initialize the buffers as described inSection3.1. AZR models are trained using a batch size of64\u00d7664664\\times 664 \u00d7 6(2 roles\u00d7\\times\u00d73 task types). We use constant learning rate=1\u2062e\u22126absent1\ud835\udc526=1e{-6}= 1 italic_e - 6and the AdamW optimizer(Loshchilov & Hutter,2019). Complete list of hyperparameters is provided inTable3.\n\nFor the main experiments, we train AZR models onQwen2.5-7BandQwen2.5-7B-Coder, resulting inAbsolute Zero Reasoner-base-7BandAbsolute Zero Reasoner-Coder-7B, respectively. Additional experiments include trainingQwen2.5-Coder-3B,Qwen2.5-Coder-14B,Qwen2.5-14B,Llama-3.1-8B(Yang et\u00a0al.,2024a;Hui et\u00a0al.,2024;Dubey et\u00a0al.,2024).\n\n##### Evaluation Protocol.\n\nTo evaluate our models, we divide the datasets into in-distribution (ID) and out-of-distribution (OOD) categories. For OOD benchmarks, which we emphasize more, we further categorize them into coding and mathematical reasoning benchmarks. For coding tasks, we evaluate using Evalplus(Liu et\u00a0al.,2023)on the HumanEval+ and MBPP+ benchmarks, as well as LiveCodeBench Generation (v1-5, May 23-Feb 25)(Jain et\u00a0al.,2024). For mathematical reasoning, we utilize six standard benchmarks commonly used in recent zero-shot trained reasoners: AIME\u201924, AIME\u201925, OlympiadBench(He et\u00a0al.,2024), Minerva, Math500(Hendrycks et\u00a0al.,2021), and AMC\u201923. For ID benchmarks, we use CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution(Gu et\u00a0al.,2024;Jain et\u00a0al.,2024), which assess reasoning capabilities regarding the input and output of programs(Li et\u00a0al.,2025).Greedy decodingis used for all baseline methods and AZR results to ensure reproducibility.\n\n##### Baselines.\n\nFor our main results, we useQwen2.5-7Bas the base model, along with its specialized base model variants:Qwen2.5-7B-Coder,Qwen2.5-7B-Instruct, andQwen2.5-Math-7B(Yang et\u00a0al.,2024a;Hui et\u00a0al.,2024;Yang et\u00a0al.,2024b). Furthermore, the zero-style models are usually trained specifically on either code or math data; and onlyEurus-2-7B-PRIME-Zero(Cui et\u00a0al.,2025)was trained jointly on both domains. For code data models, we present four variants of theAceCoder(Zeng et\u00a0al.,2025a)and two differentCodeR1models(Liu & Zhang,2025). For math data models, we haveQwen2.5-Math-7B-Oat-Zero(Liu et\u00a0al.,2025),Open-Reasoner-Zero-7B(ORZ)(Hu et\u00a0al.,2025),Qwen-2.5-7B-SimpleRL-Zoo(Zeng et\u00a0al.,2025b). All baseline models\u2019 training data and initialization settings are summarized inTable4. For follow-up scaling experiments, we compare each AZR model against its own corresponding base model, due to the lack of established baselines across different parameter scales. Finally, we compare ourLlama3.1-8B-trained model withLlama-3.1-8B-SimpleRL-Zoo(Zeng et\u00a0al.,2025b)and the base model.\n\n### 4.2Results\n\n##### Research Question 1: How does AZR compare to other zero setting models trained with human expert data?\n\nWe present the main results of reasoning models trained under both the standard zero and our proposed absolute zero settings inTable1. Notably,Absolute Zero Reasoner-Coder-7Bachieves state-of-the-art performance in both the 7B overall average and the coding average categories. Despite being entirely out-of-distribution for both math and code reasoning benchmarks, it surpasses the previous best model by 1.8 absolute percentages. Even more strikingly, it outperforms models trained with expert-curated human data in the coding category by0.30.30.30.3absolute percentages, while never having access to such data itself.\n\nStrong Cross-domain Generalization.To assess cross-domain generalization after RLVR, we evaluate math performance before and after training, comparing AZR models with other expert code models, since AZR was trained in coding environments. After training, most expert code models showed minimal changes or even declines in performance compared to their base versions, with an average increase of only 0.65 points across these models, indicating very limited cross-domain generalization. In contrast, AZR base and coder models achieved gains of 10.9 and 15.2 percentage points, respectively, demonstrating substantially stronger generalized reasoning improvements. Similarly, although also out-of-distribution on human-defined code generation tasks, our AZR models improved by 3.2 and 5.0 points, while the math models on average showed just a moderate increases in coding (+2.0 on average).\n\nOverall, these results highlight the surprising effectiveness of our approach. Unlike other RLVR models trained and evaluated on human-defined tasks, our AZR models demonstrate strong general reasoning capabilities without any direct training on downstream human-defined math or coding data, only had access to self-proposed tasks during training.\n\n##### Research Question 2: How do initializing from different base model variants (base vs. coder) affect performance?\n\nAs shown inTable1, the coder variant achieved better overall performance in both math and coding after the AZR self-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla base model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency as a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.\n\n##### Research Question 3: How does varying model size effect AZR\u2019s in-distribution and out-of-distribution capabilities?\n\nWe examine the effects of scaling model size and present both in-distribution and out-of-distribution results inFigure6(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller and larger variants:Qwen2.5-3B-CoderandQwen2.5-14B-Coder. Due to the absence of existing baselines for these zero-style reasoner models, we compare each model\u2019s performance to its corresponding base coder model.\n\nThe results reveal a clear trend: our method deliversgreater gains on larger, more capable models. In the in-distribution setting, the 7B and 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution domains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance gains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that scaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute Zero paradigm.\n\n(a)\n\n(b)\n\n##### Research Question 4: Any interesting observations by changing the model class?\n\nWe also evaluate our method on a different model class, usingLlama3.1-8Bas the base shown inFigure6. Unlike the 3B and 14B categories, this setting has an existing baseline,SimpleRL(Zeng et\u00a0al.,2025b), which enables a direct comparison. AlthoughLlama3.1-8Bis less capable than theQwen2.5models, our method still produces moderate improvements (+3.2), demonstrating AZR\u2019s effectiveness even on relatively weaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements tend to scale with initial base model potency.\n\n##### Research Question 5: Any interesting behaviors or patterns observed during AZR training?\n\nWe observed interesting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as string manipulation tasks, dynamic programming problems, and practical cases (e.g., calculating a triangle\u2019s area using Heron\u2019s formula). We show a concrete example inFigure7, where AZR proposes a code problem that searches for the sum of continuous sub-arrays matching a target value and solves it through trial-and-error.\n\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks, it repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs, it steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is reached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to confirm that its program produces correct results. We showcase more concrete examples of these behaviors inFigures26,25,24,23,22,21,20and18. We also share some fun \u201cvibe checks\u201d such as solving Sudoku and solving thesum-product gameinFigures40and41.\n\nIntermediate Planning During Code Response.Another interesting pattern emerged in our AZR models during the code induction task: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct prompting framework(Yao et\u00a0al.,2023). A similar behavior has been observed in recent formal math proving models, such asDeepSeek Prover v2, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning as a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior inlong-form responsesacross other domains.\n\nCognitive Behavior in Llama.Interestingly, we also observed some emergent cognitive patterns inAbsolute Zero Reasoner-Llama3.1-8B, similar to those reported byZeng et\u00a0al.(2025b), and we include one example inFigure26, where clear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought from the Llama model trained with AZR. One example includes the output: \u201cThe aim is to outsmart all these groups of intelligent machines and less intelligent humans. This is for the brains behind the future\u201d shown inFigure32. We refer to this as the\u201cuh-oh moment\u201dand encourage future work to further investigate its potential implications.\n\nToken Length Increase Depends on Task Type.Finally, we observed that token length increases over the course of training, consistent with findings from recent studies(Hu et\u00a0al.,2025;Liu et\u00a0al.,2025). Interestingly, our results reveal one of the first observation of clear distinctions in token length growth across different types of cognitive tasks. As shown inFigures15,17and16, the extent of lengthening varies by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by repeatedly testing inputs to match the program\u2019s output. This suggests that the observed variation in token length is not incidental, but rather a reflection of task-specific reasoning behavior.\n\n##### Research Question 6: Are all task types essential for good performance (Ablation)?\n\nDue to resource constraints, we perform the ablation studies in this section and the next using onlyAbsolute Zero Reasoner-Base-7B. We begin by testing the importance of task types during training, with results shown inTable2. In row 1, both induction and abduction tasks are removed; in row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation occurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving general reasoning capability, with each contributing in a distinct and essential way.\n\n##### Research Question 7: How much do the designs of proposer contribute to the overall performance (Ablation)?\n\nNext, we ablate two components of the proposer role and present the results inTable2. First, we examine whether conditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction and deduction tasks, rather than dynamically conditioning onK\ud835\udc3eKitalic_Khistorical triplets (row 3). This results in a 5-point absolute drop in math performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps improve performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\n\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the solver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it may not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as studied in multitask learning literature(Suteu & Guo,2019). Thus, we believe that further investigation into how to make the proposer even more potent is an exciting and promising direction.\n\n##### Additional Results.\n\nBeyond the core research questions, we present additional results, including the breakdown of individual out-of-distribution benchmark scores during training for the 7B base and coder models inFigures28and29, for th 14B base and coder model inFigures30and31. For completeness, we also report in-distribution benchmark performance during training for the 7B base model inFigure14. Finally, we invite interested readers to exploreAppendixD, where we share several experimental directions that, while not yielding strong performance gains, produced interesting and insightful findings.\n\n## 5Related Work\n\n##### Reasoning with RL.\n\nUsing RL to enhance reasoning capabilities has recently emerged as an important step in the post-training process of strong reasoning-focused large language models(Lambert et\u00a0al.,2024). One of the first works to explore a self-bootstrapping approach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to iteratively improve the model\u2019s CoT. A monumental work, o1(Jaech et\u00a0al.,2024), was among the first to deploy this idea on a scale, achieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model(DeepSeek-AI et\u00a0al.,2025)became the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in which reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to replicate the R1 process or to improve the underlying reinforcement learning algorithm(Zeng et\u00a0al.,2025b;Liu et\u00a0al.,2025;Cui et\u00a0al.,2025;Hu et\u00a0al.,2025;Yu et\u00a0al.,2025;Yuan et\u00a0al.,2025). Recent work explored RL on human defined procedural generated puzzles saw improvements in math(Xie et\u00a0al.,2025), and using one human example can almost match the performance of thousands(Wang et\u00a0al.,2025b). We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM without SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed, and refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.\n\n##### Self-play.\n\nThe self-play paradigm can be traced back to early 2000s, whereSchmidhuber(2003;2011)(of course) explored a two-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically improves both agents, enabling theoretically never-ending progress(Schaul,2024). AlphaGo and AlphaZero(Silver et\u00a0al.,2016;2017)extend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of itself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance in the game of Go. Moreover, methods such as asymmetric self-play(Sukhbaatar et\u00a0al.,2018;OpenAI et\u00a0al.,2021), unsupervised environment design(Wang et\u00a0al.,2019;Dennis et\u00a0al.,2020), unsupervised reinforcement learning(Laskin et\u00a0al.,2021;Zhao et\u00a0al.,2022;2025b), and automatic goal generation(Florensa et\u00a0al.,2018)all center around inventing new tasks for an agent to learn from\u2014typically without supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative adversarial networks(Goodfellow et\u00a0al.,2020), also belong in this paradigm where a discriminator discriminate between real data and generated data, and the generated is trained to fool the discriminator.\n\nMost recently, SPIN and Self-Rewarding Language Models(Chen et\u00a0al.,2024;Yuan et\u00a0al.,2024)use the same instance of the lanugage models themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for alignment.Kirchner et\u00a0al.(2024)uses Prover-Verifier Game for increasing legibility and eva(Ye et\u00a0al.,2024)uses self-play for alignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks(Lambert et\u00a0al.,2024). SPCChen et\u00a0al.(2025)used self-play to train on human-curated tasks to increase the critic capabilities and SPAGCheng et\u00a0al.(2024)trained using self-play in specific game of Adversarial Taboo. Concurrent works\u2014Genius, EMPO, and TTRL(Xu et\u00a0al.,2025;Zhang et\u00a0al.,2025b;Zuo et\u00a0al.,2025)\u2014leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a fixed human defined learning task distribution. Finally, Minimo(Poesia et\u00a0al.,2024)extends self-play to formal mathematics, where a pair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play paradigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python input/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.\n\n##### Weak-to-Strong Supervision.\n\nThe concept of weak-to-strong supervision has been studied in prior work, where a teacher\u2014despite being weaker than the learner\u2014still provides useful guidance(Burns et\u00a0al.,2024;Hinton et\u00a0al.,2015;Christiano,2018;2019;Demski & Garrabrant,2019;Leike & Sutskever,2023;Hubinger et\u00a0al.,2019). We consider a similar setting in which the learner may possess superhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach: guiding the learner\u2019s improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal. Furthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor\u2014they are entirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.\n\n## 6Conclusion and Discussion\n\n##### Conclusion.\n\nIn this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing RLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving their reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR), which is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\n\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains. Remarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning agents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This demonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive domain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model sizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging paradigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.",
    "extraction_method": "beautifulsoup"
  }
}