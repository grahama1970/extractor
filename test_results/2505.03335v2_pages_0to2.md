![](_page_0_Picture_0.jpeg)

# **Absolute Zero: Reinforced Self-play Reasoning with Zero Data Andrew Zhao** <sup>1</sup> **, Yiran Wu**<sup>3</sup> **, Yang Yue** <sup>1</sup> **, Tong Wu**<sup>2</sup> **, Quentin Xu**<sup>1</sup> **, Yang Yue** <sup>1</sup> **, Matthieu Lin**<sup>1</sup>

**, Shenzhi Wang** <sup>1</sup> **, Qingyun Wu**<sup>3</sup> **, Zilong Zheng** <sup>2</sup>*,* **and Gao Huang** <sup>1</sup>*,*

1 Tsinghua University 2 Beijing Institute for General Artificial Intelligence 3 Pennsylvania State University

zqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn

Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the *zero setting* avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of highquality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called *Absolute Zero*, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely *without external data*, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, *outperforming existing zero-setting models* that rely on tens of thousands of *in-domain human-curated examples*. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.

![](_page_0_Figure_7.jpeg)

*Figure 1.* **Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA**. Without relying on any gold labels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general reasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses models trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.

*Corresponding author(s)*

**Absolute Zero: Reinforced Self-play Reasoning with Zero Data**

![](_page_1_Figure_1.jpeg)

Less Human Supervision

<span id="page-1-0"></span>*Figure 2.* **Absolute Zero Paradigm. Supervised learning** relies on human-curated reasoning traces for behavior cloning. **Reinforcement learning from verified rewards**, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a respective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm, **Absolute Zero**, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks optimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that provides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.

## **1. Introduction**

Large language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement Learning with Verifiable Rewards (RLVR) [\(Lambert et al.,](#page-15-0) [2024\)](#page-15-0). Unlike methods that explicitly imitate intermediate reasoning steps, RLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets [\(DeepSeek-AI et al.,](#page-13-0) [2025;](#page-13-0) [Team et al.,](#page-16-0) [2025;](#page-16-0) [Jaech et al.,](#page-14-0) [2024;](#page-14-0) [OpenAI,](#page-15-1) [2025b;](#page-15-1)[a\)](#page-15-2). A particularly compelling variant is the *"zero"* RLVR paradigm [\(DeepSeek-AI](#page-13-0) [et al.,](#page-13-0) [2025\)](#page-13-0), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies RLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of reasoning questionâ€“answer pairs, which raises serious concerns about their long-term scalability [\(Villalobos et al.,](#page-16-1) [2024\)](#page-16-1). As reasoning models continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable [\(Yue](#page-17-0) [et al.,](#page-17-0) [2025\)](#page-17-0). A similar scalability bottleneck has already been identified in the domain of LLM pretraining [\(Sutskever et al.,](#page-16-2) [2024\)](#page-16-2). Furthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed tasks risks imposing constraints on their capacity for autonomous learning and growth [\(Hughes et al.,](#page-14-1) [2024\)](#page-14-1). This underscores the need for a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which AI systems may surpass human intelligence.

To this end, we propose *"Absolute Zero"*, a new paradigm for reasoning models in which the model simultaneously learns to define tasks that maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In contrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to hacking [\(Silver et al.,](#page-16-3) [2017;](#page-16-3) [Chen et al.,](#page-13-1) [2025;](#page-13-1) [2024\)](#page-13-2), the *Absolute Zero* paradigm is designed to operate in open-ended settings while remaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how humans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models [\(Hughes](#page-14-1) [et al.,](#page-14-1) [2024\)](#page-14-1). Similar to AlphaZero [\(Silver et al.,](#page-16-3) [2017\)](#page-16-3), which improves through self-play, our proposed paradigm requires no human supervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward enabling large language models to autonomously achieve superhuman reasoning capabilities.

Building on this new reasoning paradigm, we introduce the *Absolute Zero Reasoner (AZR)*, which proposes and solves coding tasks. We cast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable feedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a program, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We train the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of the proposed approach.

Despite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks in math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned with domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically trained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points compared to models trained in the "zero" setting using in-domain data. These surprising results highlight that general reasoning skills can emerge without human-curated domain targeted data, positioning Absolute Zero as a promising research direction and AZR as a first pivotal milestone. Besides the remarkable results AZR achieved with zero human data for reasoning, we also make very interesting findings summarized below:

- **Code priors amplify reasoning.** The base Qwen-Coder-7b model started with math performance 3.6 points lower than Qwen-7b. But after AZR training for both models, the coder variant surpassed the base by  $0.7$  points, suggesting that strong coding capabilities may potentially amplify overall reasoning improvements after AZR training.
- **Cross domain transfer is more pronounced for AZR.** After RLVR, expert code models raise math accuracy by only 0.65 points on average, whereas AZR-Base-7B and AZR-Coder-7B trained on self-proposed code reasoning tasks improve math average by 10.9 and 15.2, respectively, demonstrating much stronger generalized reasoning capability gains.
- **Bigger bases yield bigger gains.** Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7, +10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.
- **Comments as intermediate plans emerge naturally.** When solving code induction tasks, AZR often interleaves step-by-step plans as comments and code (Appendix [C.3\)](#page-24-0), resembling the ReAct prompting framework [\(Yao et al.,](#page-17-1) [2023\)](#page-17-1). Similar behavior has been observed in much larger formal-math models such as DeepSeek Prover v2 (671B) [\(Ren et al.,](#page-15-3) [2025\)](#page-15-3). We therefore believe that allowing the model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.
- **Cognitive Behaviors and Token length depends on reasoning mode.** Distinct cognitive behaviorsâ€”such as step-by-step reasoning, enumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different types of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction grows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.
- **Safety alarms ringing.** We observe AZR with Llama3.1-8b occasionally produces concerning chains of thought, we term the "uh-oh moment", example shown in Figure [32,](#page-37-0) highlighting the need for future work on safety-aware training [\(Zhang et al.,](#page-18-0) [2025a\)](#page-18-0).

## **2. The Absolute Zero Paradigm**

### **2.1. Preliminaries**

**Supervised Fine-Tuning (SFT).** SFT requires the datasets of task-rationale-answer demonstrations <sup>D</sup> <sup>=</sup>  ${(x, c, y)}$ , where *x* is the query, *c* is the gold chain-of-thought (CoT), and *y* is the gold answer, all provided by human experts or superior AI models. The model trains to imitate the reference responses to minimize the conditional negative log-likelihood [\(Ouyang et al.,](#page-15-4) [2022\)](#page-15-4):

$$\mathcal{L}\_{\text{STT}}(\theta) = -\mathbb{E}\_{(x, c^\*, y^\*) \sim \mathcal{D}} \log \pi\_{\theta} \left( c^\*, y^\* \mid x \right). \tag{1}$$

However, at the frontier level, there's no stronger model to distill from, and expert human labeling doesn't scale well.

**Reinforcement Learning with Verifiable Rewards (RLVR).** To move beyond the limits of pure imitation, RLVR only requires a dataset of task and answer <sup>D</sup>  $=$  {(*x, yC*}, without labeled rationale. RLVR allows the model to generate its own CoT and calculate a verifiable reward with the golden answer *r*(*y, yC*). However, the learning task distribution <sup>D</sup>, with its set of queries and gold answers are still labeled by human experts. The trainable policy ** is optimized to maximize expected reward:

$$J\_{\rm RLNR}(\theta) = \mathbb{E}\_{(x, y^\*) \sim \mathcal{D}, \, y \sim \pi\_{\theta}(\cdot \mid x)} \left[ r(y, y^\*) \right]. \tag{2}$$

In summary, both SFT and RLVR still rely on human-curated datasets of either queries, demonstrations, or verifiers, which ultimately limit scalability. The Absolute Zero paradigm removes this dependency by allowing the model to generate, solve, and learn from its own interactions with the environment entirely through self-play.

### **2.2. Absolute Zero**

We propose the Absolute Zero paradigm, where during training, the model simultaneously proposes tasks, solves them, and learns from both stages. No external data is required and the model learns entirely through self-play and experience, aided by some environment. We illustrate this paradigm in Figure [2,](#page-1-0) which contrasts Absolute Zero with supervised learning and RLVR, highlighting how our approach eliminates the need for any human-curated data by enabling self-improving task proposal and solution through self-play.

To make the Absolute Zero setting concrete, we now define how one model can act both as the proposer and solver role. To aid understanding, we include an illustration in Figure [3.](#page-3-0) Let ** be our parameterized language model, it is used to play two roles, proposer  $propose$ ** and solver  $solve$ ** during training.