{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Model Fine-tuning for Marker\n",
    "\n",
    "This notebook demonstrates how to fine-tune the OCR models used in the Marker project. It covers the complete workflow from data preparation to model evaluation and integration.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Marker project uses Surya's `RecognitionPredictor` for OCR (Optical Character Recognition) to extract text from document images. In this notebook, we'll fine-tune this model on domain-specific data to improve accuracy for specialized documents.\n",
    "\n",
    "### What we'll cover:\n",
    "1. Understanding the current OCR pipeline in Marker\n",
    "2. Preparing data for fine-tuning\n",
    "3. Setting up the training environment\n",
    "4. Fine-tuning the OCR model\n",
    "5. Evaluating the fine-tuned model\n",
    "6. Integrating the model with Marker\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Current OCR Pipeline\n",
    "\n",
    "The Marker OCR pipeline consists of several components working together:\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "1. **Document Loading**: PDF documents are loaded using the `PdfProvider`\n",
    "2. **Layout Detection**: The layout model identifies regions in the document\n",
    "3. **OCR Processing**: The OCR model extracts text from these regions\n",
    "4. **Block Construction**: Text is organized into a hierarchical document structure\n",
    "\n",
    "The OCR component specifically relies on Surya's `RecognitionPredictor` model, which is initialized in `marker/models.py` and used by the `OcrBuilder` class in `marker/builders/ocr.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add marker to path for imports\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import marker modules\n",
    "try:\n",
    "    from marker.builders.ocr import OcrBuilder\n",
    "    from marker.models import create_model_dict\n",
    "    from marker.providers.pdf import PdfProvider\n",
    "    from surya.recognition import RecognitionPredictor\n",
    "except ImportError:\n",
    "    print(\"Error: Could not import marker modules. Make sure the project root is correct.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the OCR Components\n",
    "\n",
    "Let's take a closer look at how OCR works in Marker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model dictionary (with CPU for exploration)\n",
    "models = create_model_dict(device=\"cpu\")\n",
    "\n",
    "# Extract the OCR model\n",
    "recognition_model = models[\"recognition_model\"]\n",
    "print(f\"Recognition model type: {type(recognition_model)}\")\n",
    "\n",
    "# Display model properties\n",
    "print(f\"Model architecture: {recognition_model.model.__class__.__name__}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in recognition_model.model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR Process Flow\n",
    "\n",
    "Here's how the OCR process works in Marker:\n",
    "\n",
    "1. The `OcrBuilder` extracts line regions from document pages\n",
    "2. It processes images for these line regions\n",
    "3. The `RecognitionPredictor` model converts these line images to text\n",
    "4. Results are integrated back into the document structure\n",
    "\n",
    "Let's visualize a sample of this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Function to visualize OCR process\n",
    "def visualize_ocr_process(pdf_path, page_idx=0):\n",
    "    # Initialize PDF provider\n",
    "    provider = PdfProvider(pdf_path)\n",
    "    \n",
    "    # Get page image\n",
    "    page_id = str(page_idx)\n",
    "    page_image = provider.get_page_image(page_id)\n",
    "    \n",
    "    # Get line regions (simulated)\n",
    "    page_lines = provider.get_page_lines(page_id)\n",
    "    \n",
    "    # Display original image\n",
    "    plt.figure(figsize=(12, 16))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(page_image)\n",
    "    plt.title(\"Original Page\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Create image with highlighted regions\n",
    "    highlighted = page_image.copy()\n",
    "    draw = ImageDraw.Draw(highlighted)\n",
    "    \n",
    "    for i, line in enumerate(page_lines):\n",
    "        # Draw rectangle around line\n",
    "        if hasattr(line, 'line') and hasattr(line.line, 'polygon'):\n",
    "            bbox = line.line.polygon.bbox\n",
    "            draw.rectangle(bbox, outline=\"red\", width=2)\n",
    "            \n",
    "            # Add line number\n",
    "            draw.text((bbox[0], bbox[1]), str(i), fill=\"blue\")\n",
    "    \n",
    "    # Display highlighted image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(highlighted)\n",
    "    plt.title(\"OCR Line Regions\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display sample of extracted text\n",
    "    print(\"Sample of extracted text:\")\n",
    "    for i, line in enumerate(page_lines[:5]):\n",
    "        if hasattr(line, 'spans'):\n",
    "            line_text = \"\".join([span.text for span in line.spans])\n",
    "            print(f\"Line {i}: {line_text}\")\n",
    "\n",
    "# Uncomment to visualize\n",
    "# visualize_ocr_process('../data/input/sample.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation for Fine-tuning\n",
    "\n",
    "Fine-tuning an OCR model requires domain-specific data. We'll prepare this data in the format expected by the Surya framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Requirements\n",
    "\n",
    "The OCR fine-tuning dataset consists of:\n",
    "- **Line images**: Cropped images of text lines\n",
    "- **Ground truth text**: Correct transcriptions for each line\n",
    "\n",
    "We'll use the `prepare_data.py` script to extract this data from PDFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation parameters\n",
    "input_dir = \"../data/input\"  # Directory with PDFs for fine-tuning\n",
    "output_dir = \"../data/ocr_data\"  # Output directory for OCR data\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Import prepare_data module\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../scripts')))\n",
    "from prepare_data import prepare_ocr_data, setup_data_directories\n",
    "\n",
    "# Setup directories\n",
    "data_dirs = setup_data_directories(output_dir, mode=\"ocr\")\n",
    "print(f\"Created data directories: {data_dirs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data preparation\n",
    "# Note: This is a placeholder call, the function needs to be properly imported from the scripts directory\n",
    "# prepare_ocr_data(input_dir, data_dirs)\n",
    "\n",
    "# Alternative manual preparation code\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def manually_prepare_ocr_data(input_dir, output_dirs):\n",
    "    \"\"\"Manual implementation of OCR data preparation.\"\"\"\n",
    "    line_crops_dir = output_dirs[\"line_crops\"]\n",
    "    transcriptions_dir = output_dirs[\"transcriptions\"]\n",
    "    splits_dir = output_dirs[\"splits\"]\n",
    "    \n",
    "    # List PDF files\n",
    "    pdf_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    all_lines = []\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        pdf_path = os.path.join(input_dir, pdf_file)\n",
    "        \n",
    "        try:\n",
    "            # Initialize PDF provider\n",
    "            provider = PdfProvider(pdf_path)\n",
    "            doc_name = Path(pdf_file).stem\n",
    "            \n",
    "            # Process each page\n",
    "            for page_idx in range(provider.num_pages):\n",
    "                page_id = f\"{page_idx}\"\n",
    "                page_image = provider.get_page_image(page_id)\n",
    "                page_lines = provider.get_page_lines(page_id)\n",
    "                \n",
    "                # Process each line\n",
    "                for line_idx, line in enumerate(page_lines):\n",
    "                    if not hasattr(line, 'line') or not hasattr(line.line, 'polygon'):\n",
    "                        continue\n",
    "                    \n",
    "                    line_polygon = line.line.polygon\n",
    "                    \n",
    "                    # Crop line image\n",
    "                    line_bbox = line_polygon.bbox\n",
    "                    line_crop = page_image.crop(line_bbox)\n",
    "                    \n",
    "                    # Save line crop\n",
    "                    line_id = f\"{doc_name}_page_{page_idx}_line_{line_idx}\"\n",
    "                    crop_path = os.path.join(line_crops_dir, f\"{line_id}.png\")\n",
    "                    line_crop.save(crop_path)\n",
    "                    \n",
    "                    # Extract text\n",
    "                    if hasattr(line, 'spans'):\n",
    "                        line_text = \"\".join([span.text for span in line.spans])\n",
    "                    else:\n",
    "                        line_text = \"\" # Placeholder, would need manual correction\n",
    "                    \n",
    "                    # Save transcription\n",
    "                    trans_path = os.path.join(transcriptions_dir, f\"{line_id}.txt\")\n",
    "                    with open(trans_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(line_text)\n",
    "                    \n",
    "                    # Store line information\n",
    "                    all_lines.append({\n",
    "                        \"doc_name\": doc_name,\n",
    "                        \"page_id\": page_id,\n",
    "                        \"line_id\": line_id,\n",
    "                        \"image_path\": crop_path,\n",
    "                        \"text_path\": trans_path,\n",
    "                        \"text\": line_text,\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file}: {e}\")\n",
    "    \n",
    "    # Create train/val/test splits\n",
    "    import random\n",
    "    random.shuffle(all_lines)\n",
    "    n_lines = len(all_lines)\n",
    "    \n",
    "    train_lines = all_lines[:int(n_lines * 0.8)]\n",
    "    val_lines = all_lines[int(n_lines * 0.8):int(n_lines * 0.9)]\n",
    "    test_lines = all_lines[int(n_lines * 0.9):]\n",
    "    \n",
    "    # Save splits\n",
    "    for split_name, lines in [(\"train\", train_lines), (\"val\", val_lines), (\"test\", test_lines)]:\n",
    "        split_file = os.path.join(splits_dir, f\"{split_name}.json\")\n",
    "        with open(split_file, 'w') as f:\n",
    "            json.dump({\n",
    "                \"lines\": lines\n",
    "            }, f, indent=2)\n",
    "    \n",
    "    print(f\"Prepared OCR data: {len(all_lines)} lines\")\n",
    "    print(f\"  Train: {len(train_lines)}, Val: {len(val_lines)}, Test: {len(test_lines)}\")\n",
    "    print(f\"Line crops saved in: {line_crops_dir}\")\n",
    "    print(f\"Transcriptions saved in: {transcriptions_dir}\")\n",
    "\n",
    "# Uncomment the following line to run data preparation\n",
    "# manually_prepare_ocr_data(input_dir, data_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Correction of Ground Truth\n",
    "\n",
    "For optimal fine-tuning, the ground truth text should be accurate. After running the data preparation script, you should manually check and correct the transcriptions in the `transcriptions_dir`.\n",
    "\n",
    "Let's create a simple tool to assist with this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image as IPImage\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def correction_tool(line_crops_dir, transcriptions_dir, limit=50):\n",
    "    \"\"\"Simple tool to help with ground truth correction.\"\"\"\n",
    "    # Get list of image files\n",
    "    image_files = sorted([f for f in os.listdir(line_crops_dir) if f.lower().endswith('.png')])[:limit]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No image files found in the directory.\")\n",
    "        return\n",
    "    \n",
    "    # Selected image index\n",
    "    current_idx = 0\n",
    "    \n",
    "    # Define UI components\n",
    "    image_widget = widgets.Output()\n",
    "    text_widget = widgets.Textarea(description=\"Text:\", layout=widgets.Layout(width='80%', height='100px'))\n",
    "    save_button = widgets.Button(description=\"Save\")\n",
    "    next_button = widgets.Button(description=\"Next\")\n",
    "    prev_button = widgets.Button(description=\"Previous\")\n",
    "    status_widget = widgets.Label(\"\")\n",
    "    \n",
    "    def update_display():\n",
    "        nonlocal current_idx\n",
    "        image_widget.clear_output()\n",
    "        \n",
    "        with image_widget:\n",
    "            image_path = os.path.join(line_crops_dir, image_files[current_idx])\n",
    "            display(IPImage(filename=image_path))\n",
    "            print(f\"Image {current_idx + 1} of {len(image_files)}: {image_files[current_idx]}\")\n",
    "        \n",
    "        # Load text\n",
    "        base_name = os.path.splitext(image_files[current_idx])[0]\n",
    "        text_path = os.path.join(transcriptions_dir, f\"{base_name}.txt\")\n",
    "        \n",
    "        if os.path.exists(text_path):\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                text_widget.value = f.read()\n",
    "        else:\n",
    "            text_widget.value = \"\"\n",
    "    \n",
    "    def on_save_clicked(b):\n",
    "        base_name = os.path.splitext(image_files[current_idx])[0]\n",
    "        text_path = os.path.join(transcriptions_dir, f\"{base_name}.txt\")\n",
    "        \n",
    "        with open(text_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text_widget.value)\n",
    "        \n",
    "        status_widget.value = f\"Saved text for {base_name}\"\n",
    "    \n",
    "    def on_next_clicked(b):\n",
    "        nonlocal current_idx\n",
    "        if current_idx < len(image_files) - 1:\n",
    "            current_idx += 1\n",
    "            update_display()\n",
    "    \n",
    "    def on_prev_clicked(b):\n",
    "        nonlocal current_idx\n",
    "        if current_idx > 0:\n",
    "            current_idx -= 1\n",
    "            update_display()\n",
    "    \n",
    "    # Connect events\n",
    "    save_button.on_click(on_save_clicked)\n",
    "    next_button.on_click(on_next_clicked)\n",
    "    prev_button.on_click(on_prev_clicked)\n",
    "    \n",
    "    # Create layout\n",
    "    button_box = widgets.HBox([prev_button, save_button, next_button])\n",
    "    app = widgets.VBox([image_widget, text_widget, button_box, status_widget])\n",
    "    \n",
    "    # Initial display\n",
    "    update_display()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Uncomment to run the correction tool\n",
    "# correction_widget = correction_tool(data_dirs[\"line_crops\"], data_dirs[\"transcriptions\"])\n",
    "# display(correction_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Prepared Dataset\n",
    "\n",
    "Let's explore our prepared dataset to understand its characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_dataset(transcriptions_dir, line_crops_dir, sample_size=10):\n",
    "    \"\"\"Analyze the prepared dataset.\"\"\"\n",
    "    # Get all transcription files\n",
    "    text_files = [f for f in os.listdir(transcriptions_dir) if f.lower().endswith('.txt')]\n",
    "    \n",
    "    if not text_files:\n",
    "        print(\"No transcription files found.\")\n",
    "        return\n",
    "    \n",
    "    # Read all text\n",
    "    all_text = \"\"\n",
    "    text_lengths = []\n",
    "    \n",
    "    for text_file in text_files:\n",
    "        text_path = os.path.join(transcriptions_dir, text_file)\n",
    "        with open(text_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            all_text += text\n",
    "            text_lengths.append(len(text))\n",
    "    \n",
    "    # Analyze characters\n",
    "    char_counts = Counter(all_text)\n",
    "    total_chars = len(all_text)\n",
    "    num_unique_chars = len(char_counts)\n",
    "    \n",
    "    # Display statistics\n",
    "    print(f\"Dataset Statistics:\")\n",
    "    print(f\"Total lines: {len(text_files)}\")\n",
    "    print(f\"Total characters: {total_chars}\")\n",
    "    print(f\"Unique characters: {num_unique_chars}\")\n",
    "    print(f\"Average line length: {sum(text_lengths) / len(text_lengths):.2f} chars\")\n",
    "    print(f\"Shortest line: {min(text_lengths)} chars\")\n",
    "    print(f\"Longest line: {max(text_lengths)} chars\")\n",
    "    \n",
    "    # Display most common characters\n",
    "    print(\"\\nMost common characters:\")\n",
    "    for char, count in char_counts.most_common(20):\n",
    "        print(f\"'{char}': {count} ({count/total_chars*100:.2f}%)\")\n",
    "    \n",
    "    # Display sample lines with images\n",
    "    print(\"\\nRandom samples:\")\n",
    "    sample_files = random.sample(text_files, min(sample_size, len(text_files)))\n",
    "    \n",
    "    plt.figure(figsize=(15, sample_size * 2))\n",
    "    \n",
    "    for i, text_file in enumerate(sample_files):\n",
    "        base_name = os.path.splitext(text_file)[0]\n",
    "        image_path = os.path.join(line_crops_dir, f\"{base_name}.png\")\n",
    "        text_path = os.path.join(transcriptions_dir, text_file)\n",
    "        \n",
    "        with open(text_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            image = Image.open(image_path)\n",
    "            plt.subplot(sample_size, 1, i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"Text: {text}\")\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to analyze the dataset\n",
    "# analyze_dataset(data_dirs[\"transcriptions\"], data_dirs[\"line_crops\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Training Environment\n",
    "\n",
    "Now that we have prepared our dataset, let's set up the environment for fine-tuning the OCR model. We'll use Unsloth for efficient 4-bit QLoRA fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies Installation\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install unsloth\n",
    "!pip install huggingface_hub\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Let's load our prepared dataset using the utility functions from Marker's fine-tuning tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset utility\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../unsloth')))\n",
    "from utils import prepare_ocr_dataset\n",
    "\n",
    "# Initialize OCR model to get tokenizer\n",
    "recognition_model = RecognitionPredictor(device=\"cpu\")\n",
    "tokenizer = recognition_model.model.tokenizer\n",
    "\n",
    "# Load dataset\n",
    "dataset = prepare_ocr_dataset(\n",
    "    data_dir=output_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Display dataset info\n",
    "for split in dataset:\n",
    "    print(f\"Split {split}: {len(dataset[split])} examples\")\n",
    "    \n",
    "# Display sample\n",
    "if 'train' in dataset and len(dataset['train']) > 0:\n",
    "    sample = dataset['train'][0]\n",
    "    print(\"\\nSample example:\")\n",
    "    print(f\"Line ID: {sample['line_id']}\")\n",
    "    print(f\"Text: {sample['text']}\")\n",
    "    if 'input_ids' in sample:\n",
    "        print(f\"Input IDs shape: {len(sample['input_ids'])}\")\n",
    "    \n",
    "    # Display image\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.imshow(sample['image'])\n",
    "    plt.title(sample['text'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Fine-tuning Configuration\n",
    "\n",
    "Now, let's configure the fine-tuning process using Unsloth's optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import adapters and trainers\n",
    "from adapters import get_ocr_lora_config, create_qlora_model\n",
    "from trainers import SuryaTrainingArguments, SuryaOCRTrainer\n",
    "\n",
    "# Setup training parameters\n",
    "output_model_dir = \"../models/custom_ocr_model\"\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "training_config = {\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"use_wandb\": False,  # Set to True if you want to use W&B for tracking\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the OCR Model\n",
    "\n",
    "Let's initialize our OCR model with LoRA adapters for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = get_ocr_lora_config(\n",
    "    r=training_config[\"lora_r\"],\n",
    "    lora_alpha=training_config[\"lora_alpha\"],\n",
    "    lora_dropout=training_config[\"lora_dropout\"],\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Try to use Unsloth for optimized QLoRA\n",
    "    from unsloth import FastLanguageModel\n",
    "    \n",
    "    # Quantization config\n",
    "    quantization_config = {\n",
    "        \"load_in_4bit\": True,\n",
    "        \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "        \"bnb_4bit_quant_type\": \"nf4\",\n",
    "        \"bnb_4bit_use_double_quant\": True,\n",
    "    }\n",
    "    \n",
    "    # Create QLoRA model\n",
    "    model, training_args = create_qlora_model(\n",
    "        base_model=recognition_model.model,\n",
    "        lora_config=lora_config,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    print(\"Successfully initialized model with Unsloth optimizations\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Unsloth not available. Using standard fine-tuning.\")\n",
    "    # Standard fine-tuning (without Unsloth)\n",
    "    from peft import get_peft_model\n",
    "    \n",
    "    # Move model to target device\n",
    "    model = recognition_model.model.to(device)\n",
    "    \n",
    "    # Add LoRA adapters\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Default training args\n",
    "    training_args = {\n",
    "        \"learning_rate\": training_config[\"learning_rate\"],\n",
    "        \"num_train_epochs\": training_config[\"num_train_epochs\"],\n",
    "        \"per_device_train_batch_size\": training_config[\"batch_size\"],\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning the OCR Model\n",
    "\n",
    "Now we're ready to fine-tune the OCR model on our custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = SuryaTrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    model_type=\"ocr\",\n",
    "    per_device_train_batch_size=training_config[\"batch_size\"],\n",
    "    per_device_eval_batch_size=training_config[\"batch_size\"],\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=training_config[\"learning_rate\"],\n",
    "    num_train_epochs=training_config[\"num_train_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(output_model_dir, \"logs\"),\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"wandb\" if training_config[\"use_wandb\"] else \"none\",\n",
    "    save_pretrained_merged=True,\n",
    "    early_stopping_patience=3,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = SuryaOCRTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"val\"] if \"val\" in dataset else None,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Fine-tuned Model\n",
    "\n",
    "Let's save our fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "trainer.save_model(output_model_dir)\n",
    "print(f\"Model saved to {output_model_dir}\")\n",
    "\n",
    "# If you want to save the merged model (adapter + base model)\n",
    "merged_dir = os.path.join(output_model_dir, \"merged\")\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "    \n",
    "# Merge adapter with base model\n",
    "if hasattr(model, \"merge_and_unload\"):\n",
    "    merged_model = model.merge_and_unload()\n",
    "    merged_model.save_pretrained(merged_dir)\n",
    "    print(f\"Merged model saved to {merged_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating the Fine-tuned Model\n",
    "\n",
    "Now let's evaluate our model on the test set to see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "if \"test\" in dataset:\n",
    "    test_results = trainer.evaluate(dataset[\"test\"])\n",
    "    print(\"Test Results:\")\n",
    "    for key, value in test_results.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Save test results\n",
    "    results_path = os.path.join(output_model_dir, \"test_results.json\")\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Original vs. Fine-tuned Model\n",
    "\n",
    "Let's compare the performance of our fine-tuned model with the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "def compare_models(original_model, finetuned_model_path, test_samples):\n",
    "    \"\"\"Compare original and fine-tuned models.\"\"\"\n",
    "    # Load fine-tuned model\n",
    "    finetuned_model = RecognitionPredictor(model_path=finetuned_model_path)\n",
    "    \n",
    "    # Prepare sample images\n",
    "    sample_images = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for sample in test_samples:\n",
    "        image_path = sample[\"image_path\"]\n",
    "        text_path = sample[\"text_path\"]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        sample_images.append(image)\n",
    "        \n",
    "        # Load ground truth\n",
    "        with open(text_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read().strip()\n",
    "            ground_truths.append(text)\n",
    "    \n",
    "    # Run inference with original model\n",
    "    original_results = original_model(images=[sample_images], sort_lines=False)[0]\n",
    "    original_texts = [line.text for line in original_results.text_lines]\n",
    "    \n",
    "    # Run inference with fine-tuned model\n",
    "    finetuned_results = finetuned_model(images=[sample_images], sort_lines=False)[0]\n",
    "    finetuned_texts = [line.text for line in finetuned_results.text_lines]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    original_distances = [levenshtein_distance(pred, gt) for pred, gt in zip(original_texts, ground_truths)]\n",
    "    finetuned_distances = [levenshtein_distance(pred, gt) for pred, gt in zip(finetuned_texts, ground_truths)]\n",
    "    \n",
    "    original_accuracy = sum(1 for d in original_distances if d == 0) / len(original_distances) if original_distances else 0\n",
    "    finetuned_accuracy = sum(1 for d in finetuned_distances if d == 0) / len(finetuned_distances) if finetuned_distances else 0\n",
    "    \n",
    "    original_avg_distance = sum(original_distances) / len(original_distances) if original_distances else 0\n",
    "    finetuned_avg_distance = sum(finetuned_distances) / len(finetuned_distances) if finetuned_distances else 0\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Model Comparison:\")\n",
    "    print(f\"Original model - Exact match accuracy: {original_accuracy:.2%}, Avg Levenshtein distance: {original_avg_distance:.2f}\")\n",
    "    print(f\"Fine-tuned model - Exact match accuracy: {finetuned_accuracy:.2%}, Avg Levenshtein distance: {finetuned_avg_distance:.2f}\")\n",
    "    \n",
    "    # Visualize samples\n",
    "    num_samples = min(5, len(sample_images))\n",
    "    plt.figure(figsize=(15, num_samples * 3))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(num_samples, 1, i+1)\n",
    "        plt.imshow(sample_images[i])\n",
    "        plt.title(f\"Ground truth: {ground_truths[i]}\\nOriginal: {original_texts[i]}\\nFine-tuned: {finetuned_texts[i]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get test samples\n",
    "# test_samples = dataset[\"test\"][:20] if \"test\" in dataset and len(dataset[\"test\"]) > 0 else []\n",
    "\n",
    "# Uncomment to run comparison\n",
    "# compare_models(recognition_model, os.path.join(output_model_dir, \"merged\"), test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integrating with Marker\n",
    "\n",
    "Finally, let's see how to integrate our fine-tuned model back into the Marker pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marker.models import create_model_dict\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.config.parser import ParserConfig\n",
    "\n",
    "def integrate_custom_model(custom_model_path, pdf_path, output_dir=\"../output\"):\n",
    "    \"\"\"Integrate custom model with Marker.\"\"\"\n",
    "    # Create configuration\n",
    "    config = ParserConfig(\n",
    "        model_list=[\"surya_det\", \"surya_rec\"],\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    \n",
    "    # Create models with custom OCR model\n",
    "    models = create_model_dict(\n",
    "        device=config.device,\n",
    "        dtype=None,\n",
    "        custom_model_paths={\n",
    "            \"recognition_model\": custom_model_path\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create converter\n",
    "    converter = PdfConverter(\n",
    "        artifact_dict=models,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Process PDF\n",
    "    print(f\"Processing {pdf_path} with custom OCR model...\")\n",
    "    result = converter(pdf_path)\n",
    "    \n",
    "    # Save output\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, os.path.basename(pdf_path).replace(\".pdf\", \".md\"))\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(result)\n",
    "    \n",
    "    print(f\"Output saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "# Example usage\n",
    "# sample_pdf = \"../data/input/sample.pdf\"\n",
    "# custom_model_path = os.path.join(output_model_dir, \"merged\")\n",
    "# output_file = integrate_custom_model(custom_model_path, sample_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Original vs. Custom Model Output\n",
    "\n",
    "Let's compare the output of the original and fine-tuned models on a sample document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(original_output, custom_output):\n",
    "    \"\"\"Compare outputs from original and custom models.\"\"\"\n",
    "    # Read outputs\n",
    "    with open(original_output, 'r', encoding='utf-8') as f:\n",
    "        original_text = f.read()\n",
    "    \n",
    "    with open(custom_output, 'r', encoding='utf-8') as f:\n",
    "        custom_text = f.read()\n",
    "    \n",
    "    # Compare text\n",
    "    import difflib\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    differ = difflib.HtmlDiff()\n",
    "    html_diff = differ.make_file(original_text.splitlines(), custom_text.splitlines(),\n",
    "                                 \"Original Model Output\", \"Fine-tuned Model Output\")\n",
    "    \n",
    "    # Display diff\n",
    "    return HTML(html_diff)\n",
    "\n",
    "# Example usage\n",
    "# original_output = \"../output/sample_original.md\"\n",
    "# custom_output = \"../output/sample.md\"\n",
    "# diff_view = compare_outputs(original_output, custom_output)\n",
    "# display(diff_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've covered the complete process of fine-tuning an OCR model for the Marker project:\n",
    "\n",
    "1. We explored the current OCR pipeline and its components\n",
    "2. We prepared domain-specific data for fine-tuning\n",
    "3. We set up the training environment with Unsloth optimizations\n",
    "4. We fine-tuned the OCR model using QLoRA\n",
    "5. We evaluated the model and compared it with the original\n",
    "6. We integrated the fine-tuned model back into Marker\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further improve OCR performance, consider:\n",
    "\n",
    "1. **Data augmentation**: Apply transforms like rotation, scaling, and noise to training data\n",
    "2. **Hyperparameter tuning**: Experiment with learning rates, batch sizes, and LoRA parameters\n",
    "3. **Domain-specific training**: Fine-tune on more documents from your specific domain\n",
    "4. **Ensemble methods**: Combine predictions from multiple models\n",
    "5. **Post-processing**: Add domain-specific correction rules for common errors\n",
    "\n",
    "By fine-tuning the OCR model for your specific use case, you can significantly improve the accuracy of text extraction in the Marker pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}